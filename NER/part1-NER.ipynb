{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224D Assignment #2\n",
    "# Part [1]: Deep Networks: NER Window Model\n",
    "\n",
    "For this first part of the assignment, you'll build your first \"deep\" networks. On problem set 1, you computed the backpropagation gradient $\\frac{\\partial J}{\\partial w}$ for a two-layer network; in this problem set you'll implement a slightly more complex network to perform  named entity recognition (NER).\n",
    "\n",
    "Before beginning the programming section, you should complete parts (a) and (b) of the corresponding section of the handout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c): Random Initialization Test\n",
    "Use the cell below to test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46994114 -0.83008197  0.23148553  0.43094097 -0.00258593]\n",
      " [-0.47666619 -0.52297046  0.45125243 -0.57311684 -0.71301636]\n",
      " [ 0.32105262  0.78530031 -0.85918681  0.02111762  0.54147539]]\n"
     ]
    }
   ],
   "source": [
    "from misc import random_weight_matrix\n",
    "random.seed(10)\n",
    "print random_weight_matrix(3,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d): Implementation\n",
    "\n",
    "We've provided starter code to load in the dataset and convert it to a list of \"windows\", consisting of indices into the matrix of word vectors. \n",
    "\n",
    "We pad each sentence with begin and end tokens `<s>` and `</s>`, which have their own word vector representations; additionally, we convert all words to lowercase, canonicalize digits (e.g. `1.12` becomes `DG.DGDG`), and replace unknown words with a special token `UUUNKKK`.\n",
    "\n",
    "You don't need to worry about the details of this, but you can inspect the `docs` variables or look at the raw data (in plaintext) in the `./data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import data_utils.utils as du\n",
    "import data_utils.ner as ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the starter word vectors\n",
    "wv, word_to_num, num_to_word = ner.load_wv('data/ner/vocab.txt',\n",
    "                                           'data/ner/wordVectors.txt')\n",
    "tagnames = [\"O\", \"LOC\", \"MISC\", \"ORG\", \"PER\"]\n",
    "num_to_tag = dict(enumerate(tagnames))\n",
    "tag_to_num = du.invert_dict(num_to_tag)\n",
    "\n",
    "# Set window size\n",
    "windowsize = 3\n",
    "\n",
    "# Load the training set\n",
    "docs = du.load_dataset('data/ner/train')\n",
    "X_train, y_train = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=windowsize)\n",
    "\n",
    "# Load the dev set (for tuning hyperparameters)\n",
    "docs = du.load_dataset('data/ner/dev')\n",
    "X_dev, y_dev = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=windowsize)\n",
    "\n",
    "# Load the test set (dummy labels only)\n",
    "docs = du.load_dataset('data/ner/test.masked')\n",
    "X_test, y_test = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=windowsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid re-inventing the wheel, we provide a base class that handles a lot of the drudgery of managing parameters and running gradient descent. It's based on the classifier API used by [`scikit-learn`](http://scikit-learn.org/stable/), so if you're familiar with that library it should be easy to use. \n",
    "\n",
    "We'll be using this class for the rest of this assignment, so it helps to get acquainted with a simple example that should be familiar from Assignment 1. To keep this notebook uncluttered, we've put the code in the `softmax_example.py`; take a look at it there, then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grad_check: dJ/db error norm = 3.565e-10 [ok]\n",
      "    b dims: [5] = 5 elem\n",
      "grad_check: dJ/dW error norm = 2.164e-11 [ok]\n",
      "    W dims: [5, 100] = 500 elem\n",
      "grad_check: dJ/dL[5] error norm = 2.646e-11 [ok]\n",
      "    L[5] dims: [100] = 100 elem\n"
     ]
    }
   ],
   "source": [
    "from softmax_example import SoftmaxRegression\n",
    "sr = SoftmaxRegression(wv=zeros((10,100)), dims=(100,5))\n",
    "\n",
    "##\n",
    "# Automatic gradient checker!\n",
    "# this checks anything you add to self.grads or self.sgrads\n",
    "# using the method of Assignment 1\n",
    "sr.grad_check(x=5, y=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement a model, you need to subclass `NNBase`, then implement the following methods:\n",
    "\n",
    "- `__init__()` (initialize parameters and hyperparameters)\n",
    "- `_acc_grads()` (compute and accumulate gradients)\n",
    "- `compute_loss()` (compute loss for a training example)\n",
    "- `predict()`, `predict_proba()`, or other prediction method (for evaluation)\n",
    "\n",
    "`NNBase` provides you with a few others that will be helpful:\n",
    "\n",
    "- `grad_check()` (run a gradient check - calls `_acc_grads` and `compute_loss`)\n",
    "- `train_sgd()` (run SGD training; more on this later)\n",
    "\n",
    "Your task is to implement the window model in `nerwindow.py`; a scaffold has been provided for you with instructions on what to fill in.\n",
    "\n",
    "When ready, you can test below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grad_check: dJ/db2 error norm = 3.205e-10 [ok]\n",
      "    b2 dims: [5] = 5 elem\n",
      "grad_check: dJ/dU error norm = 2.837e-10 [ok]\n",
      "    U dims: [5, 100] = 500 elem\n",
      "grad_check: dJ/db1 error norm = 2.849e-09 [ok]\n",
      "    b1 dims: [100] = 100 elem\n",
      "grad_check: dJ/dW error norm = 1.337e-08 [ok]\n",
      "    W dims: [100, 150] = 15000 elem\n",
      "grad_check: dJ/dL[30] error norm = 3.335e-11 [ok]\n",
      "    L[30] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[6659] error norm = 4.373e-11 [ok]\n",
      "    L[6659] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[12637] error norm = 4.509e-11 [ok]\n",
      "    L[12637] dims: [50] = 50 elem\n"
     ]
    }
   ],
   "source": [
    "from nerwindow import WindowMLP\n",
    "clf = WindowMLP(wv, windowsize=3, dims=[None, 100, 5],\n",
    "                reg=0.001, alpha=0.01)\n",
    "clf.grad_check(X_train[0], y_train[0]) # gradient check on single point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train your model on some data! You can implement your own SGD method, but we recommend that you just call `clf.train_sgd`. This takes the following arguments:\n",
    "\n",
    "- `X`, `y` : training data\n",
    "- `idxiter`: iterable (list or generator) that gives index (row of X) of training examples in the order they should be visited by SGD\n",
    "- `printevery`: int, prints progress after this many examples\n",
    "- `costevery`: int, computes mean loss after this many examples. This is a costly operation, so don't make this too frequent!\n",
    "\n",
    "The implementation we give you supports minibatch learning; if `idxiter` is a list-of-lists (or yields lists), then gradients will be computed for all indices in a minibatch before modifying the parameters (this is why we have you write `_acc_grad` instead of applying them directly!).\n",
    "\n",
    "Before training, you should generate a training schedule to pass as `idxiter`. If you know how to use Python generators, we recommend those; otherwise, just make a static list. Make the following in the cell below:\n",
    "\n",
    "- An \"epoch\" schedule that just iterates through the training set, in order, `nepoch` times.\n",
    "- A random schedule of `N` examples sampled with replacement from the training set.\n",
    "- A random schedule of `N/k` minibatches of size `k`, sampled with replacement from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nepoch = 5\n",
    "N = nepoch * len(y_train)\n",
    "k = 5 # minibatch size\n",
    "\n",
    "random.seed(10) # do not change this!\n",
    "#### YOUR CODE HERE ####\n",
    "inorder_sched = [[x for x in xrange(nepoch)] for _ in xrange(len(y_train))]\n",
    "randomN_sched = [random.choice(len(y_train), 1) for _ in xrange(N)]\n",
    "minibatch_sched = [random.choice(len(y_train), k) for _ in xrange(N/k)]\n",
    "\n",
    "# For verification that everything works\n",
    "#clf.train_sgd(X_train, y_train, minibatch_sched)\n",
    "#### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call `train_sgd` to train on `X_train`, `y_train`. To verify that things work, train on 100,000 examples or so to start (with any of the above schedules). This shouldn't take more than a couple minutes, and you should get a mean cross-entropy loss around 0.4.\n",
    "\n",
    "Now, if this works well, it's time for production! You have three tasks here:\n",
    "\n",
    "1. Train a good model\n",
    "2. Plot a learning curve (cost vs. # of iterations)\n",
    "3. Use your best model to predict the test set\n",
    "\n",
    "You should train on the `train` data and evaluate performance on the `dev` set. The `test` data we provided has only dummy labels (everything is `O`); we'll compare your predictions to the true labels at grading time. \n",
    "\n",
    "Scroll down to section (f) for the evaluation code.\n",
    "\n",
    "We don't expect you to spend too much time doing an exhaustive search here; the default parameters should work well, although you can certainly do better. Try to achieve an F1 score of at least 76% on the dev set, as reported by `eval_performance`.\n",
    "\n",
    "Feel free to create new cells and write new code here, including new functions (helpers and otherwise) in `nerwindow.py`. When you have a good model, follow the instructions below to make predictions on the test set.\n",
    "\n",
    "A strong model may require 10-20 passes (or equivalent number of random samples) through the training set and could take 20 minutes or more to train - but it's also possible to be much, much faster!\n",
    "\n",
    "Things you may want to tune:\n",
    "- `alpha` (including using an \"annealing\" schedule to decrease the learning rate over time)\n",
    "- training schedule and minibatch size\n",
    "- regularization strength\n",
    "- hidden layer dimension\n",
    "- width of context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nerwindow import full_report, eval_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using ws: 3, rs: 0.001000, lr: 0.010000\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.01 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 27.76 s\n",
      "  [10000]: mean loss 0.322155\n",
      "  Seen 20000 in 51.10 s\n",
      "  [20000]: mean loss 0.280764\n",
      "  Seen 30000 in 73.84 s\n",
      "  [30000]: mean loss 0.258772\n",
      "  Seen 40000 in 96.56 s\n",
      "  [40000]: mean loss 0.240091\n",
      "  Seen 50000 in 119.87 s\n",
      "  [50000]: mean loss 0.228467\n",
      "  Seen 60000 in 142.69 s\n",
      "  [60000]: mean loss 0.206918\n",
      "  Seen 70000 in 165.62 s\n",
      "  [70000]: mean loss 0.19797\n",
      "  Seen 80000 in 188.75 s\n",
      "  [80000]: mean loss 0.196278\n",
      "  Seen 90000 in 211.87 s\n",
      "  [90000]: mean loss 0.176021\n",
      "  Seen 100000 in 234.92 s\n",
      "  [100000]: mean loss 0.177079\n",
      "  Seen 110000 in 257.87 s\n",
      "  [110000]: mean loss 0.160704\n",
      "  Seen 120000 in 280.91 s\n",
      "  [120000]: mean loss 0.157148\n",
      "  Seen 130000 in 303.83 s\n",
      "  [130000]: mean loss 0.154148\n",
      "  Seen 140000 in 326.83 s\n",
      "  [140000]: mean loss 0.150603\n",
      "  Seen 150000 in 349.76 s\n",
      "  [150000]: mean loss 0.144174\n",
      "  Seen 160000 in 372.80 s\n",
      "  [160000]: mean loss 0.14389\n",
      "  Seen 170000 in 395.83 s\n",
      "  [170000]: mean loss 0.133939\n",
      "  Seen 180000 in 420.82 s\n",
      "  [180000]: mean loss 0.13275\n",
      "  Seen 190000 in 445.30 s\n",
      "  [190000]: mean loss 0.137737\n",
      "  Seen 200000 in 468.48 s\n",
      "  [200000]: mean loss 0.124271\n",
      "  [203621]: mean loss 0.126815\n",
      "SGD complete: 203621 examples in 496.51 seconds.\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  85.33%\n",
      "Mean recall:     71.70%\n",
      "Mean F1:         77.22%\n",
      "Training using ws: 3, rs: 0.001000, lr: 0.100000\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 27.81 s\n",
      "  [10000]: mean loss 0.586514\n",
      "  Seen 20000 in 58.15 s\n",
      "  [20000]: mean loss 0.324148\n",
      "  Seen 30000 in 85.61 s\n",
      "  [30000]: mean loss 0.263151\n",
      "  Seen 40000 in 112.22 s\n",
      "  [40000]: mean loss 0.212481\n",
      "  Seen 50000 in 140.89 s\n",
      "  [50000]: mean loss 0.27959\n",
      "  Seen 60000 in 167.81 s\n",
      "  [60000]: mean loss 0.183337\n",
      "  Seen 70000 in 190.83 s\n",
      "  [70000]: mean loss 0.173483\n",
      "  Seen 80000 in 213.79 s\n",
      "  [80000]: mean loss 0.227109\n",
      "  Seen 90000 in 236.79 s\n",
      "  [90000]: mean loss 0.154428\n",
      "  Seen 100000 in 260.40 s\n",
      "  [100000]: mean loss 0.194916\n",
      "  Seen 110000 in 283.81 s\n",
      "  [110000]: mean loss 0.148986\n",
      "  Seen 120000 in 308.19 s\n",
      "  [120000]: mean loss 0.119635\n",
      "  Seen 130000 in 331.25 s\n",
      "  [130000]: mean loss 0.156284\n",
      "  Seen 140000 in 357.02 s\n",
      "  [140000]: mean loss 0.131014\n",
      "  Seen 150000 in 380.04 s\n",
      "  [150000]: mean loss 0.113547\n",
      "  Seen 160000 in 403.11 s\n",
      "  [160000]: mean loss 0.145311\n",
      "  Seen 170000 in 426.18 s\n",
      "  [170000]: mean loss 0.108983\n",
      "  Seen 180000 in 449.29 s\n",
      "  [180000]: mean loss 0.121139\n",
      "  Seen 190000 in 473.04 s\n",
      "  [190000]: mean loss 0.136623\n",
      "  Seen 200000 in 496.00 s\n",
      "  [200000]: mean loss 0.119151\n",
      "  [203621]: mean loss 0.114271\n",
      "SGD complete: 203621 examples in 523.85 seconds.\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  86.92%\n",
      "Mean recall:     71.43%\n",
      "Mean F1:         77.69%\n",
      "Training using ws: 3, rs: 0.010000, lr: 0.010000\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 23.23 s\n",
      "  [10000]: mean loss 0.375473\n",
      "  Seen 20000 in 46.24 s\n",
      "  [20000]: mean loss 0.342248\n",
      "  Seen 30000 in 69.38 s\n",
      "  [30000]: mean loss 0.334342\n",
      "  Seen 40000 in 92.24 s\n",
      "  [40000]: mean loss 0.324017\n",
      "  Seen 50000 in 115.08 s\n",
      "  [50000]: mean loss 0.32085\n",
      "  Seen 60000 in 141.26 s\n",
      "  [60000]: mean loss 0.300695\n",
      "  Seen 70000 in 164.27 s\n",
      "  [70000]: mean loss 0.297857\n",
      "  Seen 80000 in 187.81 s\n",
      "  [80000]: mean loss 0.29371\n",
      "  Seen 90000 in 211.58 s\n",
      "  [90000]: mean loss 0.278697\n",
      "  Seen 100000 in 235.54 s\n",
      "  [100000]: mean loss 0.279369\n",
      "  Seen 110000 in 259.16 s\n",
      "  [110000]: mean loss 0.266692\n",
      "  Seen 120000 in 283.66 s\n",
      "  [120000]: mean loss 0.266047\n",
      "  Seen 130000 in 306.79 s\n",
      "  [130000]: mean loss 0.260798\n",
      "  Seen 140000 in 329.93 s\n",
      "  [140000]: mean loss 0.255771\n",
      "  Seen 150000 in 353.09 s\n",
      "  [150000]: mean loss 0.248726\n",
      "  Seen 160000 in 376.13 s\n",
      "  [160000]: mean loss 0.257969\n",
      "  Seen 170000 in 399.45 s\n",
      "  [170000]: mean loss 0.241956\n",
      "  Seen 180000 in 422.38 s\n",
      "  [180000]: mean loss 0.240495\n",
      "  Seen 190000 in 445.42 s\n",
      "  [190000]: mean loss 0.254173\n",
      "  Seen 200000 in 468.25 s\n",
      "  [200000]: mean loss 0.237377\n",
      "  [203621]: mean loss 0.234364\n",
      "SGD complete: 203621 examples in 496.11 seconds.\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  76.51%\n",
      "Mean recall:     58.03%\n",
      "Mean F1:         64.90%\n",
      "Training using ws: 3, rs: 0.010000, lr: 0.100000\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.77988\n",
      "  Seen 10000 in 22.97 s\n",
      "  [10000]: mean loss 0.446948\n",
      "  Seen 20000 in 46.02 s\n",
      "  [20000]: mean loss 0.339558\n",
      "  Seen 30000 in 68.93 s\n",
      "  [30000]: mean loss 0.315349\n",
      "  Seen 40000 in 91.81 s\n",
      "  [40000]: mean loss 0.277012\n",
      "  Seen 50000 in 114.78 s\n",
      "  [50000]: mean loss 0.296\n",
      "  Seen 60000 in 137.88 s\n",
      "  [60000]: mean loss 0.268681\n",
      "  Seen 70000 in 160.88 s\n",
      "  [70000]: mean loss 0.250784\n",
      "  Seen 80000 in 183.80 s\n",
      "  [80000]: mean loss 0.341569\n",
      "  Seen 90000 in 206.93 s\n",
      "  [90000]: mean loss 0.233621\n",
      "  Seen 100000 in 230.37 s\n",
      "  [100000]: mean loss 0.354353\n",
      "  Seen 110000 in 253.53 s\n",
      "  [110000]: mean loss 0.201389\n",
      "  Seen 120000 in 277.10 s\n",
      "  [120000]: mean loss 0.184459\n",
      "  Seen 130000 in 304.11 s\n",
      "  [130000]: mean loss 0.1914\n",
      "  Seen 140000 in 328.07 s\n",
      "  [140000]: mean loss 0.175238\n",
      "  Seen 150000 in 351.00 s\n",
      "  [150000]: mean loss 0.168646\n",
      "  Seen 160000 in 373.85 s\n",
      "  [160000]: mean loss 0.194965\n",
      "  Seen 170000 in 396.80 s\n",
      "  [170000]: mean loss 0.178964\n",
      "  Seen 180000 in 419.75 s\n",
      "  [180000]: mean loss 0.190587\n",
      "  Seen 190000 in 442.64 s\n",
      "  [190000]: mean loss 0.203983\n",
      "  Seen 200000 in 465.66 s\n",
      "  [200000]: mean loss 0.186468\n",
      "  [203621]: mean loss 0.166886\n",
      "SGD complete: 203621 examples in 493.55 seconds.\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  79.97%\n",
      "Mean recall:     68.02%\n",
      "Mean F1:         71.59%\n",
      "Training using ws: 5, rs: 0.001000, lr: 0.010000\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.9314\n",
      "  Seen 10000 in 27.00 s\n",
      "  [10000]: mean loss 0.301897\n",
      "  Seen 20000 in 53.98 s\n",
      "  [20000]: mean loss 0.258146\n",
      "  Seen 30000 in 80.95 s\n",
      "  [30000]: mean loss 0.237265\n",
      "  Seen 40000 in 107.87 s\n",
      "  [40000]: mean loss 0.225636\n",
      "  Seen 50000 in 134.94 s\n",
      "  [50000]: mean loss 0.214111\n",
      "  Seen 60000 in 162.11 s\n",
      "  [60000]: mean loss 0.194392\n",
      "  Seen 70000 in 193.28 s\n",
      "  [70000]: mean loss 0.194815\n",
      "  Seen 80000 in 220.33 s\n",
      "  [80000]: mean loss 0.195275\n",
      "  Seen 90000 in 247.29 s\n",
      "  [90000]: mean loss 0.167547\n",
      "  Seen 100000 in 274.35 s\n",
      "  [100000]: mean loss 0.162264\n",
      "  Seen 110000 in 301.37 s\n",
      "  [110000]: mean loss 0.153774\n",
      "  Seen 120000 in 331.85 s\n",
      "  [120000]: mean loss 0.147866\n",
      "  Seen 130000 in 359.38 s\n",
      "  [130000]: mean loss 0.145296\n",
      "  Seen 140000 in 386.55 s\n",
      "  [140000]: mean loss 0.143797\n",
      "  Seen 150000 in 413.62 s\n",
      "  [150000]: mean loss 0.136649\n",
      "  Seen 160000 in 441.86 s\n",
      "  [160000]: mean loss 0.135372\n",
      "  Seen 170000 in 468.98 s\n",
      "  [170000]: mean loss 0.127412\n",
      "  Seen 180000 in 496.37 s\n",
      "  [180000]: mean loss 0.12678\n",
      "  Seen 190000 in 524.42 s\n",
      "  [190000]: mean loss 0.129114\n",
      "  Seen 200000 in 551.58 s\n",
      "  [200000]: mean loss 0.115794\n",
      "  [203621]: mean loss 0.119349\n",
      "SGD complete: 203621 examples in 582.64 seconds.\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  85.78%\n",
      "Mean recall:     72.75%\n",
      "Mean F1:         77.90%\n",
      "Training using ws: 5, rs: 0.001000, lr: 0.100000\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.9314\n",
      "  Seen 10000 in 28.99 s\n",
      "  [10000]: mean loss 1.3328\n",
      "  Seen 20000 in 56.24 s\n",
      "  [20000]: mean loss 1.22365\n",
      "  Seen 30000 in 84.40 s\n",
      "  [30000]: mean loss 1.40887\n",
      "  Seen 40000 in 111.73 s\n",
      "  [40000]: mean loss 1.06423\n",
      "  Seen 50000 in 139.30 s\n",
      "  [50000]: mean loss 1.06752\n",
      "  Seen 60000 in 166.51 s\n",
      "  [60000]: mean loss 1.05479\n",
      "  Seen 70000 in 193.61 s\n",
      "  [70000]: mean loss 0.95297\n",
      "  Seen 80000 in 220.75 s\n",
      "  [80000]: mean loss 1.04181\n",
      "  Seen 90000 in 248.15 s\n",
      "  [90000]: mean loss 0.980637\n",
      "  Seen 100000 in 275.34 s\n",
      "  [100000]: mean loss 0.835605\n",
      "  Seen 110000 in 304.45 s\n",
      "  [110000]: mean loss 0.736097\n",
      "  Seen 120000 in 332.50 s\n",
      "  [120000]: mean loss 0.739223\n",
      "  Seen 130000 in 359.98 s\n",
      "  [130000]: mean loss 0.742237\n",
      "  Seen 140000 in 387.29 s\n",
      "  [140000]: mean loss 0.843931\n",
      "  Seen 150000 in 415.80 s\n",
      "  [150000]: mean loss 0.834607\n",
      "  Seen 160000 in 447.06 s\n",
      "  [160000]: mean loss 0.726202\n",
      "  Seen 170000 in 474.87 s\n",
      "  [170000]: mean loss 0.597226\n",
      "  Seen 180000 in 502.21 s\n",
      "  [180000]: mean loss 0.661503\n",
      "  Seen 190000 in 529.34 s\n",
      "  [190000]: mean loss 0.933613\n",
      "  Seen 200000 in 556.56 s\n",
      "  [200000]: mean loss 0.710089\n",
      "  [203621]: mean loss 0.715221\n",
      "SGD complete: 203621 examples in 587.70 seconds.\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  68.12%\n",
      "Mean recall:     58.82%\n",
      "Mean F1:         60.70%\n",
      "Training using ws: 5, rs: 0.010000, lr: 0.010000\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.9314\n",
      "  Seen 10000 in 27.13 s\n",
      "  [10000]: mean loss 0.369701\n",
      "  Seen 20000 in 54.14 s\n",
      "  [20000]: mean loss 0.333367\n",
      "  Seen 30000 in 82.72 s\n",
      "  [30000]: mean loss 0.322382\n",
      "  Seen 40000 in 111.40 s\n",
      "  [40000]: mean loss 0.307598\n",
      "  Seen 50000 in 140.71 s\n",
      "  [50000]: mean loss 0.30153\n",
      "  Seen 60000 in 168.52 s\n",
      "  [60000]: mean loss 0.284834\n",
      "  Seen 70000 in 196.55 s\n",
      "  [70000]: mean loss 0.282874\n",
      "  Seen 80000 in 224.33 s\n",
      "  [80000]: mean loss 0.280433\n",
      "  Seen 90000 in 252.52 s\n",
      "  [90000]: mean loss 0.260515\n",
      "  Seen 100000 in 279.73 s\n",
      "  [100000]: mean loss 0.266985\n",
      "  Seen 110000 in 306.72 s\n",
      "  [110000]: mean loss 0.259095\n",
      "  Seen 120000 in 333.67 s\n",
      "  [120000]: mean loss 0.252898\n",
      "  Seen 130000 in 360.70 s\n",
      "  [130000]: mean loss 0.242655\n",
      "  Seen 140000 in 387.62 s\n",
      "  [140000]: mean loss 0.241143\n",
      "  Seen 150000 in 414.59 s\n",
      "  [150000]: mean loss 0.237829\n",
      "  Seen 160000 in 441.57 s\n",
      "  [160000]: mean loss 0.241323\n",
      "  Seen 170000 in 468.68 s\n",
      "  [170000]: mean loss 0.229484\n",
      "  Seen 180000 in 495.70 s\n",
      "  [180000]: mean loss 0.229051\n",
      "  Seen 190000 in 522.75 s\n",
      "  [190000]: mean loss 0.237351\n",
      "  Seen 200000 in 549.83 s\n",
      "  [200000]: mean loss 0.226066\n",
      "  [203621]: mean loss 0.220074\n",
      "SGD complete: 203621 examples in 580.68 seconds.\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  79.66%\n",
      "Mean recall:     60.55%\n",
      "Mean F1:         67.43%\n",
      "Training using ws: 5, rs: 0.010000, lr: 0.100000\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.9314\n",
      "  Seen 10000 in 27.15 s\n",
      "  [10000]: mean loss 0.434625\n",
      "  Seen 20000 in 54.55 s\n",
      "  [20000]: mean loss 0.363509\n",
      "  Seen 30000 in 81.91 s\n",
      "  [30000]: mean loss 0.33556\n",
      "  Seen 40000 in 109.09 s\n",
      "  [40000]: mean loss 0.256116\n",
      "  Seen 50000 in 136.36 s\n",
      "  [50000]: mean loss 0.274007\n",
      "  Seen 60000 in 163.54 s\n",
      "  [60000]: mean loss 0.276643\n",
      "  Seen 70000 in 192.46 s\n",
      "  [70000]: mean loss 0.256921\n",
      "  Seen 80000 in 220.28 s\n",
      "  [80000]: mean loss 0.292702\n",
      "  Seen 90000 in 248.31 s\n",
      "  [90000]: mean loss 0.214597\n",
      "  Seen 100000 in 275.39 s\n",
      "  [100000]: mean loss 0.270052\n",
      "  Seen 110000 in 303.21 s\n",
      "  [110000]: mean loss 0.233305\n",
      "  Seen 120000 in 330.28 s\n",
      "  [120000]: mean loss 0.197145\n",
      "  Seen 130000 in 357.19 s\n",
      "  [130000]: mean loss 0.185355\n",
      "  Seen 140000 in 384.19 s\n",
      "  [140000]: mean loss 0.168195\n",
      "  Seen 150000 in 411.21 s\n",
      "  [150000]: mean loss 0.191248\n",
      "  Seen 160000 in 438.38 s\n",
      "  [160000]: mean loss 0.21801\n",
      "  Seen 170000 in 465.30 s\n",
      "  [170000]: mean loss 0.177537\n",
      "  Seen 180000 in 492.29 s\n",
      "  [180000]: mean loss 0.168756\n",
      "  Seen 190000 in 519.35 s\n",
      "  [190000]: mean loss 0.193819\n",
      "  Seen 200000 in 547.04 s\n",
      "  [200000]: mean loss 0.168707\n",
      "  [203621]: mean loss 0.167384\n",
      "SGD complete: 203621 examples in 577.97 seconds.\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  81.81%\n",
      "Mean recall:     65.08%\n",
      "Mean F1:         70.75%\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# The given parameters already perform fairly well\n",
    "# Lets do finer hyperparameter tuning around those given parameters\n",
    "rsVal = [1e-3, 1e-2]\n",
    "lrVal = [1e-2, 1e-1]\n",
    "wsVal = [3,5]\n",
    "for ws in wsVal:\n",
    "    for rs in rsVal:\n",
    "        for lr in lrVal:\n",
    "            print \"Training using ws: %d, rs: %f, lr: %f\" %(ws, rs, lr)\n",
    "            docs = du.load_dataset('data/ner/dev')\n",
    "            X_dev, y_dev = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=ws)\n",
    "            docs = du.load_dataset('data/ner/train')\n",
    "            X_train, y_train = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=ws)\n",
    "\n",
    "            # Tune over reg strength and learning rate\n",
    "            clf = WindowMLP(wv, windowsize=ws, dims=[None, 100, 5],\n",
    "                            reg=rs, alpha=lr)\n",
    "\n",
    "            # Predict labels on the dev set\n",
    "            clf.train_sgd(X_train, y_train, minibatch_sched)\n",
    "\n",
    "            yp = clf.predict(X_dev)\n",
    "\n",
    "            #full_report(y_dev, yp, tagnames) # full report, helpful diagnostics\n",
    "            eval_performance(y_dev, yp, tagnames) # performance: optimize this F1\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "77.22% ws: 3, rs: 0.001000, lr: 0.010000  \n",
    "77.69% ws: 3, rs: 0.001000, lr: 0.100000  \n",
    "Best so far is F1 of **77.9%** using ws: 5, rs: 0.001000, lr: 0.010000\n",
    "\n",
    "Let's take these parameters and try different training schedules and minibatch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using minibatch of size 7 ws: 5, rs: 0.001000, lr: 0.010000\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.9314\n",
      "  Seen 10000 in 32.25 s\n",
      "  [10000]: mean loss 0.290459\n",
      "  Seen 20000 in 66.15 s\n",
      "  [20000]: mean loss 0.243237\n",
      "  Seen 30000 in 98.72 s\n",
      "  [30000]: mean loss 0.217038\n",
      "  Seen 40000 in 130.17 s\n",
      "  [40000]: mean loss 0.209701\n",
      "  Seen 50000 in 161.48 s\n",
      "  [50000]: mean loss 0.188176\n",
      "  Seen 60000 in 192.97 s\n",
      "  [60000]: mean loss 0.176898\n",
      "  Seen 70000 in 224.84 s\n",
      "  [70000]: mean loss 0.162433\n",
      "  Seen 80000 in 256.67 s\n",
      "  [80000]: mean loss 0.163426\n",
      "  Seen 90000 in 288.63 s\n",
      "  [90000]: mean loss 0.143804\n",
      "  Seen 100000 in 320.40 s\n",
      "  [100000]: mean loss 0.140843\n",
      "  Seen 110000 in 352.13 s\n",
      "  [110000]: mean loss 0.148536\n",
      "  Seen 120000 in 383.92 s\n",
      "  [120000]: mean loss 0.124093\n",
      "  Seen 130000 in 415.57 s\n",
      "  [130000]: mean loss 0.122015\n",
      "  Seen 140000 in 447.51 s\n",
      "  [140000]: mean loss 0.120731\n",
      "  [145443]: mean loss 0.114332\n",
      "SGD complete: 145443 examples in 483.20 seconds.\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  83.62%\n",
      "Mean recall:     77.14%\n",
      "Mean F1:         80.10%\n",
      "Training using stochastic ws: 5, rs: 0.001000, lr: 0.010000\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.9314\n",
      "  Seen 10000 in 15.97 s\n",
      "  [10000]: mean loss 0.365845\n",
      "  Seen 20000 in 31.91 s\n",
      "  [20000]: mean loss 0.342607\n",
      "  Seen 30000 in 47.78 s\n",
      "  [30000]: mean loss 0.316046\n",
      "  Seen 40000 in 63.68 s\n",
      "  [40000]: mean loss 0.302613\n",
      "  Seen 50000 in 79.56 s\n",
      "  [50000]: mean loss 0.292618\n",
      "  Seen 60000 in 95.48 s\n",
      "  [60000]: mean loss 0.284589\n",
      "  Seen 70000 in 111.38 s\n",
      "  [70000]: mean loss 0.288115\n",
      "  Seen 80000 in 129.44 s\n",
      "  [80000]: mean loss 0.278825\n",
      "  Seen 90000 in 148.07 s\n",
      "  [90000]: mean loss 0.279153\n",
      "  Seen 100000 in 163.98 s\n",
      "  [100000]: mean loss 0.270898\n",
      "  Seen 110000 in 179.95 s\n",
      "  [110000]: mean loss 0.261565\n",
      "  Seen 120000 in 195.90 s\n",
      "  [120000]: mean loss 0.255035\n",
      "  Seen 130000 in 211.86 s\n",
      "  [130000]: mean loss 0.248901\n",
      "  Seen 140000 in 227.84 s\n",
      "  [140000]: mean loss 0.244754\n",
      "  Seen 150000 in 244.12 s\n",
      "  [150000]: mean loss 0.239761\n",
      "  Seen 160000 in 260.13 s\n",
      "  [160000]: mean loss 0.239277\n",
      "  Seen 170000 in 276.01 s\n",
      "  [170000]: mean loss 0.236951\n",
      "  Seen 180000 in 291.90 s\n",
      "  [180000]: mean loss 0.241895\n",
      "  Seen 190000 in 307.76 s\n",
      "  [190000]: mean loss 0.24617\n",
      "  Seen 200000 in 323.56 s\n",
      "  [200000]: mean loss 0.226017\n",
      "  Seen 210000 in 339.47 s\n",
      "  [210000]: mean loss 0.218339\n",
      "  Seen 220000 in 355.31 s\n",
      "  [220000]: mean loss 0.219298\n",
      "  Seen 230000 in 371.22 s\n",
      "  [230000]: mean loss 0.211395\n",
      "  Seen 240000 in 387.14 s\n",
      "  [240000]: mean loss 0.214326\n",
      "  Seen 250000 in 403.03 s\n",
      "  [250000]: mean loss 0.231881\n",
      "  Seen 260000 in 418.92 s\n",
      "  [260000]: mean loss 0.205879\n",
      "  Seen 270000 in 434.79 s\n",
      "  [270000]: mean loss 0.210817\n",
      "  Seen 280000 in 450.89 s\n",
      "  [280000]: mean loss 0.206256\n",
      "  Seen 290000 in 466.83 s\n",
      "  [290000]: mean loss 0.197138\n",
      "  Seen 300000 in 482.72 s\n",
      "  [300000]: mean loss 0.193977\n",
      "  Seen 310000 in 498.73 s\n",
      "  [310000]: mean loss 0.204324\n",
      "  Seen 320000 in 514.67 s\n",
      "  [320000]: mean loss 0.189927\n",
      "  Seen 330000 in 530.68 s\n",
      "  [330000]: mean loss 0.189147\n",
      "  Seen 340000 in 546.68 s\n",
      "  [340000]: mean loss 0.20705\n",
      "  Seen 350000 in 562.64 s\n",
      "  [350000]: mean loss 0.196072\n",
      "  Seen 360000 in 578.69 s\n",
      "  [360000]: mean loss 0.188481\n",
      "  Seen 370000 in 594.68 s\n",
      "  [370000]: mean loss 0.178219\n",
      "  Seen 380000 in 610.52 s\n",
      "  [380000]: mean loss 0.182941\n",
      "  Seen 390000 in 626.47 s\n",
      "  [390000]: mean loss 0.180222\n",
      "  Seen 400000 in 642.36 s\n",
      "  [400000]: mean loss 0.181511\n",
      "  Seen 410000 in 658.28 s\n",
      "  [410000]: mean loss 0.170595\n",
      "  Seen 420000 in 674.21 s\n",
      "  [420000]: mean loss 0.172543\n",
      "  Seen 430000 in 690.23 s\n",
      "  [430000]: mean loss 0.177292\n",
      "  Seen 440000 in 706.11 s\n",
      "  [440000]: mean loss 0.168371\n",
      "  Seen 450000 in 722.05 s\n",
      "  [450000]: mean loss 0.167562\n",
      "  Seen 460000 in 738.67 s\n",
      "  [460000]: mean loss 0.171506\n",
      "  Seen 470000 in 757.10 s\n",
      "  [470000]: mean loss 0.167769\n",
      "  Seen 480000 in 772.99 s\n",
      "  [480000]: mean loss 0.16538\n",
      "  Seen 490000 in 788.84 s\n",
      "  [490000]: mean loss 0.163067\n",
      "  Seen 500000 in 804.65 s\n",
      "  [500000]: mean loss 0.165988\n",
      "  Seen 510000 in 820.52 s\n",
      "  [510000]: mean loss 0.163427\n",
      "  Seen 520000 in 836.37 s\n",
      "  [520000]: mean loss 0.164165\n",
      "  Seen 530000 in 852.21 s\n",
      "  [530000]: mean loss 0.155007\n",
      "  Seen 540000 in 868.07 s\n",
      "  [540000]: mean loss 0.16373\n",
      "  Seen 550000 in 884.06 s\n",
      "  [550000]: mean loss 0.162881\n",
      "  Seen 560000 in 899.98 s\n",
      "  [560000]: mean loss 0.152343\n",
      "  Seen 570000 in 915.96 s\n",
      "  [570000]: mean loss 0.155014\n",
      "  Seen 580000 in 931.84 s\n",
      "  [580000]: mean loss 0.1613\n",
      "  Seen 590000 in 947.83 s\n",
      "  [590000]: mean loss 0.153791\n",
      "  Seen 600000 in 963.75 s\n",
      "  [600000]: mean loss 0.150982\n",
      "  Seen 610000 in 979.67 s\n",
      "  [610000]: mean loss 0.150986\n",
      "  Seen 620000 in 995.64 s\n",
      "  [620000]: mean loss 0.147054\n",
      "  Seen 630000 in 1011.61 s\n",
      "  [630000]: mean loss 0.151092\n",
      "  Seen 640000 in 1027.59 s\n",
      "  [640000]: mean loss 0.150058\n",
      "  Seen 650000 in 1043.51 s\n",
      "  [650000]: mean loss 0.142589\n",
      "  Seen 660000 in 1059.57 s\n",
      "  [660000]: mean loss 0.144357\n",
      "  Seen 670000 in 1075.56 s\n",
      "  [670000]: mean loss 0.147967\n",
      "  Seen 680000 in 1091.53 s\n",
      "  [680000]: mean loss 0.137055\n",
      "  Seen 690000 in 1107.44 s\n",
      "  [690000]: mean loss 0.137458\n",
      "  Seen 700000 in 1123.36 s\n",
      "  [700000]: mean loss 0.140353\n",
      "  Seen 710000 in 1139.31 s\n",
      "  [710000]: mean loss 0.136091\n",
      "  Seen 720000 in 1155.20 s\n",
      "  [720000]: mean loss 0.138785\n",
      "  Seen 730000 in 1171.24 s\n",
      "  [730000]: mean loss 0.136779\n",
      "  Seen 740000 in 1187.19 s\n",
      "  [740000]: mean loss 0.138064\n",
      "  Seen 750000 in 1202.90 s\n",
      "  [750000]: mean loss 0.135113\n",
      "  Seen 760000 in 1218.83 s\n",
      "  [760000]: mean loss 0.129569\n",
      "  Seen 770000 in 1234.58 s\n",
      "  [770000]: mean loss 0.137099\n",
      "  Seen 780000 in 1250.70 s\n",
      "  [780000]: mean loss 0.143863\n",
      "  Seen 790000 in 1266.55 s\n",
      "  [790000]: mean loss 0.134157\n",
      "  Seen 800000 in 1283.48 s\n",
      "  [800000]: mean loss 0.130283\n",
      "  Seen 810000 in 1299.28 s\n",
      "  [810000]: mean loss 0.128432\n",
      "  Seen 820000 in 1315.31 s\n",
      "  [820000]: mean loss 0.132376\n",
      "  Seen 830000 in 1331.44 s\n",
      "  [830000]: mean loss 0.127971\n",
      "  Seen 840000 in 1347.20 s\n",
      "  [840000]: mean loss 0.128742\n",
      "  Seen 850000 in 1363.29 s\n",
      "  [850000]: mean loss 0.125221\n",
      "  Seen 860000 in 1379.52 s\n",
      "  [860000]: mean loss 0.12706\n",
      "  Seen 870000 in 1395.58 s\n",
      "  [870000]: mean loss 0.125316\n",
      "  Seen 880000 in 1411.43 s\n",
      "  [880000]: mean loss 0.123327\n",
      "  Seen 890000 in 1427.20 s\n",
      "  [890000]: mean loss 0.122753\n",
      "  Seen 900000 in 1444.36 s\n",
      "  [900000]: mean loss 0.125561\n",
      "  Seen 910000 in 1462.42 s\n",
      "  [910000]: mean loss 0.121805\n",
      "  Seen 920000 in 1478.51 s\n",
      "  [920000]: mean loss 0.125832\n",
      "  Seen 930000 in 1494.39 s\n",
      "  [930000]: mean loss 0.121594\n",
      "  Seen 940000 in 1510.05 s\n",
      "  [940000]: mean loss 0.123853\n",
      "  Seen 950000 in 1526.03 s\n",
      "  [950000]: mean loss 0.116951\n",
      "  Seen 960000 in 1541.72 s\n",
      "  [960000]: mean loss 0.115261\n",
      "  Seen 970000 in 1557.92 s\n",
      "  [970000]: mean loss 0.12078\n",
      "  Seen 980000 in 1574.24 s\n",
      "  [980000]: mean loss 0.120425\n",
      "  Seen 990000 in 1590.91 s\n",
      "  [990000]: mean loss 0.119634\n",
      "  Seen 1000000 in 1607.09 s\n",
      "  [1000000]: mean loss 0.114853\n",
      "  Seen 1010000 in 1623.27 s\n",
      "  [1010000]: mean loss 0.115366\n",
      "  [1018105]: mean loss 0.116039\n",
      "SGD complete: 1018105 examples in 1651.29 seconds.\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  85.96%\n",
      "Mean recall:     72.27%\n",
      "Mean F1:         78.30%\n",
      "Training using standard epoch of nepochs iterations ws: 5, rs: 0.001000, lr: 0.010000\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.9314\n",
      "  Seen 10000 in 15.99 s\n",
      "  [10000]: mean loss 0.482412\n",
      "  Seen 20000 in 31.69 s\n",
      "  [20000]: mean loss 0.402811\n",
      "  Seen 30000 in 47.39 s\n",
      "  [30000]: mean loss 0.455767\n",
      "  Seen 40000 in 63.09 s\n",
      "  [40000]: mean loss 0.401202\n",
      "  Seen 50000 in 78.92 s\n",
      "  [50000]: mean loss 0.426452\n",
      "  Seen 60000 in 94.65 s\n",
      "  [60000]: mean loss 0.362536\n",
      "  Seen 70000 in 110.29 s\n",
      "  [70000]: mean loss 0.482228\n",
      "  Seen 80000 in 126.03 s\n",
      "  [80000]: mean loss 0.330345\n",
      "  Seen 90000 in 141.73 s\n",
      "  [90000]: mean loss 0.357565\n",
      "  Seen 100000 in 157.71 s\n",
      "  [100000]: mean loss 0.332208\n",
      "  Seen 110000 in 174.19 s\n",
      "  [110000]: mean loss 0.334683\n",
      "  Seen 120000 in 190.05 s\n",
      "  [120000]: mean loss 0.31693\n",
      "  Seen 130000 in 206.00 s\n",
      "  [130000]: mean loss 0.309932\n",
      "  Seen 140000 in 221.88 s\n",
      "  [140000]: mean loss 0.405031\n",
      "  Seen 150000 in 237.61 s\n",
      "  [150000]: mean loss 0.343379\n",
      "  Seen 160000 in 253.46 s\n",
      "  [160000]: mean loss 0.324779\n",
      "  Seen 170000 in 269.19 s\n",
      "  [170000]: mean loss 0.337122\n",
      "  Seen 180000 in 284.92 s\n",
      "  [180000]: mean loss 0.284165\n",
      "  Seen 190000 in 300.62 s\n",
      "  [190000]: mean loss 0.325185\n",
      "  Seen 200000 in 316.38 s\n",
      "  [200000]: mean loss 0.324431\n",
      "  Seen 210000 in 332.19 s\n",
      "  [210000]: mean loss 0.296808\n",
      "  Seen 220000 in 347.85 s\n",
      "  [220000]: mean loss 0.29415\n",
      "  Seen 230000 in 363.54 s\n",
      "  [230000]: mean loss 0.352681\n",
      "  Seen 240000 in 379.35 s\n",
      "  [240000]: mean loss 0.300101\n",
      "  Seen 250000 in 395.17 s\n",
      "  [250000]: mean loss 0.338184\n",
      "  Seen 260000 in 410.84 s\n",
      "  [260000]: mean loss 0.284368\n",
      "  Seen 270000 in 426.68 s\n",
      "  [270000]: mean loss 0.431179\n",
      "  Seen 280000 in 442.56 s\n",
      "  [280000]: mean loss 0.279381\n",
      "  Seen 290000 in 458.28 s\n",
      "  [290000]: mean loss 0.275578\n",
      "  Seen 300000 in 474.67 s\n",
      "  [300000]: mean loss 0.26277\n",
      "  Seen 310000 in 490.49 s\n",
      "  [310000]: mean loss 0.266368\n",
      "  Seen 320000 in 506.27 s\n",
      "  [320000]: mean loss 0.265681\n",
      "  Seen 330000 in 522.15 s\n",
      "  [330000]: mean loss 0.249645\n",
      "  Seen 340000 in 537.96 s\n",
      "  [340000]: mean loss 0.305003\n",
      "  Seen 350000 in 553.70 s\n",
      "  [350000]: mean loss 0.292603\n",
      "  Seen 360000 in 569.43 s\n",
      "  [360000]: mean loss 0.256112\n",
      "  Seen 370000 in 585.62 s\n",
      "  [370000]: mean loss 0.252595\n",
      "  Seen 380000 in 601.53 s\n",
      "  [380000]: mean loss 0.236584\n",
      "  Seen 390000 in 617.23 s\n",
      "  [390000]: mean loss 0.289633\n",
      "  Seen 400000 in 633.09 s\n",
      "  [400000]: mean loss 0.307674\n",
      "  Seen 410000 in 648.76 s\n",
      "  [410000]: mean loss 0.226672\n",
      "  Seen 420000 in 664.50 s\n",
      "  [420000]: mean loss 0.254689\n",
      "  Seen 430000 in 680.34 s\n",
      "  [430000]: mean loss 0.244063\n",
      "  Seen 440000 in 696.21 s\n",
      "  [440000]: mean loss 0.287549\n",
      "  Seen 450000 in 712.40 s\n",
      "  [450000]: mean loss 0.238556\n",
      "  Seen 460000 in 728.41 s\n",
      "  [460000]: mean loss 0.322169\n",
      "  Seen 470000 in 745.09 s\n",
      "  [470000]: mean loss 0.270004\n",
      "  Seen 480000 in 760.90 s\n",
      "  [480000]: mean loss 0.268483\n",
      "  Seen 490000 in 776.72 s\n",
      "  [490000]: mean loss 0.24275\n",
      "  Seen 500000 in 792.48 s\n",
      "  [500000]: mean loss 0.24013\n",
      "  Seen 510000 in 808.13 s\n",
      "  [510000]: mean loss 0.226838\n",
      "  Seen 520000 in 823.89 s\n",
      "  [520000]: mean loss 0.24141\n",
      "  Seen 530000 in 839.64 s\n",
      "  [530000]: mean loss 0.283456\n",
      "  Seen 540000 in 855.37 s\n",
      "  [540000]: mean loss 0.252114\n",
      "  Seen 550000 in 871.48 s\n",
      "  [550000]: mean loss 0.245302\n",
      "  Seen 560000 in 887.35 s\n",
      "  [560000]: mean loss 0.237529\n",
      "  Seen 570000 in 906.69 s\n",
      "  [570000]: mean loss 0.20156\n",
      "  Seen 580000 in 922.47 s\n",
      "  [580000]: mean loss 0.248374\n",
      "  Seen 590000 in 938.27 s\n",
      "  [590000]: mean loss 0.226864\n",
      "  Seen 600000 in 954.10 s\n",
      "  [600000]: mean loss 0.231568\n",
      "  Seen 610000 in 970.05 s\n",
      "  [610000]: mean loss 0.241828\n",
      "  Seen 620000 in 986.38 s\n",
      "  [620000]: mean loss 0.23559\n",
      "  Seen 630000 in 1002.52 s\n",
      "  [630000]: mean loss 0.249035\n",
      "  Seen 640000 in 1018.36 s\n",
      "  [640000]: mean loss 0.26126\n",
      "  Seen 650000 in 1034.25 s\n",
      "  [650000]: mean loss 0.221075\n",
      "  Seen 660000 in 1050.53 s\n",
      "  [660000]: mean loss 0.310956\n",
      "  Seen 670000 in 1067.06 s\n",
      "  [670000]: mean loss 0.198171\n",
      "  Seen 680000 in 1082.74 s\n",
      "  [680000]: mean loss 0.226876\n",
      "  Seen 690000 in 1099.61 s\n",
      "  [690000]: mean loss 0.236398\n",
      "  Seen 700000 in 1116.24 s\n",
      "  [700000]: mean loss 0.256677\n",
      "  Seen 710000 in 1132.03 s\n",
      "  [710000]: mean loss 0.176383\n",
      "  Seen 720000 in 1147.79 s\n",
      "  [720000]: mean loss 0.20668\n",
      "  Seen 730000 in 1163.65 s\n",
      "  [730000]: mean loss 0.19435\n",
      "  Seen 740000 in 1179.47 s\n",
      "  [740000]: mean loss 0.260086\n",
      "  Seen 750000 in 1195.32 s\n",
      "  [750000]: mean loss 0.244925\n",
      "  Seen 760000 in 1211.10 s\n",
      "  [760000]: mean loss 0.249535\n",
      "  Seen 770000 in 1226.87 s\n",
      "  [770000]: mean loss 0.201228\n",
      "  Seen 780000 in 1242.63 s\n",
      "  [780000]: mean loss 0.233077\n",
      "  Seen 790000 in 1258.37 s\n",
      "  [790000]: mean loss 0.182296\n",
      "  Seen 800000 in 1274.22 s\n",
      "  [800000]: mean loss 0.210699\n",
      "  Seen 810000 in 1289.76 s\n",
      "  [810000]: mean loss 0.210612\n",
      "  Seen 820000 in 1305.29 s\n",
      "  [820000]: mean loss 0.195324\n",
      "  Seen 830000 in 1320.98 s\n",
      "  [830000]: mean loss 0.182588\n",
      "  Seen 840000 in 1336.69 s\n",
      "  [840000]: mean loss 0.195407\n",
      "  Seen 850000 in 1352.39 s\n",
      "  [850000]: mean loss 0.191599\n",
      "  Seen 860000 in 1368.12 s\n",
      "  [860000]: mean loss 0.210051\n",
      "  Seen 870000 in 1383.78 s\n",
      "  [870000]: mean loss 0.244776\n",
      "  Seen 880000 in 1399.56 s\n",
      "  [880000]: mean loss 0.249487\n",
      "  Seen 890000 in 1415.82 s\n",
      "  [890000]: mean loss 0.184362\n",
      "  Seen 900000 in 1431.91 s\n",
      "  [900000]: mean loss 0.173773\n",
      "  Seen 910000 in 1447.59 s\n",
      "  [910000]: mean loss 0.165693\n",
      "  Seen 920000 in 1463.33 s\n",
      "  [920000]: mean loss 0.230844\n",
      "  Seen 930000 in 1479.11 s\n",
      "  [930000]: mean loss 0.194125\n",
      "  Seen 940000 in 1494.85 s\n",
      "  [940000]: mean loss 0.174538\n",
      "  Seen 950000 in 1513.10 s\n",
      "  [950000]: mean loss 0.203328\n",
      "  Seen 960000 in 1534.97 s\n",
      "  [960000]: mean loss 0.170907\n",
      "  Seen 970000 in 1551.57 s\n",
      "  [970000]: mean loss 0.1802\n",
      "  Seen 980000 in 1567.63 s\n",
      "  [980000]: mean loss 0.182462\n",
      "  Seen 990000 in 1583.42 s\n",
      "  [990000]: mean loss 0.190378\n",
      "  Seen 1000000 in 1599.26 s\n",
      "  [1000000]: mean loss 0.182463\n",
      "  Seen 1010000 in 1614.97 s\n",
      "  [1010000]: mean loss 0.186621\n",
      "  [1018105]: mean loss 0.16914\n",
      "SGD complete: 1018105 examples in 1642.42 seconds.\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  76.56%\n",
      "Mean recall:     69.99%\n",
      "Mean F1:         72.16%\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "rs = 1e-3\n",
    "lr = 1e-2\n",
    "ws = 5\n",
    "docs = du.load_dataset('data/ner/dev')\n",
    "X_dev, y_dev = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=ws)\n",
    "docs = du.load_dataset('data/ner/train')\n",
    "X_train, y_train = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=ws)\n",
    "\n",
    "## Minibatch\n",
    "k=7\n",
    "minibatch_sched = [random.choice(len(y_train), k) for _ in xrange(N/k)]\n",
    "print \"Training using minibatch of size 7 ws: %d, rs: %f, lr: %f\" %(ws, rs, lr)\n",
    "\n",
    "# Tune over reg strength and learning rate\n",
    "clf_mb = WindowMLP(wv, windowsize=ws, dims=[None, 100, 5],\n",
    "                reg=rs, alpha=lr)\n",
    "\n",
    "# Predict labels on the dev set\n",
    "clf_mb.train_sgd(X_train, y_train, minibatch_sched)\n",
    "\n",
    "yp = clf_mb.predict(X_dev)\n",
    "\n",
    "eval_performance(y_dev, yp, tagnames) # performance: optimize this F1\n",
    "\n",
    "## Stochastic\n",
    "print \"Training using stochastic ws: %d, rs: %f, lr: %f\" %(ws, rs, lr)\n",
    "\n",
    "# Tune over reg strength and learning rate\n",
    "clf_s = WindowMLP(wv, windowsize=ws, dims=[None, 100, 5],\n",
    "                reg=rs, alpha=lr)\n",
    "\n",
    "# Predict labels on the dev set\n",
    "clf_s.train_sgd(X_train, y_train, randomN_sched)\n",
    "\n",
    "yp = clf_s.predict(X_dev)\n",
    "\n",
    "eval_performance(y_dev, yp, tagnames) # performance: optimize this F1\n",
    "\n",
    "## Nepochs\n",
    "print \"Training using standard epoch of nepochs iterations ws: %d, rs: %f, lr: %f\" %(ws, rs, lr)\n",
    "# Tune over reg strength and learning rate\n",
    "clf_ne = WindowMLP(wv, windowsize=ws, dims=[None, 100, 5],\n",
    "                reg=rs, alpha=lr)\n",
    "\n",
    "# Predict labels on the dev set\n",
    "clf_ne.train_sgd(X_train, y_train, inorder_sched)\n",
    "\n",
    "yp = clf_ne.predict(X_dev)\n",
    "\n",
    "eval_performance(y_dev, yp, tagnames) # performance: optimize this F1\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best performing model has mean F1 of **80.10%** using minibatch of size 7 ws: 5, rs: 0.001000, lr: 0.010000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.9314\n",
      "  [1000]: mean loss 0.394272\n",
      "  [2000]: mean loss 0.341242\n",
      "  [3000]: mean loss 0.330692\n",
      "  [4000]: mean loss 0.328916\n",
      "  [5000]: mean loss 0.309743\n",
      "  [6000]: mean loss 0.305981\n",
      "  [7000]: mean loss 0.296585\n",
      "  [8000]: mean loss 0.289463\n",
      "  [9000]: mean loss 0.280904\n",
      "  Seen 10000 in 145.84 s\n",
      "  [10000]: mean loss 0.283141\n",
      "  [11000]: mean loss 0.272192\n",
      "  [12000]: mean loss 0.281742\n",
      "  [13000]: mean loss 0.263202\n",
      "  [14000]: mean loss 0.258399\n",
      "  [15000]: mean loss 0.262908\n",
      "  [16000]: mean loss 0.254838\n",
      "  [17000]: mean loss 0.255942\n",
      "  [18000]: mean loss 0.255909\n",
      "  [19000]: mean loss 0.260193\n",
      "  Seen 20000 in 286.03 s\n",
      "  [20000]: mean loss 0.251238\n",
      "  [21000]: mean loss 0.240887\n",
      "  [22000]: mean loss 0.24165\n",
      "  [23000]: mean loss 0.256174\n",
      "  [24000]: mean loss 0.233942\n",
      "  [25000]: mean loss 0.242338\n",
      "  [26000]: mean loss 0.230827\n",
      "  [27000]: mean loss 0.225829\n",
      "  [28000]: mean loss 0.228468\n",
      "  [29000]: mean loss 0.222278\n",
      "  Seen 30000 in 426.21 s\n",
      "  [30000]: mean loss 0.216958\n",
      "  [31000]: mean loss 0.217225\n",
      "  [32000]: mean loss 0.224581\n",
      "  [33000]: mean loss 0.212659\n",
      "  [34000]: mean loss 0.217542\n",
      "  [35000]: mean loss 0.21084\n",
      "  [36000]: mean loss 0.214208\n",
      "  [37000]: mean loss 0.202371\n",
      "  [38000]: mean loss 0.211527\n",
      "  [39000]: mean loss 0.205803\n",
      "  Seen 40000 in 565.39 s\n",
      "  [40000]: mean loss 0.20595\n",
      "  [41000]: mean loss 0.194517\n",
      "  [42000]: mean loss 0.203165\n",
      "  [43000]: mean loss 0.202039\n",
      "  [44000]: mean loss 0.20139\n",
      "  [45000]: mean loss 0.197663\n",
      "  [46000]: mean loss 0.190516\n",
      "  [47000]: mean loss 0.194184\n",
      "  [48000]: mean loss 0.185987\n",
      "  [49000]: mean loss 0.181973\n",
      "  Seen 50000 in 706.27 s\n",
      "  [50000]: mean loss 0.18268\n",
      "  [51000]: mean loss 0.189456\n",
      "  [52000]: mean loss 0.186136\n",
      "  [53000]: mean loss 0.184933\n",
      "  [54000]: mean loss 0.201661\n",
      "  [55000]: mean loss 0.178067\n",
      "  [56000]: mean loss 0.177389\n",
      "  [57000]: mean loss 0.183965\n",
      "  [58000]: mean loss 0.172434\n",
      "  [59000]: mean loss 0.191492\n",
      "  Seen 60000 in 847.75 s\n",
      "  [60000]: mean loss 0.174743\n",
      "  [61000]: mean loss 0.170308\n",
      "  [62000]: mean loss 0.174764\n",
      "  [63000]: mean loss 0.169016\n",
      "  [64000]: mean loss 0.168297\n",
      "  [65000]: mean loss 0.173728\n",
      "  [66000]: mean loss 0.171591\n",
      "  [67000]: mean loss 0.181136\n",
      "  [68000]: mean loss 0.174465\n",
      "  [69000]: mean loss 0.168363\n",
      "  Seen 70000 in 1003.82 s\n",
      "  [70000]: mean loss 0.16155\n",
      "  [71000]: mean loss 0.160794\n",
      "  [72000]: mean loss 0.163353\n",
      "  [73000]: mean loss 0.158231\n",
      "  [74000]: mean loss 0.159528\n",
      "  [75000]: mean loss 0.156185\n",
      "  [76000]: mean loss 0.155127\n",
      "  [77000]: mean loss 0.161047\n",
      "  [78000]: mean loss 0.151871\n",
      "  [79000]: mean loss 0.151909\n",
      "  Seen 80000 in 1145.79 s\n",
      "  [80000]: mean loss 0.154799\n",
      "  [81000]: mean loss 0.150172\n",
      "  [82000]: mean loss 0.148915\n",
      "  [83000]: mean loss 0.15543\n",
      "  [84000]: mean loss 0.160308\n",
      "  [85000]: mean loss 0.169133\n",
      "  [86000]: mean loss 0.153939\n",
      "  [87000]: mean loss 0.148615\n",
      "  [88000]: mean loss 0.147497\n",
      "  [89000]: mean loss 0.158419\n",
      "  Seen 90000 in 1292.87 s\n",
      "  [90000]: mean loss 0.146667\n",
      "  [91000]: mean loss 0.146482\n",
      "  [92000]: mean loss 0.146229\n",
      "  [93000]: mean loss 0.14702\n",
      "  [94000]: mean loss 0.139512\n",
      "  [95000]: mean loss 0.152672\n",
      "  [96000]: mean loss 0.139018\n",
      "  [97000]: mean loss 0.137684\n",
      "  [98000]: mean loss 0.144602\n",
      "  [99000]: mean loss 0.142978\n",
      "  Seen 100000 in 1449.42 s\n",
      "  [100000]: mean loss 0.141561\n",
      "  [101000]: mean loss 0.135573\n",
      "  [102000]: mean loss 0.138182\n",
      "  [103000]: mean loss 0.132767\n",
      "  [104000]: mean loss 0.142089\n",
      "  [105000]: mean loss 0.14053\n",
      "  [106000]: mean loss 0.138452\n",
      "  [107000]: mean loss 0.140341\n",
      "  [108000]: mean loss 0.136692\n",
      "  [109000]: mean loss 0.134373\n",
      "  Seen 110000 in 1590.48 s\n",
      "  [110000]: mean loss 0.133595\n",
      "  [111000]: mean loss 0.141116\n",
      "  [112000]: mean loss 0.134292\n",
      "  [113000]: mean loss 0.128563\n",
      "  [114000]: mean loss 0.126917\n",
      "  [115000]: mean loss 0.135349\n",
      "  [116000]: mean loss 0.130133\n",
      "  [117000]: mean loss 0.12949\n",
      "  [118000]: mean loss 0.130292\n",
      "  [119000]: mean loss 0.129587\n",
      "  Seen 120000 in 1731.97 s\n",
      "  [120000]: mean loss 0.128397\n",
      "  [121000]: mean loss 0.130165\n",
      "  [122000]: mean loss 0.123\n",
      "  [123000]: mean loss 0.129331\n",
      "  [124000]: mean loss 0.124923\n",
      "  [125000]: mean loss 0.123985\n",
      "  [126000]: mean loss 0.125823\n",
      "  [127000]: mean loss 0.120111\n",
      "  [128000]: mean loss 0.122282\n",
      "  [129000]: mean loss 0.121077\n",
      "  Seen 130000 in 1887.08 s\n",
      "  [130000]: mean loss 0.124388\n",
      "  [131000]: mean loss 0.122813\n",
      "  [132000]: mean loss 0.123006\n",
      "  [133000]: mean loss 0.131023\n",
      "  [134000]: mean loss 0.122987\n",
      "  [135000]: mean loss 0.125057\n",
      "  [136000]: mean loss 0.124148\n",
      "  [137000]: mean loss 0.120733\n",
      "  [138000]: mean loss 0.120546\n",
      "  [139000]: mean loss 0.120005\n",
      "  Seen 140000 in 2045.76 s\n",
      "  [140000]: mean loss 0.120762\n",
      "  [141000]: mean loss 0.122976\n",
      "  [142000]: mean loss 0.116817\n",
      "  [143000]: mean loss 0.115856\n",
      "  [144000]: mean loss 0.117666\n",
      "  [145000]: mean loss 0.1183\n",
      "  [145443]: mean loss 0.118651\n",
      "SGD complete: 145443 examples in 2142.66 seconds.\n"
     ]
    }
   ],
   "source": [
    "ws = 5\n",
    "rs = 0.001\n",
    "lr = 0.01\n",
    "k=7\n",
    "\n",
    "minibatch_sched = [random.choice(len(y_train), k) for _ in xrange(N/k)]\n",
    "\n",
    "docs = du.load_dataset('data/ner/dev')\n",
    "X_dev, y_dev = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=ws)\n",
    "docs = du.load_dataset('data/ner/train')\n",
    "X_train, y_train = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=ws)\n",
    "\n",
    "clf = WindowMLP(wv, windowsize=ws, dims=[None, 100, 5],\n",
    "                reg=rs, alpha=lr)\n",
    "\n",
    "traincurvebest = clf.train_sgd(X_train, y_train, minibatch_sched, costevery=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e): Plot Learning Curves\n",
    "The `train_sgd` function returns a list of points `(counter, cost)` giving the mean loss after that number of SGD iterations.\n",
    "\n",
    "If the model is taking too long you can cut it off by going to *Kernel->Interrupt* in the IPython menu; `train_sgd` will return the training curve so-far, and you can restart without losing your training progress.\n",
    "\n",
    "Make two plots:\n",
    "\n",
    "- Learning curve using `reg = 0.001`, and comparing the effect of changing the learning rate: run with `alpha = 0.01` and `alpha = 0.1`. Use minibatches of size 5, and train for 10,000 minibatches with `costevery=200`. Be sure to scale up your counts (x-axis) to reflect the batch size. What happens if the model tries to learn too fast? Explain why this occurs, based on the relation of SGD to the true objective.\n",
    "\n",
    "- Learning curve for your best model (print the hyperparameters in the title), as trained using your best schedule. Set `costevery` so that you get at least 100 points to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAj8AAAGJCAYAAABl11LCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAAPYQAAD2EBqD+naQAAIABJREFUeJzs3XmcHFW5//HPMyETFoEghF1JJmRlEcIVEhBF5bqACwq/\n",
       "i5CExeWil6s/F4jbVVxAJKgo/rgqVw0RSMQLoohXUdyukAQQEDUhQUgismVB9kAmyTy/P05V+nRN\n",
       "9UxPd890T9f3/XrVa2aqq6qrunu6nz7nOc8xd0dERESkKDqafQIiIiIiQ0nBj4iIiBSKgh8REREp\n",
       "FAU/IiIiUigKfkRERKRQFPyIiIhIoSj4ERERkUJR8CMiIiKFouBHRERECkXBj4iIiBSKgh8REREp\n",
       "FAU/FZjZGWbWY2Yvbfa5tBszm2Nm9zb7PESqMRjvBWb2GTPradTxBkuR3gfT58TMXtzsc4GhOZ/h\n",
       "/vya2XvN7G9m1jnQfQc1+Ike2GmDeT+DqKmzvprZeDP7lpmtNLPnzewpM7vFzD5gZts289xqZWY7\n",
       "AR8FvtiE+x5lZheZ2SNmtsHMlpjZsY3c38x2MLPPmtnPzewfyev/9Aad/0gz+76ZXWtmIxtxzOjY\n",
       "g/7YDGTbwXwcazQY7wXDZVbpljtPMzsmeU3kLYc3+/xSZnakmZ1nZjs3+1z60FLPr5ld0cdz22Nm\n",
       "e0WbzwM6gbMGej9q+anse8B27v5gM+7czI4H/gycBPwY+HfgY8CDwMXA15pxXg3wTsLrbmET7vsK\n",
       "4EPAlcAHgC3A/5jZUQ3cfwzwKWAS8MdkXUPeXNx9E+Hx+2fg5EYcM3IFg//YDGTbQXscazBY7wXW\n",
       "4OMNhqa+D1bha8CszPJAU8+o3JHAeUCrBj+t+Px+k97P6WnABmCpuz+abujuG4H5wIcHfC/uPmgL\n",
       "cAbQA0wbzPup4jx2aOb913C+44BngKXAHjm3jwfePxwfG+AeYH4THtPDk9fih6N1o4C/Arc2an/C\n",
       "t5Ddk98PS/Y5rcHXsgC4frg9Nq32ODZzAT4D9DT7PIbrAhyTvCbe3ujnBHhxA495TnLM/VrhfIbr\n",
       "ArwieSw+lnPbtOS2Vw/kmC3R8mNm+5jZd81sjZm9YGZ/MbMzM9vsZ2b/aWYrkqby9Wb2AzPbL7Nd\n",
       "2k86xcwWmNk/gN9nbhufNK09YWZPJve9XeY4ZX2hA9k32f4YM/tD0l11v5n9q1Xfzz8H2AF4l7uv\n",
       "yd7o7g+4+9eT+7nCzFbl3H+v+6r02JjZicn6V+Yc56zktqnJ3/0+V5WY2TjgIODmarZvsJOAzcDl\n",
       "6QoP3xq+A8wws30asb+7d7v72mSTwfpm/0Pg9Wa2fYOON9iPzb4Dva9GP45mdnDyOn5ztO6wZN2d\n",
       "mW1/ZmZLor975UUM8L3kFWZ2R/xe0Md5Hprc/1Nm9oyZ3WxmRzTiOirc345m9lUzW538P68xs1+Y\n",
       "2aF5125mY62PLonMsWt+rxgAS65hmwYec0zy2fKUhc+Zr5rZqMyd9vt5ZGafAeYmf66KHqf0sdzH\n",
       "zL5jofv3BQvpDf9pvbu0d6nmMyerv+c22aam53eIntvYqYSW3wXZG9z9LuAfwFsHcsBGvmBqYmZ7\n",
       "AEsITd+XAuuA44DvmNlO7p527/wTMINw8Q8RWkfeB/zWzKa6+/OZQ/83cB/wcXq/ef4AWEnoRjoM\n",
       "eDewNvm7P/3um7y4fg48DHya8Dh/Orm2apru3ww84O59vnFFKh2z0vrsY/NT4FngX4D/zWx7MvAX\n",
       "d182gOeqkiOTn3f1s11FyZvc6Co3f9yTrwbAocB97v5sZps7kp+HEJ6vSurdv5F+lvw8nvBcAi39\n",
       "2LyM8D/biPuq1V+AJ4FXAj9J1h1N+MZ4sJnt6O7PmFkH4X3mW1Uet8/3AzM7CPgFsIbQ/TES+Gyy\n",
       "Tdn/p5kdQPii9iRwESFIPIvwHvcqd799EK7jm8CJwNeBZcBuwFHAZODunO3XErohYp3AJcDG6Fqq\n",
       "eq+o4zWbmge8CNhiZr8HznX3O3vvOiA/AFYRnsMZhK7ZXYA456yaz6PrgAnAKcAHgfXJvuvNbG/g\n",
       "dmAnwheB5cC+hOdiO2BT5nxq+bwa6HMLVTy/Q/jckhxnJOGz6Vav3D13F+HaqjfITVVn0E+3F/Bt\n",
       "wotnl8z6BcATwLbJ39vm7HtEcvxZ0brPJOuuytk+ve2/MuuvA9ZVOPeX1rDvDYRuqz2jdeMJL+gt\n",
       "/TxmOyX388MqH+MrgFWVrrXC9ec9NlcDjwEd0bo9CW/AnxzIc9XHuX4+uf/tc257EXA+IUj8AuFN\n",
       "e1Ry2/bRdsckx6hmeWm031+AX+bc79Rk2/f0c+4D3p/wBjko3TXAj4DvZ9a1/GPTzMeRECwsif6+\n",
       "Drg2+b98fbLu0OS+3hRtd0bOY5b+L/X5fgBcDzwH7Butm5z8X23J7Hs98DwwNvM/+BTw23qvo8Jj\n",
       "8iRwaR+397r2nG0uS+77VdG6/t4r0v/tWl+zMwhBwRnAmwiDKNYR8kIOqfH1kT6n12fW/79k/UHR\n",
       "umo/j87Jnnuyfn7ymPX12VjVa6zW57bW53ewn9uc+39Tss1ZfWzzLeC5gTzfTW35MTMjRKbfB0aY\n",
       "2W7Rzb8A3kHoz1vk7i9E+40kBAkPEJ7gQ4GrMof/Zh93nb3tFuBtZvYi7/2NdED7mtkI4FjgOnd/\n",
       "LN3I3R8ws58Rvq33Zafk5zP9bFePvMfmGsI3lGOAXyfrTiIkJ18zkOeqj/vdFdjs7hvilRZGgP0G\n",
       "+Jy7/zhZ9y1CMHQu4VvTF5LN/0h4fKsRdxluR/TNNPJCdHtf6t2/0X4MfM3MRnnoNoLh8dg083G8\n",
       "Bfi8mW3n4Zv5UcAngP0IrSc3JT892bYaFd8PCIHM6wkfpmnLF+6+3MxuAt6QrkveN14H/MjdV0fb\n",
       "PmZmC4D3RO9PjbyOJ4DpZraXR4mk1TKz0wgtHh92998l66p5rziM8F5R02vW3RcDi6PbbjSza4E/\n",
       "ARcCbxzotUQuy/z9deDfCK0bf07uf6CfR0TbdwAnAD/x0GXTn1o/r+p6bpNzLXt+h+K5zXEq0E0I\n",
       "dit5AtjOzLaNn5u+NLvbawwhC/4s8oeqebINSR/nx4Ezgb0p78rKy6TvlQcTyTadPZH83IXQ/dOX\n",
       "/vbdHdgWuD9n3/vpP3/h6eTnjv1sV4+8x+bnhG+YJ1MKfk4G7nb3+81sd6p8rmpwCaH16sfRut8A\n",
       "XzWzjxGi/nAn7k9G5zcQzxMSa7O2jW4fzP0bxsx2IHyojkp+3gDD5rFp5uP4e8J73gwze5jwv/q/\n",
       "wAGEYIHk59LksaxGX+8HLyJc119z9ltB+Qf0GELgtyJn2+WELyEvAe5t8HXMIbRC/N1CztD/AN9z\n",
       "977ePwEws0MIH8wL3P2rmWup6r2ijtds74OGL5g/Bt5uZuZJk0ANss/XSpKk5XRFDZ9HsTGE9/e/\n",
       "VHk+tX5e1fzcQsXnd0if2+RLxFuBm9z9ib42je6/Ks0OftKE6ysJT1KePyc/v05ooruEEPE/laz/\n",
       "PvlD9vt6E91SYX01iZX17Nsvd3/azB4BDqx2lwrrR/SxT6/Hxt27zexHhG8U7wP2IuTofDzZZCDP\n",
       "VSWPA9uY2Q7u/hyAhZoNswl5TrH1hDf1switUiTbjyS0IFVjrbungdOjhDeprLRmxCP9HKve/RvC\n",
       "QjGvBYQm8dGEb2E3JLcNh8emmY/jHwgtTK8C/k54DO43s1uAf0se26MJ3QrVGtT3gwoadh3u/t9J\n",
       "rszbCC1P5wIfNbO3u/vPK+1nZrskx19OyEGJVf1eUcdrtpKHCDkqO9D/F9lq5b3HDvTzqB41vcZq\n",
       "fW6hz+d3qJ/bEwhfCq7uZ/9dCN1eea3KuZod/KwjdO9s4+79RYgnAVe4+7npCguF/nYZxPOrxVrC\n",
       "G9OEnNv2p7rI9EbgX81suvef9PwE+Ull++Ws6881hKS+Ywk5GEYp8BjIc1XJ8uTnOErfeg4nBGrZ\n",
       "ROvNyc9dM99UjqL6bxNjKX1ruhs4Jk0IjbZJR9L8kb7Vu3/dkubyq4Efu/vdZvZD4ItmNsLdtzA8\n",
       "HpumPY5JgH87ITB4kNJr7veE1qiZlFpRGmEd4YvGxJzbJlH+XpDmq0zO2XYyoeXh79D460i6578B\n",
       "fMPMxhCSRz9JaA3uJXod7gS8JqebYSDvFbW+ZivpAp6vIn2hLxOBv0V/70/40F8drav28yjv/X4d\n",
       "oYX/oDrOsSoDfW6h3+d3qJ/bmcn93dDP/uMIraJVa+pQ9+QN+zrgxGSkQ5nkyUptpvf5vj9nXVMl\n",
       "13QzcIJFlSjNbH+q74eeS0iS/HbS3VTGwvDaDyR/PgDsnIwqSW/fixDtD7TZ91eEIYMnJ8tt7v63\n",
       "6Lqqfa4qSfvoXx6tGwE86b1H66Xfdv5fZn3aj1zNEvcjX5vc19ZhxhaGr55JSB59OFq/nZlNNrNd\n",
       "a9l/EH2T8Fh9N/n7x4Q3qLRffTg8Ns1+HH9PCLRenfyOu68nvHF+lPA/8/tG3FHyP3MT4b3gJel6\n",
       "M5tC6K7MbvsL4K1WPlx6D0LOw+8zH+h1X4eZdVim8rC7ryO0zvU1XcB5hJaEU9L3h5xrqfa9oqbX\n",
       "bN77jZm9DHgL4XGsx9mZv9+f/PxZtK7az6Pnkp9bg6KkheNHwJvN7LD6TjVfHc8t9PH8DsVzmznW\n",
       "sYScuf7yePrLN+1lqFp+3mVmx+Ws/yphuN6rgdvM7L8I/7wvJlzMayk1m90IzDazp5JtZiS3P07r\n",
       "VUr9DOHFc6uZfYPwOJ9NaO04uL+d3X2lmZ1KaHW518y+Ryh42EnoijqJMMQTQqXkLwLXm9mlhObe\n",
       "9xJyBwY0rYi7b0paE04Btgc+ktmk2ueqr+v6C+EFnZ7/7wg5kmOSf04s1JxIg7dtzOwgd08TDWvq\n",
       "R3b3283sv4ELk4DyAUIr10sJH7yxI5L7+GyyDGh/M/t3Qmtc2r3zFivViLnU3Z+Otu0Bfufur+7r\n",
       "/M3si4THOS0XgLuvMbNbCa+Hm4bDY9Psx5EQEHySkD8TBwf/S+hiXeXujex6O4+Q2Px7M/tPwlD3\n",
       "fyf/veA/CNW7b0m23ZKc00hC/kajr2Mn4KHk+fgToZvoWMLoutyKucmXrE8R/m/3NLOyYdHunib6\n",
       "VvVeUUdeyDVmtoHwhWotoaX6X5NrKBsCPoDXRmpskjt0E+FzZiZwdfoelKj28+gPyc8LzOwawqip\n",
       "GwgJ6q8Dfmdm6VD3vQj/y0fFr+0aDfi5haqf38F+blMnE74o9dnllQSQuxC+DFbPaxgSWO1CeFPr\n",
       "IfwTZ4e2bQH2TrYbQ+hD/RthJMgjhOj9XdGxdiYUQltLaDL8H0Lz5Crgu9F25yXH7lUVs9JthL7b\n",
       "LZQPpSxbN5B9k/WvBu4kdIH9ldBvejGwYQCP3/6EIXwrk+M8DdxK+IbRGW13LOEF/gKhnsMp6flW\n",
       "c/2ZbV6bPD+b0+cnc3u/z1U/1/TB5Dq2zdzn1YQ3//8A3kMIGL9GGFZ5coNej6MIrWqPELojlgD/\n",
       "nLPdMcnj9Oka91+VeZ1viX6PX2MvStZf3c9575wcc2zObf9CyI/qs8xAqzw2zXwco203EUblWLT+\n",
       "1OQYV+Tsc0bOfQ7kveRoQh2j9L3gPeT8fybbHkJoYXia8IF1M3BEI64j5xgjCfWE7ibkrDxD6BY5\n",
       "q9L1UBq+nPuenjl+Xe8V/Zz7+5PXzXrCSKCHCDkoXTmPU7WvjfQ5nUQYWfRUcvyvEb3fRv+T/X4e\n",
       "Jdt+ktBluTnzWL6EUKpkDeH/4K+EujkjB/oaq+W5ref5HcznNrqPRYSWKutnuy+SU+6lv8WSnWUI\n",
       "JAnFU9x9UrPPpVksDGtfCczxUvdNISWtoT8BDnb3pc0+n+FKj6NUotdGe0u6zFcDX/Bk1oNqtVS+\n",
       "TMrMzrZQkvt5CzM+v7yPbY+x3mW4t+Tlygwl613ifgKhTsRvm3JCLcJDc+5cQvGvojsGWKg35bod\n",
       "gx5HyXcMem20szMJLU991fXL1XItP2Z2MqH58izgNsLsz/8HmORJTkhm+2MI/YoTKC8MuM6beHFm\n",
       "9ighr2UVYeTV+whNkYe6eyvNOiwiIlIorRj83EYYZfSB5G8j9Jd+3d0vytn+GELws4u7P5W9vVnM\n",
       "7LuEvJ89CZHpIuAT7j7oQ6JFRESksmbX+SljoTjXNOCCdJ27u5ndTMim78sfk/6/vwCfcfcBDXtr\n",
       "NHd/ZzPvX0RERPK1Ws7PboShbdkx/2sJLSh5HiF0kb2dUO3274SZdQ8drJMUERGR4aulWn5q4e73\n",
       "AfdFqxab2XhCrtBp2e2T4myvJ2SIVzUBmoiIiABhvrqxhNpijzf5XGrWasHPekJ9gT0y6/cgjPev\n",
       "1h2E0tp5Xk//84SIiIhIZTMJ8wwOSy0V/HiYs+ZOQtG+dLLGDkIRvEsHcKhDqDxB4urwY+LZsPAy\n",
       "+M0H4ZyGlLNvQZcQWsDana6zvRTlOqE416rrbB+TCQ0Iq5t8HnVpqeAn8RVgvpn9gdCC80HCrK7z\n",
       "AMzsQkLl4dOTvz9IKJq3jNAc925CbYfXVTh+0tU17q8ht3raSveP3DVI19JUZvaUu7fltcV0ne2l\n",
       "KNcJxblWXWf7CAOwgWGeNtJywY+7/yCZ0OxzhCTnu4E3RDV+9iSUBU+NBL4M7EOYFfke4Fh3/13f\n",
       "97QlHePfavOCiYiIyCBqueAHwN0vAy6rcFt2osSLCXNmDVBPaxU4EhERkSHRakPdm0EtPyIiIgVS\n",
       "4OCnpwjdXgubfQJDRNfZXopynVCca9V1SkspcvCT/tK2wY+7F+IfUdfZXopynVCca9V1SqspcPCj\n",
       "hGcREZEiKnDwo4RnERGRIipw8LOVWn5EREQKpMDBj7q9REREiqjAwU8hRnuJiIhIRoGDny3NPgER\n",
       "ERFpggIHP1up5UdERKRAChz8KOdHRESkiAoc/CjnR0REpIgKHPyo5UdERKSIFPyIiIhIoRQ4+NlK\n",
       "LT8iIiIFUuDgZ+tQdwU/IiIiBVLk4Ec5PyIiIgWk4EfBj4iISKEUOPjZrIRnERGRAipw8LOVWn5E\n",
       "REQKpMDBj7q9REREiqjAwc/m9BcFPyIiIgVS4OBHRQ5FRESKqMDBz1Zq+RERESmQAgc/m5XzIyIi\n",
       "UkAKfhT8iIiIFEqBgx+N9hIRESmiAgc/m/vfRERERNpOgYOfrdTyIyIiUiAKfhT8iIiIFErRgx9H\n",
       "wY+IiEihKPhR8CMiIlIoRQ9+REREpGCKHvyo5UdERKRgFPwo+BERESkUBT8KfkRERApFwY+IiIgU\n",
       "StGDH1DLj4iISKEUPfhRt5eIiEjBKPhR8CMiIlIoCn4U/IiIiBSKgh8REREplKIHP6CWHxERkUIp\n",
       "evCjbi8REZGCUfCj4EdERKRQFPwo+BERESmUogc/IiIiUjBFD37U8iMiIlIwCn4U/IiIiBSKgh8F\n",
       "PyIiIoWi4EdEREQKpSWDHzM728xWm9nzZrbEzF5e5X5HmdlmM7t7IHdX42mKiIjIMNRywY+ZnQx8\n",
       "GTgPOBS4B7jJzMb0s99o4HvAzVTfoqNuLxERkYJpueAH+DBwubvPd/flwHuBDcA7+9nvm8BVwGKq\n",
       "D2gU/IiIiBRMSwU/ZtYJTCO03gDg7p78PaOP/c4ExgKfZWDBjIIfERGRgtmm2SeQsRswAliTWb8W\n",
       "mJy3g5lNAC4EXuHuPWYDimWU8CwiIlIwLdXyM1BmNgJYAJzn7vcPbO89zoM3vBhe9SGzSUvNxs3r\n",
       "L69IREREhr9Wa/lZD2wB9sis3wN4NGf7HYHDgEPM7P8l6zoAM7NNwD+7+2/z72qvt0An0Lkb7LQb\n",
       "PDEVnn2jmR3k7uvqvxQREZHhy8xOAU7JrN65GefSaC0V/Lh7t5ndCRwL3ABgZh3Aa4FLc3Z5Cjgw\n",
       "s+5s4DXAicDqyvf2HUJ6UWzR7jBzLnDmwM9eRESkfbj7QmBhvM7MpgF3NueMGqelgp/EV4D5ZvYH\n",
       "4A7gg8B2wDwAM7sQ2NvdT0+SoZfFO5vZOuAFd1/GgE036Dy8vtMXERGRVtZywY+7/yDJvfkcsCdw\n",
       "N/CGqCtqT+AlfR2CmhOZO4BRLfeYiIiISONYaDwpjlKT3Z307vbqAaYsc19xwNCfmYiISGuLur0O\n",
       "c/e7mn0+tRrWo70ab4lD9+3NPgsREREZPAUOfu7x0NID4ecih9krYfWcZp6ViIiIDK4C57d8/Cfw\n",
       "7Rkwehe4/77Q4rN6joa5i4iItLcCBz9rPguPHUmYRPVAd1V7FhERKYICd3sBYdqMTtqkaJOIiIj0\n",
       "r+jBTzqHWLaitIiIiLQpBT+Bgh8REZGCKHrwszb5uXtTz0JERESGTNGDnyeATajlR0REpDAKHfwk\n",
       "I7zWopYfERGRwih08JNYi1p+RERECkPBT0h6VvAjIiJSEAp+QvCjbi8REZGCUPCjbi8REZFCUfCj\n",
       "lh8REZFCUfATWn52NGO7Zp+IiIiIDD4FP6ryLCIiUigKfkrBj7q+RERECkDBT2mKC7X8iIiIFICC\n",
       "H1gHOGr5ERERKYTCBz/ubAYeRy0/IiIihVD44CehWj8iIiIFoeAnUK0fERGRglDwE2h+LxERkYJQ\n",
       "8BOsRS0/IiIihaDgJ1DLj4iISEEo+AnWAruasU2zT0REREQGV+E/7M1sDLzmZNjW4NHlZs9thO7b\n",
       "YfUcd1/X7PMTERGRxip08GNmu0PXIrhgPBwB2HjoAW6bArOONrMZCoBERETaS8G7vcZeBFd1wXTA\n",
       "knUdwAyDK7tg7NwmnpyIiIgMgoIHP52Hw3TLv226hdtFRESknRQ8+Bm1TanFJ6sjuV1ERETaScGD\n",
       "n42bw5ymeXqS20VERKSdFDz46b4dlkTRzzrgXOB44PWA7202bl4YESYiIiLtoODBz+o5MGslLHJ4\n",
       "DDgZOBG4EfglsHw0LDgduhYrABIREWkPhc5pcfd1ZjYDZs4FOwGuGh1GfkFoBZoLLDPoGg8j7jMb\n",
       "9yPV/xERERneCt7yEwIg91VnwshHYEaydi1qBRIREWlPhQ9+SuKRXxcDX0D1f0RERNqPgp+t4pFf\n",
       "ywgVn/Oo/o+IiMhwpuBnq3jk1whU/0dERKQ9KfjZKh75tRnV/xEREWlPCn4SYQTXyhkwcz4sfxIW\n",
       "V9hyiYdWIhERERmOFPxESiO/Vk2E2Q+EVqCe5NYewt+zV4ZWIhERERmOlLuSo7z+z6gjYMJkWLcG\n",
       "1vxcdX5ERESGNwU/FSQBzpkAZvwOeNw9/C0iIiLDl7q9qvM74JVmFYeAiYiIyDChlp9+hGrOrzoc\n",
       "dtgV1qw0e2ZDSHhW95eIiMhwVHPwY2b/BJwOHAXsCmwBNgGrCXNCXOnuTzbgHJvGzHaHrkXwxfGh\n",
       "6KGNDYnPt02BWUeb2QwFQCIiIsPLgLu9zOzFZnY58BZgAXCEu+/n7l3uPgmYCTwKfMXM3tfY0x1q\n",
       "Yy+Cq7o0zYWIiEj7GFDLTzKh5/uBD7v7s3nbuPt64FrgWjObbmZnu/tl9Z9qM3QeHqazSG2d6R0Y\n",
       "YdB5itk41AUmIiIyfAy026vH3T8NYGaTgHHAE8A97v5CdmN3X2Jm99d/ms0ST3a6FngHYcLTuYT1\n",
       "PaPgttPVBSYiIjJ8DKjby90fBzCzrwCXAB8Dfg6sN7P/MrMDc/ZZP9CTMrOzzWy1mT1vZkvM7OV9\n",
       "bPsKM7vVzNab2QYzu9fMPjTQ+8wXT3aqmd5FRETaQa1D3e9x9+Pc/RhCsvN3gYeA683svHpOyMxO\n",
       "Br4MnAccCtwD3JR0ueV5FrgUOBqYDJwPfN7MzqrnPIJ4slPN9C4iItIOag1+9jKzqQDu3gM87O6f\n",
       "BSYBa8zs/9ZxTh8GLnf3+e6+HHgvsAF4Z97G7v5Hd7/G3e919wfd/WrgJuDIOs4hEU92qpneRURE\n",
       "2kGtwc/Xgblm9kszOwXohBAIufs3gedrOaiZdQLTgJvTde7uyd8zqjzGoYTA55e1nEOsfLLTFRs1\n",
       "07uIiMjwV1Nrhbs/Z2ZvBt5F6Gba18xeQ+ii2kDoCqvFboQmljWZ9WsJXVoVmdlDyf4jgc+5+1U1\n",
       "nkOZdJqLMKpryekhxwfKR351A7632bh5GvklIiLS2mruqklaZL5tZt8B/gl4ObAj8ABwfWNOb0CO\n",
       "Al5EaCG62Mwec/dv9bH9JWb2VGbdQndfmL/56jkw6+iQ3NxlcCqZkV+jNfJLRETaRdKzc0pm9c7N\n",
       "OJdGsxDDtIak2+s54ER3vyFaPx/Yyd3fVuVxPgmc4e4Tcm6bBtwJHObudw3w/MaEUV12Alw1Oj+t\n",
       "aJHDzPnuqzQJqoiItJV6PkNbyYByfsxskoX+n4Hs84Zqt3X3bsKDemy0fwfwWmDxAO52BIMwaau7\n",
       "rwtBzchHKqcgaeSXiIhIKxtonZ8VwJvM7FQz63OGczPbw8w+Czw2wHP6CvAeMzvNzKYA3wC2A+Yl\n",
       "x70waQlK7+dsM3uTmU1IlncBHwGuHOD9DkBc/BBC/s+5wPHACYDtbzZuXh/D80VERKRJBpzz4+5f\n",
       "N7PXAzckScZ3EBKSnwd2AV5KyL95DDjf3R8d4PF/kAQNnwP2BO4G3hDl0OwJvCTaxYALCdWmNwP3\n",
       "A3OAywd6bdVLix8aFSo/dyr/R0REpDXVlPNjZu9y9++Y2cGELql9CMnG64B7gZ+5+xMNPdMGaUR/\n",
       "ZRjVtSAZ+XUucCKh8nOW8n9ERKR9tEvOT62jvT5mZi8CfuHulzTyhIaHeOTXUgstPnmU/yMiItJq\n",
       "ak0KfgZ4C3C3mT1oZt82s/9jZrs08NxaVnnxw793V678/DjQs4/ZpKVmB68IP5ULJCIi0ky1tvxc\n",
       "5+4XmNn2wDHA64DPAAvN7L+B05ORW22rVPxw0uHgU0sBUFr88G7gYeDKneGInZNcIOC2KfCOV5nt\n",
       "twS2fVlInt64OcwjpgKJIiIig63Wlp9vA7j7Bnf/H3f/oLsfAOwHrAY+2aDzGwbiyU/XAicTcoAO\n",
       "IQxQy84CP95gn3FwzSmwfCr8aSLcOzXkEHUtVquQiIjI4Kop+HH37PQT6fqH3f3jhJnWCyKe/HQu\n",
       "YdTXdELed94s8BcTJq3PBkUzLOQQja2UQCQiIiIN0NBCgGY20swuBP7RyOO2svL8n59sLAU8lWaB\n",
       "X0b+yDBGhYBiAAAgAElEQVRQgrSIiMjga3QV5FHALGDfBh+3pZUqP4/6Wyng2UL+LPCVgiIIT8eo\n",
       "mudbExERkf419IPW3Z+lvABhwcTFD6cCt9G7lScNivICoJ7kGCIiIjJYGj7/VbHFyc9zgE8QpiTr\n",
       "IYwCOwf4G5WnKVvi4RgiIiIyWBT8NFSc/LwrcA1wHXA0MMPhJODXwH8AiwhBEcnPRQ6zV4ZjiIiI\n",
       "yGBRfkkDufs6M5sBM+eGxOW0hs/GkbBw/1IX2DWEkWEXAN3A2k3w9NWq8yMiIjL4FPw0WFr8MF5n\n",
       "NmlpGMmVGkMY8g6h1ee4Drjnne65GdIiIiLSQOr2GhKjtul7hFfnCOCAITwhERGRwqqr5cfMXgmc\n",
       "BXQBJ7n7w2Z2GrDS3W9pxAm2h3gUWNYa4EFg+i/NNjwdtn3unjAkXtNfiIiINFrNwY+ZnQhcBVwN\n",
       "TCPU+AHYmTDM6bi6z65tdN8OS6aEKs6xtcDbgG8CR+wJtic8Brx9KnyFUDAxnhNs1tFmNkMBkIiI\n",
       "SO3q6fb6FPBed383IWs3dStwWF1n1XbiUWDxCK9zgS9RPtXFlwmBT/XTX5jZmDBbvGaPFxER6U89\n",
       "3V4Tgd/lrH8KGF3HcdtO5VFgvjdcET1W64DfEEaC5ek9/YWZ7Q5di+Dq8WopEhER6V89wc9jwATC\n",
       "LO6xo4CVdRy3LeWPAjt4BVgS/KwF3gHswsCmvxh7EVzVVV5JOm4pmjk3e78iIiJFVk/w81/AV83s\n",
       "ncnf+5jZkYR+m8/XfWaFECdCX0yYEf7z9J0c/ezuYeh8xyjYuCt07lQ+jD6WP1Fq6A4bm2mFUkK1\n",
       "iIgUQz3BzxcJTQy/ArYndIFtBL7k7pc24NwKIE6EXkbo7srOCbYuWX838BCwYDSMGw2nEIKlLzCQ\n",
       "liJ1k4mISNHVnPDswQWEeRwOAmYAu7v7pxp1cu0vToROZ3uP5wR7DDgZOBE4BLiCEBR9iRD0TKfy\n",
       "7PGQP1Fq3E1WXUK1iIhIO6m7yKG7b3T3pe5+m7s/04iTKorQwrJyBsycDys2hiBmDGH6ix8CbwDO\n",
       "JwQq9xJaaiC0EqW/py1FqXWEUWTHA68HfO/ykV+dhw+0m0xERKSd1FPn5xLymxwceAG4H/ixu/+j\n",
       "1vsogjQR2mwcsOT00AKTTn9xPKFBDUotQ9nf5xBahy4AxgGnElqF5ibb9IyG205Pu7TgoH6qTWcT\n",
       "qkVERNpLPR90hxL6YrYBVhA+UScS+mHuBf4N+LKZHe3uS+s90fa3eg7MOjp0PU23EIjEQU7avWWZ\n",
       "39OWornAL4H/pO+RX31Vm87rJhMREWkv9XR7XUdIdt7b3Q9z92nAPoRP4IXAvsD/Eir2ST/Ku8Cm\n",
       "LIOD7yt1hUF591a2qyttKdqHUktRVtql1X07LKmQJLTEw+0iIiLtq57g56PAp9396XSFuz8FnAfM\n",
       "cffngM8B/1TfKRaHu69zX3Wm+4oD3P80CboXlgKVOBH6nOT3RZRXjO6m/y6t1XPg3Y+E48T7LnKY\n",
       "vTLcLiIi0r7q6fYaDewOZLu0xhDm94JQ7bmzjvsouGxX2DXARcCdwKObYNYG2MZh1OPQvTFUjPbR\n",
       "fXVphWrT998IX58NZ66GCVNg/Vp47Gew+iIYO9dskur/iIhI26on+Pkx8B0zOwdIu0oOJ/S//Cj6\n",
       "e0Ud91FolafFyA9IzMbNKyVNQ6lG0DLgWWDT/mYTn4BxL4INT8PG2+EbT8K+j4J9VPV/RESkCMy9\n",
       "Uo2YfnY025GQz3MaMDJZvQmYD3zY3Z81s0MA3P2PDTjXhjCzaYSmk8Pc/a5mn08jheHsXYtDS1GX\n",
       "lUZ+jaNUFLEssHH4wFPwk+dgxi9hwem9Z56H0CU2c777Kk2TISJSYO3yGVpz8LP1ACEI6kr+XNnq\n",
       "tX7a5YmrpDR1hZ0AV42GIwl1f06kfBRYapHD9Qb/cx/8ZWLlLrMpy9xXHDCY5y4iIq2tXT5DG1Hk\n",
       "8Bl3vydZWjrwKYI0aRpGPlIa+RUXRcyankytsdMOqv8jIiJFUNcHmpkZMAV4KZnEZne/oZ5jS71G\n",
       "bZNfFDHrceARh417qv6PiIgUQT0VnruA6wnzeuWpu1VJ6hEXM4yLIsbWAu8AvmFw7YjyCVVjqv8j\n",
       "IiLto54A5WvAasJw9+eAA4FXAn8Ajqn3xKRecTHDbFHE1MWEaTGmE8o2pXWE1hDyhI4D/hk4YxP0\n",
       "jCrNDyYiIjJ81RP8zAA+5e7rCf0iW9z9FuBjhMBImiqeMb5SUcQ/UGrpSafJuJIQu54I/JRQsHt5\n",
       "J3z/HdC1WAGQiIgMd/Xk/IwgFI8BWA/sTajp8yAwuc7zkjr1rhHUMQpm7Rqe8rQoYue+YDuV9hoD\n",
       "7ADMI39+sEu74OxbzSZtUhFEEREZruoJfpYCBwMrCUUO55hZN3BWsk6aLJ0xvtLtZpOWgk8tzwVa\n",
       "RiiMmEoLJd4NPGzw/QkqgigiIsNZPd1en4/2/zShkt7vgTcCH6jzvGRI5E1yGo8MWwucTOgCO4RS\n",
       "i1B6ezxj/Ni5ZJjZGLNx88wmLTU7eEX4OW6eus5ERKSZagp+zKyTkCH7FwB3/6u7Tyb0m+zh7r9q\n",
       "3CnK4InzgtJcoM2UZpK/mFAVejpwL33XCuo8PF5jZruHatMLz4DlU+FPE+HeqaGKtHKHRESkeWoK\n",
       "fty9mzDE3TPrH3f3nvy9pNWEbqqVM2DmfJiyDA6+D5Y/GUZ8QXlxxGytoHWEEWHHAycAtn95q87Y\n",
       "i+CqroG0FImIiAyFerq9rgbe1agTkeZIK0K7rzjA/U+TYNVEmP1AaA2KA560VhCUd4fdCNwALOss\n",
       "b9XpPDy0COXp3VIkIiIyVOod7fVvZnYsYZ6P55L1Bri7f7jek5Ohlxkldgr4qPCUprWCplPeHZaK\n",
       "W3Vmzi2vMJ2l6TJERKR56vkAOghIJzWbGK03Mt1hMryko8TMxgFLkpne5xBaey4gDPTLGxG2DNhs\n",
       "0DEbNlrl6TLWAM/uHkabaci8iIgMrZqDH3c/poHnIS1p9RyYdXRozZluoQjiRcCjlI8IewehJegc\n",
       "4BTg6hFwLfnTZawF3gYsGA1HjNaQeRERGWrqepCKehdKTFtpfG/wJHCJu8DOjX4fT6ml6AhCV1dP\n",
       "ss2XUBFFERFpFnOvvYfKzF5JKGrYBZzk7g+b2WnAymSqi5ZjZtMIOUqHuftd/W0vvZmNmxeSm2dY\n",
       "GO11IyEQin+HTHcYofZlRw8s76g8yeoXKC+i+HOHs5+FkY/AtqaASESkedrlM7Tm0V5mdiJwE/A8\n",
       "MA0Yldy0M2EiKWlbcX2geERYdjj8GELL0DxgE2HesEkd+UPmXwecT/nQ+PXAlwwW7ggrJqlWkIiI\n",
       "NEI9Q90/BbzX3d8NdEfrbwUOq+uspKWV1wdasbGU3x4Ph4/Fs8dXGjK/D2Gu3Ox+aTeaagWJiEhj\n",
       "1BP8TAR+l7P+KWB0HceVYSCtDwTdC0tTZKTD4bOWUcrxibeJg5tsq1G6X/VVpUVERKpRT/DzGDAh\n",
       "Z/1RaGLTAom7wM4h9HguojRdRg+hYTANbOYk2ywmDJlPg5u8VqO+qkq/Edg82WziE2YH3a95w0RE\n",
       "pFr1jPb6L+CrZvbO5O99zOxI4MuESU+lAHqPCOsYBbN2DS+tUY9D98by0WFjCEPm5xLi5zS4iYso\n",
       "ptKAyKgwpL4jGS4/WsPlRUSkWvUEPxcRWo5+BWxP6ALbCHzJ3S+t98TM7GzC1/w9gHuA97v7HRW2\n",
       "fTvwPuBlhMTrpcBn3P0X9Z6H9C8tiljp9jA6LC2WCKVE6OMpBTdxEcUuwnD4hwktREdSeUh9Km+4\n",
       "fMco2JgJxIZupFhohRqbKROgkWoiIk3n7nUthGDjAEL/xY71Hi855snAC8DpwGTgW8A/gDEVtr+E\n",
       "0BxwGKHAzAWEQOyQnG2nET5xpzXiXLVU9XyOga774dYe2OLgHn6e5nBr8rc7rHV4n8Nkh8UOaxxe\n",
       "nWzzRoeeZLvjot/jJd1+scNj0e89ybE/4nCMwwHdMGE5jJ1X6TXVgGvePVzz4uhctzgs6gnrB+d+\n",
       "tWjRomUwl3b5DK25zo+ZfQe4yt1/U9MB+j72bcBt7v6B5G8D/g583d0vqvIYfwGucffPZ9a3RY2C\n",
       "4Sa/FeS5e2DUDLhyv5DA3EGIYU+kNPIrrRX0K0qzqbyFMJlq1rnJvtMzv1eqIXSbh3yllQ3vJiuv\n",
       "hZS1yGHm/JAwLiIyfLTLZ2g93V67AT8zs3XA94Gr3f2P9Z6QmXUSIssL0nXu7mZ2M73HQlc6Rgew\n",
       "I/B4vecjjeEVusZCUFRWQXo/uHhUaYu8LrI4Fyi2jNKcY/Hv2YlYtxZfNOgaDyPuMxv3o8Z2R2lW\n",
       "exGRVlXzaC93fyuwNyG5+XDgTjNbamafMLOxdZzTboRhPmsy69cCe1Z5jHOAHYAf1HEeMgQ8GTLv\n",
       "vuIA9z9NglF/y58MNR4iX2lIfaWCi/GQ+bi20I3AL4HloxtfOFGz2ouItKp6hrrj7v9w98vd/VXA\n",
       "WGA+MBt4oAHnVhMzOxX4NPAv7r6+Wechtdq4Ob9QYjpE/lZ6D6lfl6x7gPyCi3EgVKlw4v4GrxgP\n",
       "E+4zO3hF/UPnK10HyTlv3FzbcUVEpF4N+fZpZiOBfyK0AI0jjGGu1XrCJ9cemfV7EKYT7+s83kEY\n",
       "gn+Su/+6n/u5xMyeyqxb6O4LB3Ky0mjdt8OSKb1zZcYQEpZn3g+jNpWG1GPQsT1cNTJslw6Xj4fO\n",
       "x91kcXdYKs4JuqJBQ+crXQeEopDdtw/seCIiQ8vMTiHUFYnt3Ixzabg6Mr4NeA3wbeAJ4Engu8Br\n",
       "SSZMrePYS4BLo787gIeAOX3scwqwAXhzETLV23Wh4siwW3NHSYURW4t6SqPFXu0hoTgd7XVrMspr\n",
       "cXKsN2dGiK11eFVm1Fm83NoDY+fVfh2LvJrr6L3v2HkwcSkctCL8HLyRaVq0aNFS7dIun6H1tPw8\n",
       "BOwK/Ax4D3Cju79Qx/FiXwHmm9kfgDuADwLbEWbIxMwuBPZ299OTv08ldLl9ALjDzNLcoA3u/nSD\n",
       "zkmGgPcqmthffZw4sTguoLgMGAnM3gL+LPxoe5g/Mswuny2c2EHlXPr85OT+aviE65h9LFz3AJzz\n",
       "OOw6Bh59CNbf3FditZntDl2L4OrxmZFpKuAoItIodUR//wqMrnDbgQ2ILs8GVhPq/SwGXh7dNg/4\n",
       "dfT3bwh9Gz2Z5bvtGrVqSZ/Pg1bkt9iky0Erkuc9aU0Z90SpleecpEUo2xqUf4zoNVRVDR/wtyTH\n",
       "2B/8QfCL+r+euCUru9TWCqVFixYtjVra5TO05pYfd788/tvMdiJ0Pb2LUGxwRK3HTo5/GXBZhdvO\n",
       "zPz96nruS4azNLE4b2RVKbHYk6H2ZjYHZi8Os8IvtdBKlB06v3UoPOFlvHG/ULcnbbEZexFc1dX/\n",
       "0PlfOXQ94M79ZqEuRv/XoyHyIiKDra7RXgBm9ioz+x4hGfkc4NeUzzsgMoi6by/NKp/VO7E4BC8r\n",
       "Z8DM+fD37hDwxEPns0PhbwDuHVU+FD4OULLbLwDeMhrGngEnnglH7RoCp+X3AtPMyqM0MxsTRpVN\n",
       "Wmp28Aro3F9D5EVEBlmNzV57AR8H/kpIdP4mIZnigGY3ZRWlyU7L1udzQAnS5ftOXFqa+iJNlI6T\n",
       "o7PLjT2w331w4MbSunOi7ePpNbLdYQc+Eu7Hu6Jzz+k+i6fxyC5bHCYubfZjrkWLluIu7fIZOuCW\n",
       "HzO7EXiQUHL3fGAvd39v8mBU+AYuMji8rCVnyjI4+L7wc+b8/qetSFuN0kTpHwI3USqICKFL61zg\n",
       "WOAcg+9PgJd2ll7qcQHFSjWEZhh8a89kiP200rHj7rN0+wPIL+AIjRoin21tqr+mkYjI8FJLE/px\n",
       "hBFYn/b+a+mIDDrvZ1b5ylbPgVlHh/yf6RaClxWUApG4/o8TYv1sDaFsJelsDaHUdIPPbSbk/Vwb\n",
       "1uXl96Sz26f31UHIXVriMHtlOOfaaTSZiEhtOT9HAX8GfmRmfzWzj5vZXg0+L5FBl99qtGJjqVUn\n",
       "bsm5l1ILT1ptejGlofNQHghlPQ484vDa96etLdC5b+/t01ao64GXAYfeD28GvrC4MROw5rU2pa1T\n",
       "V3aF4fsiIu1twMGPuy9293cT5vW6EHgT8DfCO//rzGzHxp6iyODxXnOLdS8sJVDHXVpxYBN3kz1C\n",
       "CIKgfEqNWJoUfflIuHkH+NVEePNU6Nkpf/sxwEXArk+73z0BfnoL3LimMS0yGk0mIlLPxKbPuvt3\n",
       "3f0o4CDgS8DHgHVm9pNGnaDI0Fo9B2atDMnPccCTDWzS2eZ/CfwHYc6xKeTn61wMXEBobVlHCIRO\n",
       "At5YYXuS9TstC3k4J24Px73V7GVxfs7k2vJ2shOupjlNxwMnALa/8n9EpO01OAt8G8I76A3NzuRu\n",
       "90x1LYO3UJpe4oXSyKt4VFd2SUeBdS2H8d1wi5ePPDvG848TjzLLjlR7tcPH35JfTPHGHujq7r/I\n",
       "Yt40GROeKO3T1+i0vkfKadGipZhLu3yGWnIxhWFm0yAUnHP3u5p9PtK6Qn2eBaeHfJi0xeYCQldY\n",
       "NhE55OPkT3vRuS/8eadw1OMJ9YCyBRXvJuQFPQ/ssBFevA38dSUs3L/35KjnAm8nf0qORR5ymFZ/\n",
       "tDyxeT2hK+3nwOXAkclxTiS/LFc4jvuqGhLJRaRdtctnaN1FDkXaV9wFtishz+c6wty9B26CySuy\n",
       "Q+o9k0PkvuIA6H6oclL0GEIQ0gN8g5BYfecouGkEjJtQnp+TdlH9nMp1RNO8nTixOe5q+zWhm24R\n",
       "sJTyYf15xxERaT+qFitSgQ94ktVKum+HJVNCC052Kg0oH1WW6gB2JH/Y/Qp65+2k03FsBjZPhs7J\n",
       "pcApe/x08tfHqDylxxagZx8zG1P9dTZHf5PMNvv8RKT1KPgR6YPXXEMoFtcTmmKlGkGpSvWB4kAp\n",
       "DmDi9XFQdA5her2rO8LfleoPpcnaxyfHWRcdYy6l2j9LdobZi1u59o/qFolILdTtJTLIvKye0PUr\n",
       "YNamMDqsJ9miUn2geM6xeNh9vD4Oir5E7wCpmuNXqkx9JK1f+0d1i0Rk4NTyIzIE4hak0E0zK+6m\n",
       "2Q98VO8AJa32/HnKA5h0/QWEvJ308z1u4YmrUOd1tcXHcfquTB1yf1qze0l1i0Rk4BT8iAyxbFda\n",
       "GFW25PTeo7rGECZanXk/jHppKUBKiyxm83YqBUhp/aFsknR6/I9uBhtZWp/N/7H9zfZdCF3T4eqx\n",
       "rdW9lK1bFOtIbhcRKaduL5Gmi0eVpV1hPYS/P7AS/nZUeeVpKOXt7EWpeyvu6oqrUP+REGvFXW3x\n",
       "8btXlvZLq1GfSBiS/x3gTZ0w+h1w5dhau5caPZlqeryk1azCVj2E1ikRkXL6ViTSZNWMKjOzzCSs\n",
       "aZ2h3QjTaxxJeVcXlAIkgJ+mLUibsscPgcuSiSGQifN/4mTqeymvK1Q2wsygY7bZxBNg1OPQvTFz\n",
       "7g1NSi4/3rXkt2pBCBa7b8/ZvwW770RkSDW7yuJQL7RJdUotxVvIrdi8zwLoWhWqQj+WVGy+Nadi\n",
       "dOWKzeG4XfeH7d5YoRr1m6OK1nFl6Me8vyrR4ZwX9eRXx761B8bOG9jjEB+vryrZva8Z2D2/araq\n",
       "WmvRUs3SLp+hqvAsMsyVt2R0jIKNu4ZG3d6tMP0f40Wnwp87w9q4GnX8e1wZOlslOm4Rehb4ezeM\n",
       "3AaWd+Tn5vQAU5aFYpBVX+cpsDxKEM/mKK0COn8Id703e83lVbuzVNVapD/t8hmqbi+RYc4bUIso\n",
       "PYbZpMPBp4bAIk6gjrvU4lFl8e95NYcWdJbXHMoKScn9dUWVd3Vljxd3760DXgd0HgUH3WI2KdOl\n",
       "pdFhIqLgR0TKVKpGHY8ei4Oi+Pc4X+hc8osyQu/5zJ6bAOMfg6s6KuUEldfzqTR0Pw2+vgEcsQfY\n",
       "Hr2Pc1BDRocpb0hkeNNoLxGJxCPP0iHyUD56bBX5I8ziQoyVijKmo8leTWk+s7dZCHzSkWTrgI8C\n",
       "5xt0jYcJ98HIE0otNvHxYhcTgrO+RqRt3Fzv6LCkFWoxLDwDlk+FP02Ee6eG7rSuxbWOYBORoaPg\n",
       "R0S28j6rUY8hzAw/jTDCDMoDkUotQnOATyT7zCW0CP2GUsvQvZQCpexQ+wXAW0bDDqPzjxcP3f8D\n",
       "5aO+0olgjwe+aCFXaOPI8pIBsfzRYb2pqrTIcKduLxEp431Xo94Mz90Dt8yAK/eDcyzk9pxPmFQ1\n",
       "7Y6Ku6bioow3EVpoPk8pV6hS11l2qH3e8c5P9l+xETo3gu0UjhPvOxdYD1w0ChZPgNOAKwhD99OS\n",
       "AUscZq8MLV8l+d1bI/dW3pDI8KbgR0QqqpRMHYKCtC5RxyiYtSuwPSzu7LvmUDojfRzwxIFSnEAd\n",
       "B0J91TBa5DBzIRAla1cKoi4mCYQILUjrHDoNtjwLvhtMuM9s8hPQvUs4ofHbw1Ujy3ORXk8j8oZE\n",
       "pHnU7SUiA+bu69xXnem+4gD3e/d3X7kLrNoXZj8QgpFzCF1TiyjvmnqGEOjEuUKVus7ivKFKXV2L\n",
       "ohab7ttLXVrxvtmJW8ckxxsFfM/gV8BeO8KCneH3o2HvcbBgNLx95xD4ZLu3Oqkmb6jRVa1FpHH0\n",
       "DUVEGsJ7VapOW4TimkMbR8KS/WGqlVpy4pFkcddZHAjldXXd1w0bF+RXwR5h5UFUNg2n0si0+PfP\n",
       "UwqgYlOBJZRXvE6FvKHeVa3T1qY7p8IOM80mroRNi6uvv6RRZSIN1ewqi6pOqUVLcRa2VpO+sae8\n",
       "MvNaD5OsHpBUqHaH46IqzNlli8PEpfnHHzsPJr5Q2vfNOfvHx670e95+HlWVvqViVenyKtRxReye\n",
       "6FqPcTigGyYsD9v3V4264n6Te1f+7n28zOPT77ZatOQt7fIZqpYfERkyvrV16N/nQscMOG1P6Ng2\n",
       "JCtvfBQ23AWzk2TqKTbQebt8a7HGccCS03vXK0pVGplWKRcJyitJjwTOADqeglHPQMe+sGkDbA9M\n",
       "/C14lBTdV/6RjYSeSXDbxN7znMWjyirt97OJ8IGZcPXI/uZNa/QcayLDWrOjL0WtWrRoiRe2tk50\n",
       "LYfx3X21sPR9jHS+so94aY6ydKmm5See2yzbepOey4090NXdu3XmkAr3FR8zXtY6nOYw4YlSi8yE\n",
       "JyrvtzZZ97KopSy7lM+b1ug51rQUc2mXz1C1/IhIS/F+h9r3n/PiZflHHTPgR10wf2RpePsUSnk7\n",
       "8Uiy+Pc4F+k6Sq03qQ7gt1ZKiq40ND+bxJ3mH8WVrh8G5gFXjAYb3XtUWaVpRJZRnntUNs+ZQecp\n",
       "oRVs9RyYqKk9RBIKfkSkZXkd85b1HUQ953D93nDli8prFaVzkp1PCGiuISQq/4zS0PpYNUPz4+6z\n",
       "NBCKAxiP7i8VjyrLBlDx/cTrK9U2uvMM2GEmmJd3/WUnhLX9w8SvpcBSCdfSrhT8iEjbywui8msV\n",
       "bQN0PAGzdikfpda5b6mAYqxSq07capROExIHQnEAkx1VlgYlDxOG9h9J5VpI8fo+axuNhDdG22YD\n",
       "JQN6OuG200vzoGHKEZJ2peBHRAppIK1KZpOWlgooxvJadaB8aP4fgZ8C36YUCMUBTKXWm3OT38+n\n",
       "PICKt49bmCq1QqXHPIBSV198eyqeomNmcqA04briNjW1yjWLWrIkpeBHRKRf8Wz3sbjmT3Z0WFyF\n",
       "eg0w40mwNSH/aNzI/FFl2aAkDqBuBL5DeS2kuIWpUitUeszx0bZL6V37KBXn/8Q5QnE32WaDjtlm\n",
       "E08ILWXdu5RazbozrWbdt8Pqi2DsR3tPkzIC2PZlQxGIaLSblGl2xrUy1bVo0dLqC2Wjx+KRZ+lo\n",
       "r0qjyrzXaKpwrEojufqqbfSYw7gnwhKP8EpHfh3o+TWK4mOm2x6aM9rsnGTbNztM2ggHPlW6PR7t\n",
       "9lgVv8cj3450GN9TPlLuUYcZ3nv03KJ+R/LV/hxqtFuD/hfa4jO06SegJ06LFi3DYaFygcDJAx2a\n",
       "X/5BnBZNXOSVCyumy0ErKgdip3l+gcj+ijxWKsI41fMDtGp+j4+ZFxRWGvLv3jtQbExRxrDvwIpm\n",
       "asn9P2iLz1B1e4mIVMH7zhEa4ND81dFUHNOtNKrsAXoXZEyFecO81zQicTfSLTkFIvOKPMa5QpUS\n",
       "pSE/n6ia3/tK6M5uC/lD9PfdFrqmw9Vjq+2m6iunBw7apvKEtI8DPfuE3C7lAhWBgh8RkQbpJ0Aq\n",
       "2y4/gEnnPsvmFkFc1brS/ZRGsMW1jeJk6VSaK3Q+5fk/cdAS5whVUxG7Us5RvD4Nch4lP8k7HqL/\n",
       "83fA5VSbcN33fGqjZsILI8it2J3WWbpyZzhi51rnYUvOQQnVw0Wzm57UZKdFixYtYaFil1b/Va3z\n",
       "j9VXd9yNPTD2aThwY353mHt+PtFA50VL18ddYdVU0q4m/2niUph8f5ILtbHU7ZeXo3Sah67F7O0f\n",
       "qXD/A8tFotdcbNXvO5yWdvkMVcuPiEiL8IotQgNvPfAqK2Unc5FN7T1cH8pHrOVVwa70e9zVlq6P\n",
       "q2RXM0Q/ey7ZlpoFo2Hc6FCU8guE7rUZOcc5l96j3eJzibvlsqPttnbHGew7Hvwhs4kb8ke4jdse\n",
       "ruxsp9IAba3Z0ZeiVi1atGhp3lKefF2ptSWeyT5tSbm1j9/j1pR032O8fNRZXpJ3pZakSi01catR\n",
       "NccZyMi4gY5w66ulqndCNWUtc/s/CRNfCCPsJiyvNal7aF4v7fEZqpYfEZFCi5Ov40Tp2BhC0DHz\n",
       "fhi1qXJF7PR3DH60fWk+tWuAt5BfBHIV5BaKrJSUHbfUVKp2XSkXKW3JWhGtq7RfXutRX79XaqmK\n",
       "pw/Zd0H4fZtpML4LLh0JX0qOcQRgo6BnJ7htomoPDS4FPyIiBeb9TgLbQ0i2/sBK+NtR1X4Y53S1\n",
       "7Qc+Kr8IZF6hyLiAY5yUXSmwqdTtljfaLa9bLrvfQEe4xftmK3VfDDzXCQ+fAlcA1wInUXnCXHWV\n",
       "DSYfqDgAABQCSURBVLaOZp+AiIg0l7uvc191pvsDk+GBfWDWFTBlGRx8X/g5cz6sHFArROmYKw5w\n",
       "/9Mk6F4YgqisOcBHgFspjUyDUuvQD4HH6N1Sk/19DvAJwnxo8XGmRr+Ts67SfgMd4RYfM2016iIE\n",
       "QScChxICn+nAvYSWnmX0LgOQiittS6Mp+BERka2yQUv4uerM+rtfVs+BWStDnk9Psq4H+KvDw6vg\n",
       "1IVw/QqYtSkEQj2EAOgiYGdKQU4cZMS/x8HSH4EzCMc5hxDcLKJ0v+cAH05u3zXZ7zpCADTLw/p0\n",
       "GhGoHHBVCr6WEoKauOssDXigFDRlu8piHYQWMxkMemBFRGTQeZ8j2R7cOpItf2RaXP8o7g47hzDa\n",
       "63xCgJEGS0scTlkNpy4Jc4fFOUrpnGPP3QOnkjO32EUw66NgJ8Di0XAk1Y1wi/OY0paqSvWO0qAp\n",
       "r0suFYpaNuChlxwKfkREZEh4FUUg87ZJAqLFvSti3wk8uglmbYBtvHwy1QfrKSx4ppnNgdnJfZ5j\n",
       "pSArDrgqBV+vJQQ1eQFPnGcUB09ZpaKW0ngKfkREpKX13Wo0ONWTe99nfyPc4laltKVqi+UnVqet\n",
       "V2mX3AWELrE4wXz2ytBVKIPBknH7hWFm0whfFw5z97uafT4iItJeQktV12J4xXh4L2Ek2zpK3XVH\n",
       "EOYTu4iQI7S2Bzo2Q+cLsPHRaqfTaIZ2+QxVy4+IiEgDlVqNNn4Nbj0R5neW6h1dRGjtWbcJuque\n",
       "N0waqyWDHzM7m1AcYQ/gHuD97n5HhW33BL4CHAbsD1zq7h8aqnMVERHJSoKZU/uaWkQBT/O0XPBj\n",
       "ZicDXwbOInSQfgi4ycwmVXihjCJUlPo8YexisfrxRESkZVWT5C1DrxXr/HwYuNzd57v7ckKH6Qbg\n",
       "nXkbu/vf3P2D7n4V8NQQnqeIiIgMQy0V/JhZJ2HStJvTdR4ysm+mNF2viIiISM1aKvgBdiMURliT\n",
       "Wb8W2HPoT0dERETaTasFPyIiIiKDqtUSntcTymDukVm/B/Bog+/rEjPL5ggtdPeFDb4fERGRYcfM\n",
       "TiGUsI7t3IxzabSWCn7cvdvM7gSOBW4AMLMOQq3wSxt8dx8azgWaREREBlPSGFDWIBAVORzWWir4\n",
       "SXwFmG9mfwDuAD4IbAfMAzCzC4G93f30dAczOyT5dUdg9+TvbndfNqRnLiIiIi2v5YIfd/9BKArF\n",
       "5whJzncDb4hq/OwJvCSzW9qC44TRYqcCq4GuQT9hERERGVZaLvgBcPfLgMsq3NarWJS7K3FbRERE\n",
       "qqKgQURERApFwY+IiIgUioIfERERKRQFPyIiIlIoCn5ERESkUBT8iIiISKEo+BEREZFCUfAjIiIi\n",
       "haLgR0RERApFwY+IiIgUioIfERERKRQFPyIiIlIoCn5ERESkUBT8iIiISKEo+BEREZFCUfAjIiIi\n",
       "haLgR0RERApFwY+IiIgUioIfERERKRQFPyIiIlIoCn5ERESkUBT8iIiISKEo+BEREZFCUfAjIiIi\n",
       "haLgR0RERApFwY+IiIgUioIfERERKRQFPyIiIlIoCn5ERESkUBT8iIiISKEo+BEREZFCUfAjIiIi\n",
       "haLgR0RERApFwY+IiIgUioIfERERKRQFPyIiIlIoCn5ERESkUBT8iIiISKEo+BEREZFCUfAjIiIi\n",
       "haLgR0RERApFwY+IiIgUioIfERERKRQFPyIiIlIoCn5ERESkUBT8iIiISKEo+BEREZFCUfAjIiIi\n",
       "haLgR0RERApFwY+IiIgUioIfERERKZSWDH7M7GwzW21mz5vZEjN7eT/bH2Nmd5nZC2b2VzM7fajO\n",
       "tZWZ2SnNPoehoOtsL0W5TijOteo6pdW0XPBjZicDXwbOAw4F7gFuMrMxFbYfB/wU+BXwMuCrwLfN\n",
       "7HVDc8YtrSj/iLrO9lKU64TiXKuuU1pKywU/wIeBy919vrsvB94LbADeWWH79wIPuPu57r7C3S8D\n",
       "rgU+NDSnKyIiIsNJSwU/ZtYJTANuTte5uyd/z6iw24x4+8Qv+theRERECqylgh9gN2AEsCazfi2w\n",
       "Z4V99sjZfg2wk5mNauzpiYiIyHC3TbNPoAm2TX5ONrOmnsgQ2NnMpjX7JIaArrO9FOU6oTjXquts\n",
       "H5OTn9v2uVWLa7XgZz2whdCaE9sDeLTCPo/Ru1VoD+Bpd9+Ys/3Y5OfVNZ7jcHNns09giOg620tR\n",
       "rhOKc626zvYyFljU7JOoVUsFP+7ebWZ3AscCNwCYWQfwWuDSCrstBo7LrPtnKj8pNwEzgdXAC3We\n",
       "soiISJFsSwh8bmryedTFQj5x6zCzfwHmA2cBdwAfBE4CJrv7OjO7ENjb3U9Pth8L/AW4DJgHvAb4\n",
       "GnCcu/9yyC9AREREWlpLtfwAuPsPkpo+nyN0Z90NvMHd1yWb7Am8JNp+tZkdD1wC/F/g78C7FPiI\n",
       "iIhInpZr+REREREZTK021F1ERERkUCn4ERERkUIpVPAz0AlTB/lcXmlmPzGzh82sx8zemrPN58zs\n",
       "ETPbYGa/NLP9M7dva2aXmdl6M3vGzK41s90z27zYzK42s6fM7Akz+7aZ7ZDZ5qVm9lMze87M1pjZ\n",
       "XDMbkdnmYDP7ffLYPWhm51Z5nR83szvM7Onk2Neb2cT/396ZB0tRXXH4+wlRSy3cDUnKfRdB3OMS\n",
       "i7gh7ksEoimMJUYpExdS7pq4kGhSRtQiGDVSStyNCRXXJLigEUQUBERFkcUFlAQDiKKAnPxx7sCl\n",
       "M/Ae84T3Xs/5qrp6uvvc2/c33dNz+tytbFol9ZE0Np17jqThko4ok8bl6L4k3b/9y6RV0lVJV768\n",
       "USaNWdrvSLonlfNzSeMk7VkmrfLnfvF6LpY0IB1Xa9eY0rWVdJ2kKUnHJElXVLFr9VqbjJnVxQL0\n",
       "wLu2n4YP0nQb8AmwaTOV5wi8UffxwGLg2MLxi4H/AscAHYEhwLvAWpnNrcA0oAs+Lchw4F+FfJ4E\n",
       "RgN7AwcAbwP3ZsfbAOPxboudUrlmAr/KbNrh4ykNBnZO3+VnwJmN0Pkk0Cul6wQ8hg8zsE6ZtAJH\n",
       "p/y2BbYD+gELgA5l0VhF897AZOA14MaSXc+rgHHAZtmyUZk0prQb4r/HO4G9gC3xoUa2KZNWYOPC\n",
       "tTwEf+4eVBaNKe0vgH8D3YAtgJOAucDPynQ9v45llWbekhZgJHBLti3gA+DiFlC2ZZyfVLYZQN/C\n",
       "TTIf6JG21we+BE7MbHZMee2btndO23tkNl3xgSTbp+1uwCIyJxAfZmA20DZt98EHoGyb2VwHvFmD\n",
       "1k1SmQ6sA62zgNPLqBFYD5iIDy3xLMn5KYtW3PkZs5xjpdCY7K4Hhq3geGm0FnTdBLxdNo3Ao8Ad\n",
       "hX2PAIPLprWpS11Ue6m2CVObk63xUarz8s7FHbhKefcEvlGwmQi8B3w37doPmG1mo7O8nybdxJnN\n",
       "OFs6lAD4xLDtgA6ZzfNmtqhgs6Ok9VdS2wZp/Ulal06rpDaSegJrAS+UUSM+rtZjZvYM/kCtUCat\n",
       "28urpd9N1UKVITbKpPFY4FVJD6dqidGSemfHy6QVWPJ/8CNgUAk1PgkcKmn7pHU3PCrzZAm1Nom6\n",
       "cH6obcLU5qRSpmoTtn4zs1mQbtyiTfvMZmZ+MN1knxRsqp2HlbRpEPlo3Tfh4dNK+4nSaJXUUdI8\n",
       "vHr1dqC7mU2iRBoBkmPXGbi0UozscFm0voRXkXfF3063Bl6QtB7l0QiwDa5vInA4Xt1xi6RehfRl\n",
       "0FrheDy6cVchXavXaGYDgQeBiZIW4NVS/c3s/kL6Vq+1qbS4QQ6DFbKqZmJtKF9r4Hhj+T2wC3Bg\n",
       "I2xbo9a38Lrt9YGTgQckdWlCWWpllWlM0Y+bgUPNbEF2vobO2aq0mtlT2ebrkkbibSC649e5lrLU\n",
       "yqq8Z9cAXjazSqPYsZJ2Bc7G22HUWqZaWR3PojOAJ8zsoyaWpVZW5e/zXNxp7wlMAHYHbpI0w8zK\n",
       "ej1rol4iP7VMmNqcVH6U1cr7UWazpqR2DdgUW+i3BTYq2FQ7DwWbapPH5jYrJPWqOBL4vplNzw6V\n",
       "RquZLTSzyWY2xswuw0PJfVh6j7V6jXhIfFNgtKSFkhYCBwHnpjfN0lzPHDObgzfo3JZyXc/pwBuF\n",
       "fW/hjWXz9GXQiqQt8cbOf8x2l0nj5cC1ZvaQmU0ws3vw2Q8qUdoyaW0SdeH8pDfUyoSpwDITpo5o\n",
       "rnKtgCn4hc/L2w7Yh6XlfRVYWLDZEX9oVWxGABtI2iPL+2D8uo9M28OBjvIpRSocBsxh6UNxBHBQ\n",
       "urlzm7fSn8JySV1IBwDHAQeb2bSyaq1CG2ANMyuTxqHArsBuaekMvALckz6XSesSUnXX9sCMkl3P\n",
       "F/Herzk74D3AoHzX83S8WuXxbF+ZNAp/0c9ZzNIITJm0No1V3aK6pSx4uHo+S7td34b3xmmuru7r\n",
       "4n8WnfGb8/z0efN0/CK8/jTvjjgJWDPLYyD+kOqCv5FX6474BH4z590R78mOr4F36X0Kr7Lpij8c\n",
       "+mU27fC33bvxhmo9gHlA70boHIh3qzwI9/Ary9qZTavXivdQ+B4+23HHtL0Id/hKoXEF2p/D2xWU\n",
       "6XregN+zWwH7A/9MeW9cFo0p7V74kAyX4kM0nJLS/rBM1zPLfxrw6yrHyqLxdnx+yyPxe/cEvG3O\n",
       "dWXT2tRllWbe0hbgnHRBv8A9zr2bsSxdcKdnMe6pVz4PymyuTjfGfLwF/HaFPNYCBuBO3Dzgz8Bm\n",
       "BZsNgXvxsR5m4+HedQo2W+BvQp+lH8pv8YhFbtMReD6V5T3gwkbqLOqrLL0Kdq1aazrXlHRvfZw0\n",
       "HFImjSvQ/izZOD9l0ArcD3yYruf7wH3A1mXSmKU9Cv+Tmo+3Ezmjik2r14o36P6qWPaSaVwXd9yn\n",
       "AJ/jTs01ZF3Jy6K1qUtMbBoEQRAEQV1RF21+giAIgiAIKoTzEwRBEARBXRHOTxAEQRAEdUU4P0EQ\n",
       "BEEQ1BXh/ARBEARBUFeE8xMEQRAEQV0Rzk8QBEEQBHVFOD9BEARBENQV4fwEQVA3SOoiaXGVSRuD\n",
       "IKgjwvkJghIhaVNJt0qaJukLSTMkPSVp/4Ld7pIelDQ92U2V9KikozObrZKjUFnmSnpd0gBJ2zWi\n",
       "LIslHVvIq9PXr3q5539OUv/C7heB9mY2d3WVIwiClkc4P0FQLh7BZ1vvhc9Cfiw+8ejGFQNJxwEv\n",
       "Aesku53wSQf/CvSrEhU5BJ+MthNwGT4x8FhJB9dQPjVs0kAGy84AvVKY2UIzm9nUMgRB0LoJ5ycI\n",
       "SoKkDYADgYvNbJiZvW9mo8zsejN7NNmsC9wJPGpmx5jZUDObamYTzWyQmXWuEhWZZWYzk93fgEOB\n",
       "kcCdkhr7DJmc1mNSBOiZrNy9Jb0paX5a98mOVSJG3SUNkzQfOEXSRpLul/SBpM8kjZPUM0t3Fz4r\n",
       "+3kp/VeStqhW7SXpJEkTUgRsiqS+he91qqRLJQ1K0a9pks7Mjq+ZomHTk4apki5p5PcSBEEzEM5P\n",
       "EJSHeWk5QdKay7E5HNgIn125JsxnQ74Z2BLYo5HJ9knrShTpRABJp+IzTF+KR6AuA66V1KuQ/nqg\n",
       "f7L5B7A2MAo4EugA3A78SdLeyf5cYETa3x74FvBBsVCS9gQexGdt3xW4Kp3/tILpz4GXgc7AQOBW\n",
       "STtk5zoGOBnYATgVn1U7CIIWSs3h4yAIWhZmtkjSj4E7gLMljQaGAQ+Y2fhkVvnDnlhJlxyGZ7Ks\n",
       "eprZ4w2crpJ+K+CVRhTvP2k9q1DtdDXQ18yGpO1pkjoAZwGDM7v+mU2FG7PPAyR1BboDo8xsrqQF\n",
       "wOf5+aT/q3XrCww1s1+l7UmSdgEuBO5O+wx43Mz+kLZ/I+kCoAvwNrA58I6ZvZiOv7/cbyEIghZB\n",
       "RH6CoESY2V+Ab+NtfZ7C/6BHV4lk5IzF2wl1BtYF2jTiVBUvwmota6qC2wYYJOnTygJcnvbnvFJI\n",
       "20bSlZLGS5qV0nXFHZGVYSe8EXTOcGB7LespjSvYfARslj7fBXSWNFHSzZIOW8kyBEGwmonITxCU\n",
       "DDP7Ehialn6S7sAjLHcD7ySznfB2O5jZAlKbnCqRkeWxc1o3pXpnvbTuXSlLxleF7c8K2xfi1U3n\n",
       "AeOBz4GbgLVqKEdjRC8sbBvp5dHMxkjaGuiGt4d6SNJQMzu5hrIEQbAaiMhPEJSfN/GIDnh7mU+A\n",
       "i2vNLDVyPhd3mMY0MtmCtF4SVTKzj4HpwLZmNrmwTGsgvwOAIWZ2X6rSmwLsyLKRqAU0/IL3Zsqr\n",
       "mPfE1LapUZjZp2b2kJn9BOgBnJQaoAdB0AKJyE8QlARJGwMP4725xgOfAnvhUZIhAGY2T1Jv4EFJ\n",
       "jwG3AJPwKMwRKati1GUTSe3xrvG7AuenfI9aCQdhJjAf6CZpOvCFmc0BfgncImkO8Hc8crMXsIGZ\n",
       "FcfoyXkb+IGk/YDZeNudzQo2U4F9JW2JR45mVcnnd8AoSVcADwH7AecAfarY5iyJFqXeYdOB14DF\n",
       "eLujGWY2u4E8giBoJiLyEwTl4VN8/J4L8IbO44Fr8B5PP60YpYbD++NVRYOBt4Cn8fZBPao0dh6K\n",
       "/7mPA64DJgCdzGxYYwtmZovwaNFZwIf4mEKY2Z14tdfpKf/n8LGHJufJq2TZDxiNO0zPpvIVG0Tf\n",
       "gDtybwAfs7Q90JL8zGwM7qz0xL+vq4ArzWwwKyYv01zgIrz32cvAFngvtCAIWihaichuEARBEARB\n",
       "qyciP0EQBEEQ1BXh/ARBEARBUFeE8xMEQRAEQV0Rzk8QBEEQBHVFOD9BEARBENQV4fwEQRAEQVBX\n",
       "hPMTBEEQBEFdEc5PEARBEAR1RTg/QRAEQRDUFeH8BEEQBEFQV4TzEwRBEARBXRHOTxAEQRAEdcX/\n",
       "AMj3RX2DM2E7AAAAAElFTkSuQmCC\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f22c510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# Plot your best learning curve here\n",
    "counts, costs = zip(*traincurvebest)\n",
    "figure(figsize=(6,4))\n",
    "plot(5*array(counts), costs, color='b', marker='o', linestyle='-')\n",
    "title(r\"Learning Curve ($\\alpha$=%g, $\\lambda$=%g, window size=%d, batch size=%d)\" % (clf.alpha, clf.lreg, 5, 7))\n",
    "xlabel(\"SGD Iterations\"); ylabel(r\"Average $J(\\theta)$\"); \n",
    "ylim(ymin=0, ymax=max(1.1*max(costs),3*min(costs)));\n",
    "ylim(0,0.5)\n",
    "\n",
    "# Don't change this filename!\n",
    "savefig(\"ner.learningcurve.best.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.9314\n",
      "  [200]: mean loss 0.454136\n",
      "  [400]: mean loss 0.431805\n",
      "  [600]: mean loss 0.401252\n",
      "  [800]: mean loss 0.400494\n",
      "  [1000]: mean loss 0.401521\n",
      "  [1200]: mean loss 0.393017\n",
      "  [1400]: mean loss 0.398996\n",
      "  [1600]: mean loss 0.384103\n",
      "  [1800]: mean loss 0.382504\n",
      "  [2000]: mean loss 0.373063\n",
      "  [2200]: mean loss 0.382219\n",
      "  [2400]: mean loss 0.376331\n",
      "  [2600]: mean loss 0.37085\n",
      "  [2800]: mean loss 0.368259\n",
      "  [3000]: mean loss 0.371922\n",
      "  [3200]: mean loss 0.368865\n",
      "  [3400]: mean loss 0.376425\n",
      "  [3600]: mean loss 0.36439\n",
      "  [3800]: mean loss 0.375313\n",
      "  [4000]: mean loss 0.384731\n",
      "  [4200]: mean loss 0.368322\n",
      "  [4400]: mean loss 0.356872\n",
      "  [4600]: mean loss 0.356436\n",
      "  [4800]: mean loss 0.372482\n",
      "  [5000]: mean loss 0.360563\n",
      "  [5200]: mean loss 0.355331\n",
      "  [5400]: mean loss 0.359044\n",
      "  [5600]: mean loss 0.367231\n",
      "  [5800]: mean loss 0.364123\n",
      "  [6000]: mean loss 0.36627\n",
      "  [6200]: mean loss 0.358125\n",
      "  [6400]: mean loss 0.358088\n",
      "  [6600]: mean loss 0.36719\n",
      "  [6800]: mean loss 0.351456\n",
      "  [7000]: mean loss 0.355389\n",
      "  [7200]: mean loss 0.369232\n",
      "  [7400]: mean loss 0.35745\n",
      "  [7600]: mean loss 0.351873\n",
      "  [7800]: mean loss 0.352285\n",
      "  [8000]: mean loss 0.350245\n",
      "  [8200]: mean loss 0.35581\n",
      "  [8400]: mean loss 0.352481\n",
      "  [8600]: mean loss 0.358982\n",
      "  [8800]: mean loss 0.351604\n",
      "  [9000]: mean loss 0.3564\n",
      "  [9200]: mean loss 0.349023\n",
      "  [9400]: mean loss 0.356292\n",
      "  [9600]: mean loss 0.359697\n",
      "  [9800]: mean loss 0.351058\n",
      "  [10000]: mean loss 0.361956\n",
      "SGD complete: 10000 examples in 707.34 seconds.\n",
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 1.9314\n",
      "  [200]: mean loss 0.776345\n",
      "  [400]: mean loss 0.829889\n",
      "  [600]: mean loss 0.813322\n",
      "  [800]: mean loss 0.540588\n",
      "  [1000]: mean loss 0.574643\n",
      "  [1200]: mean loss 0.56375\n",
      "  [1400]: mean loss 0.542828\n",
      "  [1600]: mean loss 0.520444\n",
      "  [1800]: mean loss 0.458677\n",
      "  [2000]: mean loss 0.470547\n",
      "  [2200]: mean loss 0.464942\n",
      "  [2400]: mean loss 0.454042\n",
      "  [2600]: mean loss 0.406435\n",
      "  [2800]: mean loss 0.486523\n",
      "  [3000]: mean loss 0.47113\n",
      "  [3200]: mean loss 0.409398\n",
      "  [3400]: mean loss 0.489617\n",
      "  [3600]: mean loss 0.439312\n",
      "  [3800]: mean loss 0.432525\n",
      "  [4000]: mean loss 0.430573\n",
      "  [4200]: mean loss 0.4393\n",
      "  [4400]: mean loss 0.401872\n",
      "  [4600]: mean loss 0.420769\n",
      "  [4800]: mean loss 0.446872\n",
      "  [5000]: mean loss 0.39897\n",
      "  [5200]: mean loss 0.38781\n",
      "  [5400]: mean loss 0.407987\n",
      "  [5600]: mean loss 0.399244\n",
      "  [5800]: mean loss 0.441286\n",
      "  [6000]: mean loss 0.429403\n",
      "  [6200]: mean loss 0.39889\n",
      "  [6400]: mean loss 0.426276\n",
      "  [6600]: mean loss 0.472921\n",
      "  [6800]: mean loss 0.450123\n",
      "  [7000]: mean loss 0.430858\n",
      "  [7200]: mean loss 0.431715\n",
      "  [7400]: mean loss 0.362532\n",
      "  [7600]: mean loss 0.38238\n",
      "  [7800]: mean loss 0.362605\n",
      "  [8000]: mean loss 0.385733\n",
      "  [8200]: mean loss 0.385275\n",
      "  [8400]: mean loss 0.357625\n",
      "  [8600]: mean loss 0.369294\n",
      "  [8800]: mean loss 0.391539\n",
      "  [9000]: mean loss 0.375275\n",
      "  [9200]: mean loss 0.383759\n",
      "  [9400]: mean loss 0.41769\n",
      "  [9600]: mean loss 0.423681\n",
      "  [9800]: mean loss 0.394689\n",
      "  [10000]: mean loss 0.436413\n",
      "SGD complete: 10000 examples in 4314.31 seconds.\n"
     ]
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGJCAYAAACpTmgpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAAPYQAAD2EBqD+naQAAIABJREFUeJzs3Xl8XHW9//HXJ0u3pEu6l7Z0YysCslkoSG9ZZFNZCopY\n",
       "UKsoeNV75d5LUfxpGzcEFxBE1IsispStKItXBBGQslVAFJDVtkDLVqBbkiZpks/vj3OmnU5mzkxm\n",
       "JjMnmffz8cgjmTPnfM93JjNz3vP9fs/3mLsjIiIiEmdV5a6AiIiISDYKLCIiIhJ7CiwiIiISewos\n",
       "IiIiEnsKLCIiIhJ7CiwiIiISewosIiIiEnsKLCIiIhJ7CiwiIiISewosIiIiEnsKLCIiIhJ7CizS\n",
       "J5nZp8ysy8x2LHdd+hszW2hmz5a7Hv1VIa/ddNua2eJw2cji1jQ/hdYnLo/HzM4ys5fNbEA56yHb\n",
       "KLBUsKQPv33LXZc8lfXKnWY2w8x+bmYrzGyzmW0ws2Vm9h9mNqicdcuXmQ0DzgW+l7K81syuN7Ob\n",
       "zay2PLXrVwp57fbq697MDjKzRWY2vDf3Uw5mNjf8zEv3Mytl9SuBAcCZZaiqpFFT7gqI5Ok3wHXu\n",
       "3l6OnZvZB4GbgM1hXZ4m+HA7BPg+8B765gfdpwm+yCxJXujuW8zs08DrwCnANb1VATM7DxgFbAKm\n",
       "AV9y942FbmNmBhwGfN3d5/ZC1XNVyGu3FK/7g4BFBAfsDb24n3L6MfDXlGX/Sr7h7m1mdhXwX8Cl\n",
       "paqYZKbAImVnZnXu3tyTbdy9CyhXWJkGXA+sBA5z9zeT7r7czGYAxxZpXz1+bgq0ALgt3QHR3VvM\n",
       "7PfASfRSYDGzLwBz3P3o8PZXCA7SJxSyjZl9hOB/Ug9M6Y2656qQ126JX/dWov2UwwPufksO690I\n",
       "LDSzQ9393t6ulERTl5BkZWYTzexXZvammbWa2dNmtiBlnSlm9lMze97MWszsbTO70cympKyX6J+e\n",
       "aWbXmdm7wAMp980ws1+b2TozWx/ue3BKOdv15fdk23D9uWb2WNiV85KZfS5RRg5PyUKgDvhMSlgB\n",
       "wN3/5e6Xhvv5tZmtTLP/bvvK9NyY2Unh8jlpyjkzvG/38HbW/1UmYRDbE/hTxGq3AEeZ2ZBcyszD\n",
       "ucBVSbevBo4zs50L2cbdb3L3BcAdhVYw6f+0S/h/Wm9mb5nZt8L7dzSz28xso5m9bmZnp2yf92s3\n",
       "ddsUY8L33Ibw/XexmQ1M2jbre9TMFgMXhjdX2rbukuQxMxPN7Jdm9lr4GlsRlpvaVdiQy3sxF2Hd\n",
       "XzKzf5jZmHzK6F6kDTWzyC/t7v4E8C5wfBH2KQVSC4tEMrNxwCNAJ3AJsJbgm+ovzWyYu/84XHV/\n",
       "YDZwHbCaoFn+88B9Zra7u29OKfom4AXgq3T/JncjsAL4CrAfcAbwVng7m6zbmtk+wJ3AGuAbBO+D\n",
       "b4SPLZfxAR8G/uXuj+SwLhFlZlqe+tz8HmgCPgr8JWXdU4Cn3f2fPfhfZXJQ+PuJiHX+EP5OdIkV\n",
       "jZntAkwCnkksc/c1ZrYBmAu8WIRtitlqcEO433OBDwFfC/d7JvBH4BzgNOCHZvaYuz+QpbxCXveJ\n",
       "7VeG688G/gNoAD4Z3p/Le3QpsDNwKvBl4O1w27cBzGwHYDkwDPgF8BzB838SMBjYUsTHQ7jPGcCf\n",
       "wzp8wN3fDYPGiByLeMfdU99rVxK0tnWa2QPAOe7+eIbtnwAO7kmdpZe4u34q9Af4FNAF7BuxzhUE\n",
       "H24NKcuvA9YBg8Lbg9Jse0BY/mlJyxaHy65Js37ivv9NWb4UWJuh7jvmse1tBGMdxictm0HwYduZ\n",
       "5TkbFu7nlhyf418DKzM91gyPP91zcy3wBlCVtGw80AF8rSf/q4i6fivc/5As6/0OuL4XXo/HhPuf\n",
       "mrL8FeBbxdgmfN10+3/0sJ6J/9PlScuqwn12ERz8EsuHA83AlUV67W63bcr2v01Z9yfh8j3D27m+\n",
       "R/8ndR9J910Vvk+iPjNyfjxZth8J7EbwxeIRYHjSOnPDdXL5SX6uZhMEqU8RhMxzCYJ9C7B3hvr8\n",
       "HGgu9utdPz3/UZeQZGRmRvDN6Xag2sxGJ36Auwg+jPcFcPfWpO1qzWwUwSC29cA+aYr/WcSuU+9b\n",
       "Bowys/ocqh25rZlVA0cAv3P3NxIrufu/2NZ6EGVY+HtTDuvmK91zcwMwluCDOuFkggPlDT35X0UY\n",
       "BXS4e0uW9W4FjrXin+7ZEP5OHbPTlHRfMbYplisSf3gwtuRxglazXyYt3wA8T9CakU0hr3uAy1Ju\n",
       "JwaKHhvWpafv0e2YWRXBuKDbPegqyabQx7MncD9BK80R4XOZ8CTB+ziXn63dtu7+sLt/1N1/7e53\n",
       "uPsFwIEE/7fzM9RjHTDY+uiZf/2JuoQkyhiCA92ZpD/jxcN1CPumv0owaHMHtm96T3d6ZLdxHUle\n",
       "Sbm9LvzdQHAgipJt27HAIOClNNu+RPYug8SZJ0OzrFeIdM/NnQRnbJxC0DxO+Pff3P0lMxtLjv+r\n",
       "QphZHXAUMBA4mqC1KnWdKoJv07l8wG9w94+Ff3em/E6oJfNnVT7bFEvqa20D0Oru76Ys30hu4amQ\n",
       "1z107/5aQdDCMAXyeo+mGkPwun86h3Wh8MdzO0Gr4lGpIdrd17PtfVAQd/+Xmd0KzDMz87BZJUni\n",
       "eSrrNAqiwCLREi1wV7P9oMZkT4W/LyVoZr0IeJhtp0NeT/rB3aljWpKlHnwSchl/UMi2Wbn7RjN7\n",
       "Ddgj100yLK+O2Kbbc+Pu7Wb2O+BEM/s8MIFgzMlXw1V68r/K5B2gxjKcmRS2qFxH0GQ/gqBFp1tg\n",
       "CVsbTsyyr3TWhr9TXy91ZD69Np9tiiXday3ToO1yvHZTX3s9fY8WqtDHczNBfU8jGC+zrYBggO+o\n",
       "HMt5K3xNRllNMC1BHd3DVANBl1BbjvuTXqLAIlHWEnR91Lh7tm8zJwO/dvdzEgvCJtTebpbvqbeA\n",
       "VoKBhal2IrdvUXcAnzOzAz37wNt1pB8cmM+ptTcQDKA8Atid4IP/hvC+nvyvMnku/D2NlG/RYavJ\n",
       "tcCt7v43M7sF+J6ZVbt7pgNTTyValsaxbZBnFcHzt6KI2/RXuwAvJ93eiSCIrApv5/oezfQeWEvQ\n",
       "WrRnMSqbg3MIQs9PzWyTuyfPDXQwubewTKV7a0+q6cBmd0/X8jMN0MzPMaDAIhm5e6eZLQU+bmbf\n",
       "dfdnku83szHunviG20H3b2lfSrOsrMLH9CfgBDOb4O6vA5jZTgQDOHNxITAfuMLMDnP3t5LvDM9q\n",
       "+KC7X0IwRmC4me3p7k+F908gaIHoaRPzPQSnWJ5CEFgedfeXkx5Xrv+rTB4Of+9P92b/nwHr3f1X\n",
       "4e1bgZ8ShKc/puwrry4hd19pZi8RDLRM1H/XsJy0B6d8tull5ew2+AJwd9LtL4W/E2Ozcn2PJlrX\n",
       "Gkg60Lt7V9jKd5qZ7eeZz6opFgc+R9ANdZWZNbn77eF9iTEsudg6hiXd+8DM3gscR3A2Xjr7ErRc\n",
       "SpkpsAjAZ8ws3URnFxOcgngo8KiZ/S/BN42RBG/iw9nWLHsHcHp4WuezBKPxDyfoZojbBFSLgSOB\n",
       "B83scoL3wRcIDtJ7ZdvY3VeY2ccJWjeeNbPfEBwsBxB005xMcNokBDPGfg/4rZldQtDkfBbBQMwe\n",
       "XRLBg9lmbyE45XQI8N8pq+T6v4p6XE8DHyA4uwkAM/teWMZBSeu+aWYPho/1jynl5NslBEF31icI\n",
       "Ag8E4y1udfet4zPM7EyCrrADPJgHJ+s2SarIEKItmBfnfnc/NM+6Q+bXeineA1PDsRh/JHj/zQeu\n",
       "TQRlcn+PPhb+/o6Z3UBwVtBt4TiS8wjeO/ebWeK05gkEr4ODPcuMxD3l7m5mpxGcmXajmR3r7vcW\n",
       "MIblBjNrIQjnbxEE/88RdAN1O93azPYjCG635vsYpHgUWCpb4tvgWXT/QHXgV+7+mgXX2PgGMI/g\n",
       "VNp3CA7uC5PW/0+C5tv5BN9ul7Ht27enlBs1L0lP5izpSbnbbrg/YWbHAD8gOJX3VYIQs2v4k5W7\n",
       "325mexE0Wx9PMJ9FO8E4kXMIToXEgzkjTgR+RNAyk5iXYhe6n5kR9RgSbiCYz6KL4PTM5Dq9leP/\n",
       "KsqvgG+a2SB3b7XgejKnAId699lvLyNorv9S8hkoBbqAoKvpEoKzV8YRjGNIZgSDfqty3cbMjiZ4\n",
       "3g4BRpvZMuA5dz8jvD9x5sprOdQx0/+pJ8vzeu1mWJbY/hSC1/P5BCHjUoLXYkJO71F3f8zMvk7w\n",
       "uXA0wfM9DXgl/Dw4INzPfIKz5lYTtOIkxl719PGkWye5Ph1mdnK4j9+Z2RHunjqtfq5+G9b77LDu\n",
       "bxGMlWl093RdiB8BXnbNchsL1n1AtEhlCpu7Z7p7TqGlP7Lg4ocrgIVJ3T/9XtjCeDuwV2p3mlQm\n",
       "C2YJXgV818OZq6W8YjW+QKRUrPuU5zsTzFdxX1kqFBNhk/6FBJOHVZK5wBKFFUmyAGgjes4oKSG1\n",
       "sEhFMrPXCcaZrCQ4Y+fzBHN37BNOIiciIjGiMSxSqf5AMHh1PMG3qIeA8xRWRETiSS0sIiIiEnsa\n",
       "wyIiIiKxV3FdQuEFv44iGP1drFMxRUREKsEggtmD/+ju75RyxxUXWAjCyrXlroSIiEgfNp/g2mIl\n",
       "U4mBZVX4ez7brp0ive8igsmapHT0nJeenvPS03NeWrsRfOlfVeodV2JgSXQDPefuT5S1JhXEzDbo\n",
       "+S4tPeelp+e89PScl5bZ1knRSz6kQoNuRUREJPYUWERERCT2FFhEREQk9hRYpFSWlLsCFUjPeenp\n",
       "OS89PecVohIH3UoZuLs+VEpMz3npVcpzbmZDCM4WiYPnzWzfclein3nO3VvKXYlUCiwiItJTuwGP\n",
       "l7sS0mv2A2J35pUCi4iI5Os04NlyV0KKZiZwTbkrkYkCi4iI5OtZzYHSfyTNsRJLGnQrIiIisRer\n",
       "wGJmXzWzv5rZRjN708x+a2a75LDdXDN7wsxazexFM/tkKeorIiIipRGrwALMAS4FDgA+ANQCd4Uj\n",
       "0tMys2nA74F7gPcCFwNXmNmRvV9dERERKYVYjWFx92OSb5vZp4C3gH2BZRk2Owv4l7ufE95+3sze\n",
       "T3AxrLt6qaoiIiJSQnFrYUk1Ivz9bsQ6s4E/pSy7K1wuIiIi/UBsA4uZVRF07yxz939GrDoOeDNl\n",
       "2ZvAMDMb2Fv1ExERkdKJVZdQisuA3YH3l7siIiIivcnMzgNGAZuAacCX3H1jodtYcK7yYcDX3X1u\n",
       "L1S9ZGIZWMzsJ8CxwBx3fy3L6m8A41OWjQM2untbxHYXmdmGlGVLKmVqbRERiQcz+wLB8e7o8PZX\n",
       "gN8AJxSyjZl9hOBYWg9MyaNepwKnpiwe3tNyiiVWgSVMgpcCxwNz3f3lHDZ7mOAfkuwDwENZtjtb\n",
       "Ex6JiPQeMxsDUy+EAbNgYA20dUD7cli10N3XxrXsMjg3/Em4Gviume3s7i/mu4273wTcFE71sX9P\n",
       "KxV+gd/uS3x43aayXJYhVoGFoBvoVILA0mxmiZaT9e7eCmBm5wM7uHtirpWfAV80swuAKwmavhKp\n",
       "UkREysDMxsL0h+DaGcFMFQZ0AY/OhNMOMbPZ+QaL3iy71MK5xiYBzySWufuasAdgLtAtsOSxTbyn\n",
       "sM1R3AbdngUMA+4DXkv6+WjSOuOByYkb7r4K+CBBq8qTBKczf8bd7y5JjUVEJI2pF8A10+FAth0v\n",
       "q4DZBldPD1pH4lj2NmZWb2bfNrNvmNl3zezniZM5ouYH66EZ4e/U8SqbgB2LuE2fF6sWFnfPGqDc\n",
       "fUGaZfcTzNUiIiKxMGAWHJjhm/2BBsPmmOX7uT1sTnTZA2blV+42ZjYMuBf4prvfGi77OfBt4Bzg\n",
       "y8B3C90P0BD+bk5Z3pR0XzG26fNiFVhERKS/GFiTuSeiCpgynbzHQkwhuuyBxTi2XQSsTISV0L3A\n",
       "xeEA167klcOpOJYCg3Ioe4O7fyz8uzPld0ItmY/R+WzT5/XbB5ZVffXDVj/4ZZpbj3P358pdHRGR\n",
       "/qWtA5z0waILeHkFwXjDPLx8E/j0zGW3deRXbsDMJgCnAx9OuettYCxwJnBD8h3u3gWcmMfuEmNt\n",
       "UnsY6oDUM1kL2abPq9zAcmrnALo6d+bmmn+Y2V4KLSIixdS+HB6ZGYwrSfWIw8a/uJPXmZpmG/8C\n",
       "j0zLXHb78nzKTTILqAb+krI8EYRGufvKAveRkChnHEEgSrTWjABWFHGbPq9yA4sRDN09qaOWGwbd\n",
       "BmS9KrSIiORq1UI47ZBgEOyBFjQGdBEEitNXBPfHsWwgCCvr3X1zyvJEF8xPUjfIt0vI3Vea2UvA\n",
       "bmw762fXsJw/p9s4n236g8oNLAmTgNrOfjuqWkSkHNx9rZnNhvlFnyulN8sO3U8wNdiYRFlmtiNB\n",
       "l48DNWa2p7s/lVSnfLuEAK4CPkEQeAAWALcmz8FiZmcCXwUOcPc3c9kmSRXxOyu4xxRYqoDq/nGO\n",
       "uohInIQH+25ndvaBst8JZ4m92MyeDRe/CSwkaH05H7gbeCpDET11AfA9M7sEWE/Q1fOplHUMGMi2\n",
       "4JF1GzM7GjgDOAQYbWbLgOfc/Ywi1bukFFi6gE683NUQEZH4cPd7gHvS3PWfvbCvDuB/sqzzM4KJ\n",
       "UnuyzZ3AncWoYxz0+Saigq0GtlS/Uu5qiIiISGaVG1gceAVYWrOF5tbjyl0dERERyaxyu4SWVLfj\n",
       "tZqHRUREpA+o3MDS1DnbvUNXaxYREekDKrdLSERERPoMBRYRERGJPQUWERERiT0FFhEREYm9yg0s\n",
       "lfvIRURE+pzKPWwPY2C5qyAiIiK5qdzAMpK6cldBREREclO5gWWoAouIiEhfUbmBZeDg4eWugoiI\n",
       "iOSmcme6rR3aUO4qiIiIAJjZecAoYBMwDfiSu2/Mso0BhwFfd/e5vV7JMqvcwFI9SIFFRETKzsy+\n",
       "AMxx96PD218BfgOcELHNR4BjgXpgSinqWW6V2yVUUzOi3FUQEenPzGyMNdiVNtqesfH2vI22Z6zB\n",
       "rjSzMXEuuwzOBa5Kun01cJyZ7ZxpA3e/yd0XAHf0duXionJbWKqqNIZFRKSXmNlYGniIecxgEmBA\n",
       "F7CGmdzCIWY2293Xxq3sUjOzXYBJwDOJZe6+xsw2AHOBF7MV0Xu1i5fKbWGpZli5qyAi0m+N4ALm\n",
       "MZ3JbDukVgGTMeYxnRFcGMuyk5hZvZl928y+YWbfNbOfm9nA8L4hxdgHMCP8nTpeZROwY5H20S9U\n",
       "bgtLNfXlroKISL9VzSwmZfj2PxFjIHOs0fbNq+yBzIksu5pZeZWbxMyGAfcC33T3W8NlPwe+DZwD\n",
       "fBn4bqH7ARLjKZtTljcl3SdUdGDxoeWugohIv1VDTcbOiipgONOBx/MqeziZO0Kqwn0X7iJgZSKs\n",
       "hO4FLg4HxXYlr2xmVcBSYFAOZW9w94+Ff3em/E6opZKP0WlU7pNR3Vms5jwREUnVQQdO+mDRBWxg\n",
       "BfCRvMrewE040zOW3UFHXuWGzGwCcDrw4ZS73gbGAmcCNyTf4e5dwIl57C4x1iZ1iEYdsCGP8vot\n",
       "BRYRESm+TpazmplMThMr1uC08Rdf5E/kU7RdbH9hNdMylt3J8nzKTTILqAb+krI8EYRGufvKAveR\n",
       "kChnHEEgSrTWjABWFGkf/UIFB5aOweWugohIv7WehdzCIcxjOhMxqkicyePcwgrWszCWZQeqgfXu\n",
       "vjlleaLb5iepG+TbJeTuK83sJWA3tp0ptGtYzp/zqHu/VcGBZUsuLyoREcmDu681s9ks5UKqmUUN\n",
       "NXTQQSfLWc/CQk477s2yQ/cTTCQ7JlGWme1I0OXjQI2Z7enuTyXVKd8uIQjmYPkEQeABWADc6u5b\n",
       "T2k2szOBrwIHuPubSdtWUSFn/FZyYBlY7iqIiPRn4cF+QR8s+51wJtmLzezZcPGbwEKC1pfzgbuB\n",
       "pzIU0VMXAN8zs0uA9QTdQ59KWceAgYThxMyOBs4ADgFGm9ky4Dl3P6NIdYqdyg0sNe0Dyl0FERGJ\n",
       "J3e/B7gnzV3/2Qv76gD+J8s6PwN+lnT7TuDOYtclziqiGSmtmvbacldBREREclO5gaWqs8oaTd1C\n",
       "IiIifUDlBpaApucXERHpAxRYREREJPYUWERERCT2Kj2wDC93BURERCS7Sg8samERERHpAxRYRERE\n",
       "JPYqd+K4rmqgU4FFRCR/M83SXTJZ+qiZ5a5AlMoNLO310Nk8otzVEBHpw64pdwWkclRuYNlSBx2d\n",
       "o8pdDRGRPug5YL9yV0J6zXPlrkA6lRtY2uuga3NDuashItLXuHsL8ES56yGVpXIH3bbXg1eNLHc1\n",
       "REREJLvKDSxb6kDzsIiIiPQJlRtY2usA11lCIiIifUDlBpYtdWBdQ8tdDREREcmucgNLez1UddSX\n",
       "uxoiIiKSXaUHlrpyV0NERESyq+DAMsSp3jK43NUQERGR7Co3sHQMbqeqs9YabUC5qyIiIiLRKjew\n",
       "bBnUHv6lM4VERERirpIDS1v4lwKLiIhIzFVuYGkfvDn8S4FFREQk5io3sGwZ3BL+pcAiIiISc5Ub\n",
       "WNoVWERERPqKyg0sbUOawr8UWERERGKucgPLltomuqocBRYREZHYq9zAQtdm2uu7UGARERGJvQoO\n",
       "LB2ttA/tAoaXuyYiIiISLXaBxczmmNntZrbGzLrM7Pgs688N10v+6TSzsdF76thM2zBQC4uIiEjs\n",
       "xS6wAEOAvwFfCG97jtvtDIwPfyYAa6NX37KZ1uGGAouIiEjs1ZS7Aqnc/U7gTgAz68mmb7v7htxX\n",
       "b99M6/AqFFhERERiL44tLPl60sxeM7O7zOyg7Ku3b6ZtRBVuCiwiIiIx1x8Cy2vAmcA84CTgVeA+\n",
       "M9snerO2zbQOB68a0es1FBERkYLErkuop9z9BeCFpEUPm9kM4GzgE5m3bA0G3brpLCEREZGY6/OB\n",
       "JYO/AgdHr/LdT/P4aFjZMdm+ZbeFC5e4+5LerpyIiEjcmdmpwKkpi8v2Jb+/Bpa9CbqKIpzxQ3ae\n",
       "cBVHn93li7uOK0mtRERE+ojwC/x2X+LNbF/g8XLUJ3ZjWMyszsz2NrO9w0XTw9uTw/vPN7Orktb/\n",
       "spkdZ2Y7mdkeZnYxMBe4LHpPG4MuIfNB1mi1vfNoREREpBji2MLyPuDP4d8O/Cj8+9fApwnmWZmc\n",
       "tH4t8ENgItAC/B04wt3vj97NusTEcQBDgXcLrrmIiIj0itgFFne/j4iWH3dfkHL7+8D3e76nt5ID\n",
       "yzAUWERERGIrdl1CpbM6NbCIiIhITFVwYHm5jbatg50VWERERGKsggNLm9M2dHN4Q4FFREQkxio4\n",
       "sABtQ5vDvxRYREREYqyyA0t7fRNuUMaJcERERCS7yg4sVDXTMagdtbCIiIjEWoUHFprZMmQLCiwi\n",
       "IiKxpsDSXteBAouIiEisKbC0D+1CgUVERCTWKj2wNNE6XIFFREQk5io9sDSHk8cpsIiIiMSYAkvr\n",
       "iCoUWERERGJNgWXziGo0D4uIiEisKbC0NlSjFhYREZFYU2DZ3FCLAouIiEisKbC0NgwA6qzRqstd\n",
       "GREREUlPgaVta+PK0HJWRERERDJTYGnbOt5W3UIiIiIxVZPvhma2P/BJ4GBgFNAJbAFWAXcAV7v7\n",
       "+iLUsTclt7AosIiIiMRUjwOLmY0Evge8AVwH/Je7b0m6fzQwF/iRmf3V3S8vUl17gwKLiIhIH9Cj\n",
       "wGJmY4AvEYSUpnTruPvbwM3AzWZ2oJl9wd0vK7yqvSI5sGguFhERkZjqaQtLl7t/A8DMdgWmAeuA\n",
       "v7t7a+rK7v6Imb1UeDV7jVpYRERE+oAeBRZ3fwfAzH4E7AYMAd4L1JrZEuDH7v50yjZvF6muvaGZ\n",
       "9vrE3wosIiIiMZXvWUJ/d/dj3X0uwYDbXwGrgd+a2aJiVa4EmvEq6KzZjAKLiIhIbOUbWCaY2e4A\n",
       "7t4FrHH3RmBX4E0z+89iVbCXtQDQObAVBRYREZHYyjewXApcaGZ3m9mpwAAIwou7/wzYXKwK9iZ3\n",
       "OoFWtgxuQ4FFREQktvKah8Xdm83sw8BngG8Dk8zsMODvBK0Wo4pXxV7XzJYh7SiwiIiIxFbeE8e5\n",
       "uwNXmNkvgf2B9xFMb/8v4LfFqV5JNNNe34ECi4iISGzlHVgSwuDy1/CnL2qmfWgNmodFREQktno0\n",
       "hsXMdjWzaT3c5uieVankmmkd3oVaWERERGKrR4HF3Z8HPmRmHzczi1rXzMaZWSPBFP5x1kzrCEeB\n",
       "RUREJLZ63CXk7pea2VHAbWa2mqAr6C2CM4MagB0JLoj4BvBtd3+9iPXtDU20jhhPMP5GREREYijf\n",
       "MSyT3P3DZrYXcDiwO1APrAWeBc5w93VFqmNva2bzSEMtLCIiIrGVb2D5ipnVA3e5+0XFrFAZNLN5\n",
       "VDUw1Bqtyhd5V7krJCIiItvLd+K4TcBxwN/M7BUzu8LMPmJmDUWsW6k00zIyEdzqI9cUERGRssg3\n",
       "sCx198OBkcBZQBOwGFhrZkvMbECR6lcKzWwemaivuoVERERiKN/AcgWAu7e4+/+5+5fd/T3AFGAV\n",
       "8LUi1a8UmmltGBj+rblYREREYiivwOLub2ZYvsbdv0rQ4tJXNLO5YVD4t1pYREREYijfFpa0zKzW\n",
       "zM4H3i1mub2smbbhg8O/FVhERERiqKiBBRgInAZMKnK5vamZtuGJSfAUWERERGKo4GsJJXP3JmBy\n",
       "McssgWbats4Zp8AiIiISQ8VuYemLmvFqcGtGgUVERCSWFFigGYCuagUWERGRmFJg2RpYaltQYBER\n",
       "EYklBZZEYOkYuBnNwyIiIhJLCixbA8ugNtTCIiIiEksFBRYzm2Nm15rZw2Y2MVz2CTN7f3GqVxJB\n",
       "YNlSp8AiIiISU3kHFjM7CfgjsBnYl2AOFgi6Vc4rvGolEwSWtvotKLCIiIjEUiEtLF8HznL3M4D2\n",
       "pOUPAvsVVKsScqcDaKdteAcKLCIiIrFUSGDZBbg/zfINwIgCyi2HZlobOlFgERERiaVCAssbwM5p\n",
       "lh8MrCig3HJoZnODo8AiIiISS4UElv8FLjazA8LbE83sNOCHwOUF16y0mmkZ5cAwazTLuraIiIiU\n",
       "VCHXEvoVFzMCAAAgAElEQVQeQeC5BxhC0D3UBvzA3S8pQt1KqZmW0QYYUA9sKnN9REREJEnegcXd\n",
       "HfiOmf0A2IngQP9Pd++LB/tmWkYnWpuGocAiIiISKwVfrdnd24BnilCXcmqiZXRd+PcwYE05KyMi\n",
       "IiLbyzuwmNlFgKe5y4FW4CXgVnd/N999lFAzLaMT0/Jr4K2IiEjMFNLCsg+wd1jG8wTjP3YBOoFn\n",
       "gX8Hfmhmh7h73Ftgmtk8akD4twKLiIhIzBRyltBSggG3O7j7fu6+LzARuBtYAkwC/gL8qOBa9r5m\n",
       "WkYnZupVYBEREYmZQgLLucA33H1jYoG7bwAWAQvdvRn4JrB/YVUsiWZaRwwO/1ZgERERiZlCAssI\n",
       "YGya5WMIricEway3A9KsEzfNdNXWAS0osIiIiMROIYHlVuCXZjbPzCaFP/OAXwK/C9eZRTC+Je6a\n",
       "gTpgI9vCloiIiMREIYNuzyIYn7IEqA2XbQGuAv4rvP0scEYB+yiVILA4b2BqYREREYmbvFtY3H2T\n",
       "u38WGE1wxtA+wGh3/5y7N4XrPOnuT/akXDObY2a3m9kaM+sys+Nz2GaumT1hZq1m9qKZfbKHD6cZ\n",
       "qAbbhLqEREREYqeQLiFga3D5e/hTjBlihwB/A76Q2EXUymY2Dfg9wRlL7wUuBq4wsyN7sM9mADpr\n",
       "m1FgERERiZ2CZro1MwNmAjuSMrjW3W/Lp0x3vxO4Myw/l03OAv7l7ueEt583s/cDZwN35bjbMLAM\n",
       "2ExNuwKLiIhIzBQy0+104LfAnhlWKbj1JkezgT+lLLsLuKgHZQSBpWPwZgY2jSlSvURERKRICgkV\n",
       "PwZWEZza3AzsAcwBHgPmFlqxHhgHvJmy7E1gmJkNTLN+OkFgaa9vRV1CIiIisVNIl9Bs4DB3f9vM\n",
       "uoBOd19mZl8hCDP7FKWGpREElrZhW1BgERERiZ1CAks10BT+/TawA8GcK68AuxVYr554Axifsmwc\n",
       "sDG8knQmF5nZhuDPukFwGCzfOJnjNA+LiIiImZ0KnJqyuGzHyEICyzPAXsAKYDmw0MzagTPDZaXy\n",
       "MHBsyrIPAA9l2e5sd38CwIwG4F12OukFWDnHGs18kUeenSQiItKfufsSgrnWtjKzfYHHy1GfQsaw\n",
       "fCtp+28A04AHgGOA/8i3UDOrM7O9zWzvcNH08Pbk8P7zzeyqpE1+Fq5zgZntZmb/DnyEfAbdNo/p\n",
       "Ch/TkHzrLyIiIsWXV2AxswEEFz98GsDdX3T33QiuIzTO3e8poE7vA54If5xgNt0ngMbw/vHA5MTK\n",
       "7r4K+CBBq8qTBKczf8bd7851h+60Ax00j0u0qmgci4iISIzk1SXk7u1mticpk7q5+zuFVsjd7yMi\n",
       "SLn7gjTL7gf2LXDXzTRtHQozDHi9wPJERESkSArpEroW+EyxKhIDzWyakJipTi0sIiIiMVLoWUL/\n",
       "bmZHEAzAaQ6XG+Du/l8Zt4ynZpomVId/K7CIiIjESCGBZU+CsSUAuyQtN7Jc/yemmmkar8AiIiIS\n",
       "Q3kHFnefW8R6xEEzzWMS10PSXCwiIiIxUqrr/fQFTXQMGQxoen4REZGYKSiwmNkcM7vWzB42s4nh\n",
       "sk+EV0vua5qBOmAjCiwiIiKxkndgMbOTgD8CmwlOKU5caHA4cF7hVSs5BRYREZGYKqSF5evAWe5+\n",
       "BtCetPxBYL+CalUeCiwiIiIxVUhg2QW4P83yDcCIAsotl2agHgUWERGR2CkksLwB7Jxm+cGU9uKH\n",
       "xZJoYdmAAouIiEisFBJY/he42MwOCG9PNLPTgB8Clxdcs9JTl5CIiEhMFTJx3AUEgecegqsb3w+0\n",
       "AT9w90uKULdSSw4smodFREQkRgqZOK4L+I6Z/QDYiWD8xz/dfVOxKldizUAtnTVNVHeohUVERCRG\n",
       "Cjmt+Zdmdqi7t7n7M+7+aB8OK5C4FlJ7vSaOExERiZlCxrCMBv5gZq+a2ffNbO9iVapMgsDS2tAG\n",
       "DLNGs+jVRUREpFTyDizufjywA/AtYBbwuJk9Y2bnmdnU4lSvpILA0jJ6C0FX2aB0K5nZGGuwK220\n",
       "PWPj7Xkbbc9Yg11pZmNKWFcREZGKUsigW9z9XeAXwC/MbDJwKrCAIMRUR20bQ0FgaRrfEd4eRjCL\n",
       "71ZmNpYGHmIeM5hEcF3qLmANM7mFQ8xstruvLWWlRUREKkFRLn5oZrXA/gQtLdMI5mjpa4LAsmFy\n",
       "cmDZ3gguYB7TmUwQViB4BidjzGM6I7iwFBUVERGpNIUMujUzO8zMrgDeAq4kOCX4g8CkItWvlILA\n",
       "sn5KV3i7e2CpZhaTSD+2ZSJGNbMSN9V1JCIiUjyFdAmtBkYBfwA+C9zh7q1FqVV5hIFlmoe3u8/F\n",
       "UkNNhrgSRL+B1Fuj1bCYkeo6EhERKZ5CuoQagfHufqK735wcVsxsj8KrVnJBYFk3PRFJurewdNCB\n",
       "d1sa6ALq2BF4l314TF1HIiIixVPIWUK/cPf1idtmNszMzjSz5cDfi1K70moDulg3PfGcdA8sw3mT\n",
       "1Rm2XoPTyu+BC1jPyFy7jkRERCS7ggfdmtm/mdlvgNeB/wH+DBxYaLml5o4DzbQ2DATaSQos1mhm\n",
       "jfY1TuJQ/kgTr+IkRrp0Aa/i3MIKXmWBL/Lv0MKayK6jmsLOzhIREak0eR04zWwC8Cng08AY4Hpg\n",
       "IHCCuz9TtNqV3JubqZ83n2uoZguL7FL7PM5jfIIhjOBk6ljMm/yUpVxINbOooYYOOuhkOetZuHVc\n",
       "SqLrKF1o6QrvFxERkZz1OLCY2R3AUcCjwLeBG919s5l9BjKO8Ig9MxvL8CkNnPzy2HCg7Gi6GM0a\n",
       "dud3OB/gc/4L/18WAcFcM5l1spzVzGRymsiyBqeT5b3xGERERPqrfLqEjgWeAL7h7le5++ZsG/QJ\n",
       "I7iAk1+uTTNQFg4HbuKgnMtaz8Kwiyh919F6Fhaz6iIiIv1dPl1CBwOfAX5nZm8CvwJ+XcxKlUUw\n",
       "x0p6PRwo6+5rzWw2S7mQGg5kJLvRzGs0c9d2XUciIiKSkx63sLj7w+5+BsF1hM4HPgS8TDAV/5Fm\n",
       "NrS4VSyRbHOs9HCgrLuv9XW+wNf6TD7OE3yWu32dL1BYERER6blCTmtucvdfufvBwJ7AD4CvAGvN\n",
       "7PZiVbBkss2xUthA2WXA+wvYXkREpKIV5VpC7v68uy8kmJL/Y/TFwbfBQNn0Ch8ouwyYYY02oYAy\n",
       "REREKlZRAkuCu3e4++/c/bhillsS61nIzWM38gr0wkDZB8PfBxdURxERkQqlCcxCwUDZl67j+q+d\n",
       "yuAb12ScYyWfshf5a9ZoKwm6hW4uXq1FREQqgwLLdnZ6m5YbNnrzDe/phcI1jkVERCRPRe0S6gea\n",
       "gbpeKnsZsLc1Wn0vlS8iItJvKbBsr7cDSzVwQC+VLyIi0m8psGyvGRhoRnUvlP0c8C7qFhIREekx\n",
       "BZbtNYe/i97K4ou8C3gIBRYREZEeU2DZXq8FltAyYLY1mgY7i4iI9IACy/ZKEVjqgPf2UvkiIiL9\n",
       "kgLL9no7sDwGtKEJ5ERERHpEgWV7vRpYfJG3AX9F41hERER6RIFle73dwgLBNP3vt0bLdG1oERER\n",
       "SaHAsr1SBJZlwARgWi/uQ0REpF9RYNleKQLLQ+FvdQuJiIjkSIFle5sBpxcDiy/yd4Fn0MBbERGR\n",
       "nCmwJHHHgRZ6t4UFdCFEERGRHlFg6a43ryeUsAzY3RptVC/vR0REpF9QYOmuFIHlwfD3Qb28HxER\n",
       "kX5BgaW7UgSWVcBrqFtIREQkJwos3TXRy4HFF7mjcSwiIiI5U2DprhQtLBAElv2t0QaVYF8iIiJ9\n",
       "mgJLd6UMLAOA/UuwLxERkT5NgaW7UgWWpwi6n9QtJCIikoUCS3fNQH1v78QXeQfwMFkCi5mNsQa7\n",
       "0kbbMzbenrfR9ow12JVmNqa36ygiIhIXNeWuQAyVqoUFgm6hs63RqnyRd6XeaWZjaeAh5jGDSYAB\n",
       "XcAaZnILh5jZbHdfa2ZjGMGFVDOLGmrooINOlrOehe6+tkSPRUREpNcosHRX6sDSCOwOPN3t3hFc\n",
       "wDymMzlpWRUwGWMe01nKhWZ2bi6hpgSPRUREpNeoS6i7UgaWR4FOMl1XqJpZTMLS3jcRYzCHM4Gf\n",
       "bg01iTWTQ80ILuyFeouIiJSUAkt3JQssvsibgSfINI5lIHUZ4krwnxvKZOo5KTLUVDMrcVPjYURE\n",
       "pK9Sl1B3zcBgM6rc6TaupJjMbAx7UMdmTrHLbdbWsScH8xPex5epYwoOaeNIF7COFxjAEIxJaXdQ\n",
       "BQxkiDWasZgx6joSEZG+SoGlu+bw9xCC0457xdYBtQdsDRC7hAFid+7hk8xkLcbDrOZAJqeJLGtw\n",
       "tvAQXcyKDDV1TAWeZXdamR09HgZYUPQHKiIiUgTqEuouEVh6t1soeUDt9mNP4HDgV9zFCxzPLazg\n",
       "Vba19XQBr+LcwgrWs5BOlrMaT7uPINTcDTxEO3vk2nUkIiISN2ph6a40gSXbgFpj3/CU5dkszXzK\n",
       "spkt5BYOYR7TmYhRRaKrJwg165jvi3ytXW4HY+ySdn9VQE3wWtAp0iIiEkexDCxm9gXgHGAc8Hfg\n",
       "S+7+1wzrzgX+nLLYgQnu/lYeuy9NYKmhJnJAbRggwpCQsasml1ADQAcdWbqOJtu/2/9jFJ/hBKZq\n",
       "nIuIiMRJ7AKLmZ0C/BA4k+C037OBP5rZrlkOljsDm5Ju53tgLU1gyRYgOujItahsoQYg7DqamXE8\n",
       "zDDe4kkaOYEqjXMREZG4ieMYlv8CfuHuV7n7c8BZQAvw6Szbve3ubyX9pB/XkV1pAku2sSedLC/q\n",
       "/tazMHI8zJO8jxd5IcP5RhrnIiIiZRWrFhYzGwDsC3wnsczd3cz+BMzOsvmTZjaQYMbYxe7+UJ7V\n",
       "KE1gCQJE5rEn61lYzN3lNB5mvFXl0k0ludOYIBGR4ojbAWg0UA28mbL8LWC3DNu8RtB99BgwCDgD\n",
       "uM/MDnD3v+VRh5IElpzHnhR5n0R16RSxmyquShkgcr0WVDH3KSLSX8UtsPSYu78AvJC06GEzm0Ew\n",
       "9uUTeRTZEv7u9dlucxp7UkrR41xgEC+VvlLFU/IAkcO1oIjT/1/6BLXaSaWKW2B5m+DaOuNSlo8D\n",
       "Xu9BOX8l0/V5trnIzDakLFvi7kvM2EzpricUH1HdVHfRzqnMtUY7wBf5o7kUF7sP1hwDRNHqne3U\n",
       "dY0Jkh5Sq52UkpmdCpyasnh4OeoCMQss7t5uZo8DRwC3AZhZFcFUapf0oKi9CbqKopzt7k9kuK+U\n",
       "F0CMjchuqql8k3p+A/zJGu1YX+QPRJWV6wdrSUNNtgBRy0FFPSDkeOq6SM7Uaicl5O5LgCXJy8xs\n",
       "X+DxctQnjh+YPwKuMrPHCFpKvgwMBq4EMLPzgR3c/ZPh7S8DK4B/sm0My1zgyJ7uOLgI4NQLYZfh\n",
       "0PbfZq/Ph/blsKpimlqjuqms0Y4GbqOZO22qPUATkzOGjBw+WM3s3JKGmmwBooFdmMaL7MmwohwQ\n",
       "KmBMkJSYWu2kgsUusLj7jeHVg78JjAf+BhyddGAaD9sdTmoJ5m2ZSDD+5O/AEe5+f0/2a2ZjYfpD\n",
       "cO0MOACwkdA1Eh6dCaepqZXg6tI2wxawhWc5kqMyhQwWU8VgDo38YK3naEYxjbnFCTU5PYBsAaKJ\n",
       "1bQyhGMybN/TA0INz7Ca3bd7fAlrgAE8n3NZfVjsugb7shxb7fScS38Uu8AC4O6XAZdluG9Byu3v\n",
       "A98vfK9TL4BrpsOBScuqgNkGV0+H+WpqBXiXRk5icIaQMYOneRkYzFDSB4PE+kMYhzM+ct6XwRzK\n",
       "AC7lw0VqAh/JOlZDhgDhtPAnBnIQxsiM9c6xG8cabTCnsRs308ERVHcbE3Q3W5jPkdZo81jMA6U8\n",
       "uOhMqdzE8qDvVEWG7gE02AftfTSwpC8+5yJRYhlYymPALDgwwyH2QAvuT+42GjALBtZAW0dFdRtF\n",
       "N0nDXTQBp7Oe83F2zvjB+g7PUkMtxs5py6oChjIFZ0oxJrOzRpvFCbyP69jI0QzNOPfNKO4rtBvH\n",
       "Gs2AnzOUnTiAI1nKJ7od9CbRSD0X0MRSdmAdx9BQim4xnSmVm7gFrfA19SlmMDUidMMkGljPcual\n",
       "rNMHnnOJt62fQcOYw8by1EGBZauBNdFNAoNq0nQbEXyKVVC3UbYm6XbW+SJfahfbh1jNThkvBRDM\n",
       "5DsrMhys50UGMARjYsb95dDiYY02DriFOv5GFSexlG9nnDyvwaJP7a7l2Wz7A74InA7M9xv8XuDe\n",
       "DPX6GHcwjWN4X7G6xbIGm1IHiL465qIHz1MRw2T6cqbyTU7gO8CpzOI6ruZATmRa2tA9nsPZwDI+\n",
       "kCHmJz3nudQ7lq1MUnLbBfhq4BflqYcCy1ZtHUQePSdMh/fcC1+q7G6jXAeS5jKT7wgujLy+UTsP\n",
       "0sksnIn5tnhYo9UCNxK81k/yV3wNUf+jqHrfTQen8X5rtN18kT+XYX9zgIuAi3yRXxdVN1/kbpda\n",
       "XdYWpNxPx87aKsCo4gaIrAe0Wgb2yTOlcgxaxWqJiSznT8yniTbq+bhf7EvsxzYm44ST7/paG28t\n",
       "kc95PePsVDuKBi6LfK2AxamVSXquaIEz+TMo2/m3vSieHxZl0b4cHpkZhI9UjziMeB7e3TWXbqN+\n",
       "LdtFFMNrIOV0KQCzQkMNjKXVGq3KF3lXt/sDPwAOAg71Rb4m28OLrPfefJ96bgDut0Y7whf5U8nb\n",
       "WqNNAm4CHoAcL62Q/cylnehkSuTBcyCHWKPtyji+xYcyBpsZPMFjbGJ8sQJE5EH2No60s+0xhjKj\n",
       "T54plcP/xRrtOnZjZw4uQotVVCg9glqu4VZ/3ZdAEWasrmUEr3BnZLfRHVyGYXyw73XnVYpsYaSo\n",
       "3ZpRAb6EFFi2WrUQTjskaCk50Nh69HzE4fQVsOJQ2GsZ2C7pt68i6Fbq53pwDaRsH6wFh5o/sJ75\n",
       "7AP8wRrtdBbj272Bq6ljKhN5L1/xn/qyXB9illO7DwXupon7bZI9QCs7hfXuZBqjmUs79Zziizy3\n",
       "A3G2g0s7TdQwIPLgOZwZwHMMg4jWGriHkXTRhDOyKAEi6iB7HDvwEP9GFY+ymlkZA2cvnClVpG+V\n",
       "1Vn/LzCFDvYtSotVtrFhW9g9p3Ig+5eKd7iGJv6NDzA1w/6MYXwEiHo9xbc7rwLk1JpazO7fYLxh\n",
       "2fX/A2yOth48mZ9xQK3Zrlm6jWoGmmHueC6Dc4u1Tq6KUVaxr4FUaKihjn2Aa9jEPxhDB8cxMeUN\n",
       "DNfzWbvcflWM5mtf5G/b++yjvMVTHM1x3fZ1FatZi7MoxwKzHVw2chvVzMLZPePLbgMrgU/SwnVY\n",
       "hkNMFbCZ12jjIVbzyYwBYiqjrNHey2Jey3rQz3aQ/R1reIcP8xYPR3SxHWmNNp/F3FXKQcUZQ80Q\n",
       "vsbnOJPpTIs8o2wjt/kiX2CX2/MY6b/E9OQ042JOMpjtS8U6zmEcyyL318JqwLG0z0C8u/MqQbYw\n",
       "spwH2MQOxQjT1mhHUc/UjIe+EtILLkn2a/tEdRs9Chw+BXjY7IGfwPTFUYNzg4XRA3hzWWfrh2/2\n",
       "4FO0AcOlvgZSlv3dZY22N/fxOMcxMc0bmKI3X7/EeZzEgLT7Oo6JPdpXMcb6tHG/L/IH7FLbmLX7\n",
       "JWp/t/M6p9NEE0+wA5s4huFpD/rH2MkcyGyGMyXbQTYycE6ikTq+SRPXMJGNHM2wgpuuC52w8G4+\n",
       "TjNV7MePuJYTs7YkZmshG8RYO8EOooHfpN3fb5ljn7fLqGNKsbrOcmq5HG3R9W4NzwPpi915ZRKr\n",
       "WbubmUIVnZHvz1oGRtZ7F77DsZwHLGA4r7GaCWk/g0pIgaVHsnUbXfk14Itw29VwLVkG55LDvC9Z\n",
       "1zGzc3MLIrnNM9MXT9v2Rf6aXWrv8iHGp12h2M3XRTzzpWhjfSC6tWZ1ML4oa4vVUNazhGUcw6yM\n",
       "Y2Ge5W9AB05bLge0LF1sn+RWpnM0BxflGk85XH4hMtR8gAFcx62+2s+xn9iFWVsSs7WQjWcAa3kw\n",
       "43iRE5nOP/gBw3iF1eyYbWxYrrJ+qchxLFrk+LEhrOpJnYopbmc4lfI0eGu0euoZGxlGmnklDNOZ\n",
       "W2aHMt2+bNczmoM4nslpBnufThMt1PNZnuJWVoYtpVXlCy3m7uXad1kkXQdhv4hrCUVtn0Nrxj4r\n",
       "4YmpmT/JD98EOPx5WOZ1jmgCM/hTXeZ19l0JWx6BKz6WvtXnIYf5V8HKM+E9T8HTu2Qua+Y/4YVD\n",
       "M4Qfh9NWwIqcW3RyVayybLw9z+czNM0DXM4L/obv2pO6xWFfW/eZ2wf0TIbX/J2TOmqZxLY8vRpY\n",
       "WrOFDR17uac/u2m7fY22Z/hixAfdFazmc+zJxVzESRm6l17FWcpVvs4zHzRz3d9l/JN3ODTDASEI\n",
       "bftwPHM4hKv5IaczJOPOrgO66GJ+ho/dcH/+tr8nW70h/L80ZOjyuoUV1PNvdPFXzmBCxv1dzous\n",
       "5eDIctZR1DNystZ7HbMBMq5zF62cQjv1zPFF/o+c99m73X7Jdc90hlPvPJ8NdmUx3gvh44vqrpwH\n",
       "fJ1rGcPHscj3TCfLI+v0II8xnN3Zg7q0HX+vAndwo7/hp2xXry7msJHp5HkMLYRaWHoot+6Qzvbo\n",
       "OV1qq4K/o9apBuiKXmfHadA5bftWk2QHGux2OvApmJFlf6NGwB6XwC+K1KKTPYzk2k2VU6gp4nV7\n",
       "su6vB/sqaovV+qnAAGAg0Aa0w3ZfcqcuZMNlNVx/EwxYDtUd0FkD7bOg5eQa+OK55NJVlW08RSct\n",
       "vsjX2+LcWn6yPgfZ9jecyYzlTg6KOAPqWZ4GOumiNfJ/s4lXqWUQxpiM++vB2IysLWTBacabMCZk\n",
       "3F8VXuyxYQXXe9t7OP060/kW9dwM/NEa7WBf5Cui9lfUFojcBpMSx/mGCjqz50+cRjNV1PEbNlLH\n",
       "ak6ObCHLPpbpg4ziPo7OMKB7ItDBHombiWOf6eKH/U22OV1efjn423fPvM4rr2Rf5+UVUDcIbIf0\n",
       "9agCWtYBC2HNYvAdM5fVsAM0nBIdfuoPhr0uhZ/l0rWUNYzk0k2Vc0Dq4B8Zr9uzGujg71CcEMWI\n",
       "7F0v4b6KEshyH380YBYcY7QcG1xVa/v/seU8W3OOgSzH7qzsz+eoLPtzqtjCXpEDfP/Aa8BerOMH\n",
       "EYOKnc3cQzuzcMZke3w9CptRYbIHzyc5HESLFYJzHouW7rE9sWoTx3E0sAy4Owwtb2Qso5hnrGQL\n",
       "B/UcA3ip5htiMW8zkLrI0D2QITbExtHAg3mf2XMENVzL73yNL7DFNoZb2Dfqy0JO78/x1qeuKB+r\n",
       "yvQf2eZ0aQ/7owtdZ+NfoHUW+A6ZPw3feMudK83enQOPfDJzWa/cCEMPiQ4/03aGzp2jQ83Yo8w4\n",
       "DPY6MzrYLL4RmnaPntdm4odh7z1gYfbJ+jZMdG4CTl5Dt+6QmyfCxtwCRE5jfdavupCba+Zn6Hrp\n",
       "YEPHBcF2xQlk2cv5yf1mvAo77RzdilY/qJiBbKvIlp8cns/OVdH7W8dN4TWeMp+N00mTL/J3wlaf\n",
       "uZzI1G7/m9/yMutZyHAuyRZuexA2ixlwSzp4Ppts+2Lxitks5kjgQdbzZxs+aCBVXROpxujE2VL9\n",
       "Cs2tx7n7c0VrgWi0oQzJcFo+JK5TFrSe5XAgzrGrdSwNPMq88DW1LWjszv9xPJt4O+ug6TqmsiOr\n",
       "eA+DMo5lepR7aI6ccwnagvdAj1rkihCmtz5XTL0QJs8J+otKT4GlV2QbnLsqnKukGOtMvTC34JN1\n",
       "npkvwS73RYef1S/DgGqwDLMzVAHjJgD3BKkhKtjUzoWRWbq8hg2F1n2jQ830j5pRB9MOZ+NSuP7c\n",
       "NN0h3wMO2w+GXgS/ijh4/vcvoW2v6P01HAoTRrPh/0V0vfx6iRlPwvSPRJc1Yi7slqVOX/gpVB0Q\n",
       "Xc7i6cBzsGEj+KjM/7/x0+GLL8H8ocUIZLkFwF2yX6NrPSdn3d8olub4wWqs29G5fg8YsCrpfzMV\n",
       "Wp52eIVcwi3U5ngx1OIE3NyDSPEGz2dfJ/u+fNHKBTbXzmIVt/GRNrY/oG/ZmVurnrZ/t5sYya7R\n",
       "b3Um22ftc4zkPE4Mrx+WHAxu5xg725YxgmMZxODI18I7BOO0ogacDmCkHWF70cCtaYPILcw1s1nu\n",
       "vpbh/Jh5TE17VuAxNHA369nC3azmiIyhtJ27WM3+HMWgtM/BJIz72I1qtvSoxSMijPRea3EtsH+G\n",
       "SvYuBZZekMucLhD2Dxe4jpnlFI5ym2dmWpaWoXX3BttGfVq88E/gBHj3XrD01wCiCnjtJWhpj+7y\n",
       "+tdL4TWeIibrs05gJNTXwVhouTJNdwjAjF2gc5foEDXyw8HYoahPjElToHMKHEPmrpdL9wDaoCpL\n",
       "IJs8FTqnRtdph5Oz1+mNl92ZZ7bmyuhWtPZ74cm94ZKI/dW/H6aS21iYbAe0H/wfNE2JrvuUKbDX\n",
       "EjYsiN5fx6qcuvyCOl03lZbZaf43D00NQumWvdn4fxHh9uSjoKsrOmiNP8aMxUErYNR6gw7IcWwR\n",
       "0c/lpy8y4yIYfUS2AJhbkMSi1zl8Lgz/t+h9jT7CjJnUDfoRH2tNdxYUHN9VzV85iVY2RE5YaAzg\n",
       "n/ycE0lfzocZxzKO4EN8iw0cyGqOy/JasMjXy0SG08LfmEdV2v2dyFT+wK+t0a5hqJ3IpAwnp0wC\n",
       "flvdwLrO/+TmjAPeO9jQ8WVGVz8afFylUQVsrN6Md66ODFrbuivL2Fpc0nG221Fg6SW59A8XY51c\n",
       "w1Fu+8ulZShbi07bcndeNHt3A/jEzGGkpT3HrrMsAenVV905wmzFM9HhZ80rYfiJ6PJ662VobQfP\n",
       "0LXSBax8EQYNAJuSuZzXVrhzoNkrWeq06l8wtA4s/enYVAHvrIZNLeARZ3i1ha0LWVvRPgZ7LgMb\n",
       "mXl/03aCjunRY2FGzDXj/dkPaK37QkdLlsk8HDa9J3p/449hw7RV3PRCRKvI5AlmnAJjj4quUyKU\n",
       "RpLQ6SAAABMlSURBVIXb+jqoqs4ySH00cAYMy3CmX2K9nWdC+85ZH9/W+mWq94z5wHyYQPT+Ju4A\n",
       "U26AxmytMGQ+mF0zA5a+DC9URe9rwiTgn9RWR8+wvLS2iy3V77C6dWTGAPHmoFfYsGUcH+gclrGc\n",
       "W6rdF3Wcb4snXZe9hYzoVrT2rlsZ+cYHOdIHp93fJKCeY4FjGeTRT0OVDckplHasGYh3RrwVqgbi\n",
       "nT0I5lFh5II7YO206NfUkIOy1/v/fcuMq2HCsZnLKh0Fln6gWBO55dYKk1uLTm5hpBgBKdHllW1/\n",
       "7/45bB2K6PLa2ByWs1Pmcpr+f3v3HiRXWeZx/PvLhGRqgiEQcllZQiAhRDEYEVDwUhFFQAV2FYFV\n",
       "C7ztKuuWAluKuKuii+WlVIQCXBFBWEEFdqUMKGBEKJYEjCQCogy5kCwhgQAmmZDMJJnk2T/e03DS\n",
       "dPd0Jj3dZ2Z+n6pTPd399HvefqanzzPvubz3pb/mWhvhUgHRV5823gtbjoKYXL2tdV1ZOwf3lYPG\n",
       "XK151UrYswNU+SyaF0eGuBemVGmnFPfUUuiaX3vU54mb4RXH1B5FGz8BYhJdi2qMinx0DjAHJvXR\n",
       "p2dXwgs9EIdUz8HyZennWsXmks4IDpWW9VGUPr0aOkaDxtd+f/TR7/VrgFNg6XUQM6uvb2RH2t1a\n",
       "a9Ru2mmpIKu2AXoDcO5a6N4McVD1dS3tBP6Ztu13onRaY8Wut40azfojD+CmJ+HUZRUKiGmwafy+\n",
       "tD88puYIxIiRr5A4B/abQ9fcGp+FD5wMI9rour5GzJmnsGX9SNRdfX0b2+GbT8HoAyBeqJ6G7e2j\n",
       "4JAP1S5KZ3yALdv2YNVTla+evArYsm8b3UcfwU0P1CjGjjpCYi5MP652MbLtCNi7j0pr2gzonV67\n",
       "31ecCpxae/d987hgsZ00bkSn72KksQVSo4qfRhZRzexTfb+/vouodb+DTUdBTKgxyrQUOAlWzIWY\n",
       "Xj2up7e+vs+4u3YRteQxaB8JE2dUHxVZsxx4HXQuqF1AbCgVpTMac1B8X/l89s6sUK5xbNGS7No4\n",
       "tfr93LoIFkrd98P9h1Rf35IbYO+3gKZVaIiU/+iF3i2g0dVjNnfBxvlw/4HV19W9IIK7tPf2XoK2\n",
       "6hv07b3AJroWjOJnn6tQQHwLOLOD3rYRtQfj9mgDvgaTOmqPkI3YAdu3wcT26jFbutjOuJr97qGX\n",
       "7n3moBG/YhVjqxYaW0dthp7u2kXp1k10T+7lpvYJVYu27kk90PW3tQvzD+0PdML27tq/v1VLs7NV\n",
       "a3ymVj8JHe21/zl5fhXwRlhyZ/W2migihtUCHE76djy81X0Z6gswAaZeAzMehVmd6XbqNcCEgWin\n",
       "r7j0/EFL4b4dsD0gIt3etyM9no/b/Xaa2addy2Xt9aV25+9Iz5Uv9+2AqdektuqN6ysHfbeTXrOj\n",
       "wvOl/s94tP626snBrvxedj+fu5bLvtbXd67qi6kzB2PaH+djBBdWWD5KMKb98brW17HnuprtdOy5\n",
       "LuWgQe+v7vXtdwNj90uPfSl7/ktZzNj9Ava7ob4+Tb0GbttBx4eDca8Oxs9Itx0fDrg1+xzM6qzc\n",
       "RmmZ1Vl/Dgbq7+rBaNU2tKkrK8LigmV4L80uoprdViPW18iNdf39aVQR1bgCsJmFcmOL4EYXSH3m\n",
       "aSZ7jdxacYO+18itwMz6+tR3YdDI91f/+pgAU5bTcWJZoXFiwJTl9ReljSk268/BQP1dLQwXLM16\n",
       "wy5YvHjpc2nUxrpR62vkBr2o+WzuKFpjis3cOmcypv1xxu3Rw/g9tjBuj540ssLMXetT7cKgke+v\n",
       "3vU18vPZiGKzsevrz9/V/statQ31XEJmNigMxok5W6Ux12Epbp8a1VbR5kVLbRy0oMaZfnVPfTJQ\n",
       "/W7lNtQFi5mZWUEUvTBv5TbUZwmZmZkVRDToMhVD0YhWd8DMzMysLy5YzMzMrPBcsJiZmVnhuWAx\n",
       "MzOzwnPBYmZmZoXngsXMzMwKzwWLmZmZFZ4LFjMzMys8FyxmZmZWeC5YzMzMrPBcsJiZmVnhuWAx\n",
       "MzOzwnPBYmZmZoXngsXMzMwKzwWLmZmZFZ4LFjMzMys8FyxmZmZWeC5YzMzMrPBcsJiZmVnhuWAx\n",
       "MzOzwnPBYmZmZoXngsXMzMwKzwWLmZmZFZ4LFjMzMys8FyxmZmZWeC5YzMzMrPBcsJiZmVnhuWAx\n",
       "MzOzwnPBYmZmZoXngsXMzMwKzwWLmZmZFZ4LFjMzMys8FyxmZmZWeC5YzMzMrPBcsJiZmVnhuWAx\n",
       "MzOzwnPBYmZmZoXngsXMzMwKzwWLmZmZFV4hCxZJn5K0QlK3pPslHdlH/BxJiyT1SFoi6axm9dXq\n",
       "I+kfWt2H4cY5bz7nvPmc8+GjcAWLpNOB7wBfBl4HPATcIWlClfgDgduA3wKvBb4HXCXpnc3psdXJ\n",
       "XyrN55w3n3PefM75MFG4ggU4D7gyIq6NiMeATwKbgY9Wif8ksCwiPhsRnRFxOXAzcG5zumtmZmYD\n",
       "rVAFi6RRwOHAvNJjERHZ/aOrvOzofHzmzhrxZmZmNsgUqmAB9gXagGfKHl8LTK7ymkkV4p8Bxkoa\n",
       "3djumZmZWSuMbHUHWqA9u50pqaUdGWb2knR4qzsxzDjnzeecN59z3lwzs9v2mlEDoGgFy3PAdtKo\n",
       "Sd4kYE2V1zzNy0dfJgFdEbGlQvzU7Pb6fvbR+u/BVndgGHLOm885bz7nvPmmAvObucJCFSwRsVXS\n",
       "g8A7gF8CSBoBvB24tMrLFgDvKnvsOKon8g7gg8AKoGc3u2xmZjactJOKlTuavWKlY1qLQ9JpwLXA\n",
       "J4CFwDnAqcDMiHhW0teBV0bEWVn8VOBPwOXANcCxwCXAuyLiN01/A2ZmZtZwhRphAYiIG7NrrnyV\n",
       "tKtnMXBCRDybhUwG9s/Fr5D0buBi4DPAk8DHXKyYmZkNHYUbYTEzMzMrV7TTms3MzMxexgWLmZmZ\n",
       "Fd6wKlh2dVLF4UrSWyXNlfSUpB2STqkQ81VJqyVtlvQbSdPLnm+XdLmk5yRtlHSzpIllMftIul7S\n",
       "BknrJF0laUxZzBRJt0naJOkZSd+S1DYw77x1JF0gaaGkrux9/kLSjApxznuDSDpb0kNZHjZImi/p\n",
       "hLIY53uASPp89v1ycdnjznkDSbowy3N++XNZzODIeUQMiwU4nXQa81mkC9/8APgrMKHVfSvaApxA\n",
       "Ouj574AdwMllz58PrANOAmYBtwDLgNG5mO8DK4E5pOkW5gP/W9bOr4FFwJHAm4DHgetzz7cBj5BO\n",
       "nzss69da4GutztEA5PzXwJnAq7L3eivp1PsO533Acv6e7L1NA6YDFwFbgUOd7wHP/ZHAcuCPwHf9\n",
       "GR/QXF8IPAxMzC37DMactzyZTfylPQBcmrsvYBVwfqv7VuSFsoIly9sa4LzcY2OBbuD07P5ewBbg\n",
       "vbmYQ7K23pDdf1V2//BczPGkCwdOzu6fCPSSKypJp7uvB0a2OjcDnPd9s/y82Xlvat6fBz7ifA9o\n",
       "jvcEOkmXoPgdWcHinA9Yvi8EFld5blDlfFjsElL/JlW0yg4kXUk4n8suUkFYyuXrgT3KYjqB/wPe\n",
       "mD10NLA+Ihbl2v4t2R9BLubheOmUdkgTW44FDm3Q+ymqcdntX7Nb530ASWqTdAYwGrgX53sgXQ7c\n",
       "GhF3kTaYJc75wDlYaRf/Mkk/kVS6NMigyvmwKFjo36SKVlkpX5UmnJyUi9maffDLYybnYtbmn4yI\n",
       "XtIGOh9TaT35fgw5Sld3/h5pyLW0r9l5HwCSZkl6gbS7+ErgtIhYivM9ILKicDZwQfZQ/roazvnA\n",
       "uJ90KMTxwNmkIuVeSXsyyHJeuAvH2aA1UDNJDscZKi8HXg28uY5Y5333PEban74X8H7gZ5Lm1Ih3\n",
       "vvsp+6/+EuAdEbG19DB9v3fnfDdExO25u3+S9ADpeJTTSJ//SgqZ8+EywtKfSRWtsqez20q5fDoX\n",
       "M0rS2D5iyo8yHwnsUxZTaT35fgwpki4jzY31tohYnXvKeR8AEbEtIpZHxOKI+AJpKPxsXvpecL4b\n",
       "5/XABGCRpG2StgFvBT4taSv+jDdFRGwgHRA7jUH2OR8WBUtWzZcmVQR2mlRxQav6NUg9Qfpw5XM5\n",
       "FjiKl3L5ILCtLOYQYEouZgEwTjtPC38s6TP5QHZ/PjBLaaqGkuOADcBOp+UNdkouA04Bjo2IlWUh\n",
       "zntztAEjIsL5brx5wGuA12bLbOAPwE+yn53zJsh2BR0MrBl0n/NWH8HcrIU0/NXNS6eO/oB0RoBP\n",
       "a355rsaQvkBmkw6aOif7ef/s+c+R9k3mT4NbCozKtXEF6bTcOaT/rCqdBver7I8hfxrcT3LPjyCd\n",
       "jnc7adj+eNI+z4tanaMByPkVpFML30ran1ta2nMxzntjc/514C2kmWdnZfd7SQWj892c38HdwMX+\n",
       "jA9ojr+dfa9MBY4BfpO91/GDLectT2aTf3GfypLeQ6oIj2x1n4q4ZB/KHdmyPffz1bmYr5CGE7tJ\n",
       "R3pPL2tjNHAZqSh8AbgZmFgWszdwPdBFOrXtKnLXHclipgC3AZtIB3V9i/QfcMvz1OCcl+e6tJxZ\n",
       "Fue8Ny7nV5H+q+/JvjjvBN7ufDf1d/Diac3O+YDl+KfAU9nn/EngBuDAwZhzT35oZmZmhTcsjmEx\n",
       "MzOzwc0Fi5mZmRWeCxYzMzMrPBcsZmZmVnguWMzMzKzwXLCYmZlZ4blgMTMzs8JzwWJmZmaF54LF\n",
       "zIYNSXMk7agwkZuZFZwLFrMhRNIESd+XtFJSj6Q1km6XdExZ3Osk/VzS6ixuhaS5kt6Ti5mabdxL\n",
       "S5ekP0m6TNL0OvqyQ9LJZW0d1vh3XXX9d0u6uOzh+4DJEdHVrH6YWWO4YDEbWv6bNBPumaQZWU8m\n",
       "TTA3vhQg6RTgfqAji5tJmojsF8BFFUYf3k6aiPEw4AukyUMfknRsP/qnfrxm5wbStPX9EhHbImLt\n",
       "7vbBzJrPBYvZECFpHPBm4PyIuCcinoyIhRHxjYiYm8WMAX4EzI2IkyJiXkSsiIjOiLg6ImZXGH14\n",
       "PiLWZnG/JE0z/wDwI0n1focsz24XZyMtd+X6/XFJf5HUnd2enXuuNDJzmqR7JHUDH5C0j6SfSlol\n",
       "aZOkhyWdkXvdj0kz1H4me/12SVMq7RKS9D5Jj2YjTU9IOq8sryskXSDp6myUaaWkf8w9PyobdVqd\n",
       "vYcVkj5fZ17MrE4uWMyGjhey5e8ljaoS805gH9Isqf0SacbUS4ADgMPrfNlR2W1ptOa9AJI+SJop\n",
       "9gLSSM8XgP+QdGbZ678BXJzF3Am0AwuBdwGHAlcC/yXpyCz+06QZ2a/M1vc3wKryTkl6PfBz0gy2\n",
       "rwEuzNZ/VlnovwK/B2YDVwDflzQjt66TgPcDM4APkmaBNrMG6vfQqpkVS0T0Svow8EPgk5IWAfcA\n",
       "P4uIR7Kw0ka2s/S6bCN/V66pMyLitj5WV3r9VOAPdXTvuez2+bJdMl8BzouIW7L7KyUdCnwCuC4X\n",
       "d3EupuS7uZ8vk3Q8cBqwMCK6JG0FNufXJ71sj9R5wLyI+Fp2f6mkVwOfBa7NHgvgtoj4z+z+NyWd\n",
       "C8wBHgf2B5ZExH3Z809WzYKZ9ZtHWMyGkIj4H+CVpGNXbidtVBdVGDHIe4h03MtsYAzQVseqSlv+\n",
       "6G9fs91TBwFXS9pYWoB/yx7P+0PZa9skfVHSI5Kez153PKl42BUzSQfi5s0HDtbO1c3DZTFPAxOz\n",
       "n38MzJbUKekSScftYh/MrA4eYTEbYiJiCzAvWy6S9EPSSMa1wJIsbCbpOBQiYivZMSYVRiCqeVV2\n",
       "uzu7PvbMbj9e6kvO9rL7m8ruf5a0K+YzwCPAZuB7wOh+9KOeN72t7H6Q/cMXEYslHQicSDq+50ZJ\n",
       "8yLi/f3oi5lV4REWs6HvL6SRE0jHf/wVOL+/jWUH2n6aVOQsrvNlW7PbF0dvIuIZYDUwLSKWly0r\n",
       "+2jvTcAtEXFDtrvrCeAQdh7x2Urf/5T9JWurvO3O7FidukTExoi4MSL+CTgdeF92ELSZNYhHWMyG\n",
       "CEnjgZtIZwE9AmwEjiCNRtwCEBEvSPo48HNJtwKXAktJox0nZE2Vj27sK2ky6TTo1wDnZO2+exc2\n",
       "6muBbuBESauBnojYAHwZuFTSBuAO0gjJEcC4iCi/hkre48Cpko4G1pOORZlYFrMCeIOkA0gjNM9X\n",
       "aOc7wEJJ/w7cCBwNfAo4u0Js3oujMtlZRauBPwI7SMfRrImI9X20YWa7wCMsZkPHRtL1Vc4lHWz7\n",
       "CPBV0pky/1IKyg5ePYa0G+U64DHgt6TjXU6vcMDtPNIG+WHg68CjwGERcU+9HYuIXtKozCeAp0jX\n",
       "fCEifkTaJfSRrP27SdeGWZ5/eYUmLwIWkYqc32X9Kz8o99uk4uvPwDO8dHzLi+1FxGJSgXEGKV8X\n",
       "Al+MiOuoLd+nLuBzpLOWfg9MIZ29ZGYNpF0Y9TQzMzNrCY+wmJmZWeG5YDEzM7PCc8FiZmZmheeC\n",
       "xczMzArPBYuZmZkVngsWMzMzKzwXLGZmZlZ4LljMzMys8FywmJmZWeG5YDEzM7PCc8FiZmZmheeC\n",
       "xczMzArv/wGKy+56KxY8pQAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1228de0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "# Plot comparison of learning rates here\n",
    "# feel free to change the code below\n",
    "ws = 5\n",
    "rs = 0.01\n",
    "k = 5\n",
    "N = 50000\n",
    "\n",
    "docs = du.load_dataset('data/ner/train')\n",
    "X_train, y_train = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=ws)\n",
    "\n",
    "minibatch_sched = [random.choice(len(y_train), k) for _ in xrange(N/k)]\n",
    "\n",
    "clf1 = WindowMLP(wv, windowsize=ws, dims=[None, 100, 5],\n",
    "                reg=rs, alpha=.01)\n",
    "trainingcurve1 = clf1.train_sgd(X_train, y_train, minibatch_sched, costevery=200)\n",
    "\n",
    "clf2 = WindowMLP(wv, windowsize=ws, dims=[None, 100, 5],\n",
    "                reg=rs, alpha=.1)\n",
    "trainingcurve2 = clf2.train_sgd(X_train, y_train, minibatch_sched, costevery=200)\n",
    "\n",
    "figure(figsize=(6,4))\n",
    "counts, costs = zip(*trainingcurve1)\n",
    "plot(5*array(counts), costs, color='b', marker='o', linestyle='-', label=r\"$\\alpha=0.01$\")\n",
    "counts, costs = zip(*trainingcurve2)\n",
    "plot(5*array(counts), costs, color='g', marker='o', linestyle='-', label=r\"$\\alpha=0.1$\")\n",
    "title(r\"Learning Curve ($\\lambda=0.01$, minibatch k=5)\")\n",
    "xlabel(\"SGD Iterations\"); ylabel(r\"Average $J(\\theta)$\"); \n",
    "ylim(ymin=0, ymax=max(1.1*max(costs),3*min(costs)));\n",
    "legend()\n",
    "\n",
    "# Don't change this filename\n",
    "savefig(\"ner.learningcurve.comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f): Evaluating your model\n",
    "Evaluate the model on the dev set using your `predict` function, and compute performance metrics below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict labels on the dev set\n",
    "yp = clf.predict(X_dev)\n",
    "# Save predictions to a file, one per line\n",
    "ner.save_predictions(yp, \"dev.predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.97      0.99      0.98     42759\n",
      "        LOC       0.83      0.86      0.85      2094\n",
      "       MISC       0.85      0.69      0.76      1268\n",
      "        ORG       0.76      0.65      0.70      2092\n",
      "        PER       0.89      0.83      0.86      3149\n",
      "\n",
      "avg / total       0.95      0.95      0.95     51362\n",
      "\n",
      "=== Performance (omitting 'O' class) ===\n",
      "Mean precision:  83.62%\n",
      "Mean recall:     77.14%\n",
      "Mean F1:         80.10%\n"
     ]
    }
   ],
   "source": [
    "from nerwindow import full_report, eval_performance\n",
    "full_report(y_dev, yp, tagnames) # full report, helpful diagnostics\n",
    "eval_performance(y_dev, yp, tagnames) # performance: optimize this F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save your predictions on the test set for us to evaluate\n",
    "# IMPORTANT: make sure X_test is exactly as loaded \n",
    "            # from du.docs_to_windows, so that your predictions \n",
    "# line up with ours.\n",
    "# Load the test set (dummy labels only)\n",
    "docs = du.load_dataset('data/ner/test.masked')\n",
    "X_test, y_test = du.docs_to_windows(docs, word_to_num, tag_to_num, wsize=5)\n",
    "\n",
    "yptest = clf.predict(X_test)\n",
    "ner.save_predictions(yptest, \"test.predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part [1.1]: Probing neuron responses\n",
    "\n",
    "You might have seen some results from computer vision where the individual neurons learn to detect edges, shapes, or even [cat faces](http://googleblog.blogspot.com/2012/06/using-large-scale-brain-simulations-for.html). We're going to do the same for language.\n",
    "\n",
    "Recall that each \"neuron\" is essentially a logistic regression unit, with weights corresponding to rows of the corresponding matrix. So, if we have a hidden layer of dimension 100, then we can think of our matrix $W \\in \\mathbb{R}^{100 x 150}$ as representing 100 hidden neurons each with weights `W[i,:]` and bias `b1[i]`.\n",
    "\n",
    "### (a): Hidden Layer, Center Word\n",
    "For now, let's just look at the center word, and ignore the rest of the window. This corresponds to columns `W[:,50:100]`, although this could change if you altered the window size for your model. For each neuron, find the top 10 words that it responds to, as measured by the dot product between `W[i,50:100]` and `L[j]`. Use the provided code to print these words and their scores for 5 neurons of your choice. In your writeup, briefly describe what you notice here.\n",
    "\n",
    "The `num_to_word` dictionary, loaded earlier, may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron 1\n",
      "[0]: (0.460) recorded\n",
      "[1]: (0.474) olympic\n",
      "[2]: (0.463) apology\n",
      "[3]: (0.483) outcome\n",
      "[4]: (0.508) english\n",
      "[5]: (0.592) open\n",
      "[6]: (0.509) index\n",
      "[7]: (0.498) australian\n",
      "[8]: (0.488) prize\n",
      "[9]: (0.619) cup\n",
      "Neuron 3\n",
      "[0]: (0.980) achievement\n",
      "[1]: (0.983) broadcast\n",
      "[2]: (0.988) radio\n",
      "[3]: (1.009) quiz\n",
      "[4]: (1.029) arithmetic\n",
      "[5]: (1.082) instruction\n",
      "[6]: (1.052) electronic\n",
      "[7]: (1.148) educational\n",
      "[8]: (1.072) curriculum\n",
      "[9]: (1.093) select\n",
      "Neuron 4\n",
      "[0]: (0.825) korea\n",
      "[1]: (0.866) continually\n",
      "[2]: (0.874) dock\n",
      "[3]: (0.898) partially\n",
      "[4]: (0.871) china\n",
      "[5]: (0.978) indirectly\n",
      "[6]: (1.040) thereby\n",
      "[7]: (1.085) temporarily\n",
      "[8]: (1.003) behalf\n",
      "[9]: (1.015) permanently\n",
      "Neuron 6\n",
      "[0]: (0.407) estimated\n",
      "[1]: (0.414) english\n",
      "[2]: (0.417) recorded\n",
      "[3]: (0.418) apology\n",
      "[4]: (0.427) prize\n",
      "[5]: (0.427) outcome\n",
      "[6]: (0.438) example\n",
      "[7]: (0.491) open\n",
      "[8]: (0.445) index\n",
      "[9]: (0.501) cup\n",
      "Neuron 8\n",
      "[0]: (1.319) 'd\n",
      "[1]: (1.328) worse\n",
      "[2]: (1.363) thereafter\n",
      "[3]: (1.367) nothing\n",
      "[4]: (1.393) sadly\n",
      "[5]: (1.555) unfortunately\n",
      "[6]: (1.584) there\n",
      "[7]: (1.398) aside\n",
      "[8]: (1.521) afterwards\n",
      "[9]: (1.828) suddenly\n"
     ]
    }
   ],
   "source": [
    "# Recommended function to print scores\n",
    "# scores = list of float\n",
    "# words = list of str\n",
    "def print_scores(scores, words):\n",
    "    for i in range(len(scores)):\n",
    "        print \"[%d]: (%.03f) %s\" % (i, scores[i], words[i])\n",
    "        \n",
    "#### YOUR CODE HERE ####\n",
    "neurons = [1,3,4,6,8] # change this to your chosen neurons\n",
    "hdim = clf.params.W.shape[0]\n",
    "topscores = [[[] for _ in xrange(10)] for _ in xrange(hdim)]\n",
    "topwords = [[str() for _ in xrange(10)] for _ in xrange(hdim)]\n",
    "for i in neurons:\n",
    "    z = clf.params.W[i,100:150].dot(clf.sparams.L.T)\n",
    "    idx = argpartition(z, -10)[-10:]\n",
    "    topscores[i] = z[idx]\n",
    "    topwords[i] = [num_to_word[n] for n in idx]\n",
    "    print \"Neuron %d\" % i\n",
    "    print_scores(topscores, topwords)\n",
    "    \n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b): Model Output, Center Word\n",
    "Now, let's do the same for the output layer. Here we only have 5 neurons, one for each class. `O` isn't very interesting, but let's look at the other four.\n",
    "\n",
    "Here things get a little more complicated: since we take a softmax, we can't just look at the neurons separately. An input could cause several of these neurons to all have a strong response, so we really need to compute the softmax output and find the strongest inputs for each class.\n",
    "\n",
    "As before, let's consider only the center word (`W[:,50:100]`). For each class `ORG`, `PER`, `LOC`, and `MISC`, find the input words that give the highest probability $P(\\text{class}\\ |\\ \\text{word})$.\n",
    "\n",
    "You'll need to do the full feed-forward computation here - for efficiency, try to express this as a matrix operation on $L$. This is the same feed-forward computation as used to predict probabilities, just with $W$ replaced by `W[:,50:100]`.\n",
    "\n",
    "As with the hidden-layer neurons, print the top 10 words and their corresponding class probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output neuron 1: LOC\n",
      "[0]: (0.005) japan\n",
      "[1]: (0.005) russia\n",
      "[2]: (0.005) korea\n",
      "[3]: (0.008) england\n",
      "[4]: (0.005) mexico\n",
      "[5]: (0.006) pakistan\n",
      "[6]: (0.005) germany\n",
      "[7]: (0.007) china\n",
      "[8]: (0.007) italy\n",
      "[9]: (0.005) norway\n",
      "\n",
      "Output neuron 2: MISC\n",
      "[0]: (0.003) belgian\n",
      "[1]: (0.003) german\n",
      "[2]: (0.003) english\n",
      "[3]: (0.003) turkish\n",
      "[4]: (0.004) brazilian\n",
      "[5]: (0.004) italian\n",
      "[6]: (0.005) open\n",
      "[7]: (0.004) danish\n",
      "[8]: (0.005) cup\n",
      "[9]: (0.004) israeli\n",
      "\n",
      "Output neuron 3: ORG\n",
      "[0]: (0.001) colleges\n",
      "[1]: (0.002) computing\n",
      "[2]: (0.002) curriculum\n",
      "[3]: (0.002) reuters\n",
      "[4]: (0.002) libraries\n",
      "[5]: (0.002) commons\n",
      "[6]: (0.002) corp\n",
      "[7]: (0.003) &\n",
      "[8]: (0.002) microsoft\n",
      "[9]: (0.002) inc\n",
      "\n",
      "Output neuron 4: PER\n",
      "[0]: (0.004) miller\n",
      "[1]: (0.006) carter\n",
      "[2]: (0.005) wilson\n",
      "[3]: (0.007) clinton\n",
      "[4]: (0.009) thompson\n",
      "[5]: (0.004) jim\n",
      "[6]: (0.004) martin\n",
      "[7]: (0.005) scott\n",
      "[8]: (0.005) pat\n",
      "[9]: (0.006) jr.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "def softmax(x):\n",
    "    if not hasattr(x[0], \"__iter__\"):\n",
    "        x = [x]\n",
    "    probs = exp(x - amax(x, axis=1, keepdims=True))\n",
    "    probs /= sum(probs, axis=1, keepdims=True)\n",
    "    return probs\n",
    "\n",
    "topscores = [[[] for _ in xrange(10)] for _ in xrange(5)]\n",
    "topwords = [[str() for _ in xrange(10)] for _ in xrange(5)]\n",
    "\n",
    "z = (clf.params.W[:,100:150].dot(clf.sparams.L.T)).T + clf.params.b1\n",
    "h = tanh(z.T)\n",
    "y_hat = softmax((clf.params.U.dot(h)).T + clf.params.b2).T\n",
    "\n",
    "for i in range(1,5):\n",
    "    idx = argpartition(y_hat[i,:], -10)[-10:]\n",
    "    topscores[i] = y_hat[i,idx]\n",
    "    topwords[i] = [num_to_word[n] for n in idx]\n",
    "    print \"Output neuron %d: %s\" % (i, num_to_tag[i])\n",
    "    print_scores(topscores, topwords)\n",
    "    print \"\"\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c): Model Output, Preceding Word\n",
    "Now for one final task: let's look at the preceding word. Repeat the above analysis for the output layer, but use the first part of $W$, i.e. `W[:,:50]`.\n",
    "\n",
    "Describe what you see, and include these results in your writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output neuron 1: LOC\n",
      "[0]: (0.000) surrounding\n",
      "[1]: (0.000) left\n",
      "[2]: (0.001) reaches\n",
      "[3]: (0.000) outside\n",
      "[4]: (0.000) located\n",
      "[5]: (0.000) at\n",
      "[6]: (0.001) near\n",
      "[7]: (0.000) visiting\n",
      "[8]: (0.000) inhabited\n",
      "[9]: (0.000) in\n",
      "\n",
      "Output neuron 2: MISC\n",
      "[0]: (0.000) modern\n",
      "[1]: (0.000) existence\n",
      "[2]: (0.000) tour\n",
      "[3]: (0.000) ancient\n",
      "[4]: (0.000) medieval\n",
      "[5]: (0.000) world\n",
      "[6]: (0.000) major\n",
      "[7]: (0.000) million\n",
      "[8]: (0.000) series\n",
      "[9]: (0.000) league\n",
      "\n",
      "Output neuron 3: ORG\n",
      "[0]: (0.000) charity\n",
      "[1]: (0.000) oracle\n",
      "[2]: (0.000) communications\n",
      "[3]: (0.000) grove\n",
      "[4]: (0.001) &\n",
      "[5]: (0.001) corporation\n",
      "[6]: (0.001) v\n",
      "[7]: (0.001) inc.\n",
      "[8]: (0.001) workshops\n",
      "[9]: (0.001) enterprise\n",
      "\n",
      "Output neuron 4: PER\n",
      "[0]: (0.001) aunt\n",
      "[1]: (0.001) jr.\n",
      "[2]: (0.002) dejected\n",
      "[3]: (0.001) mike\n",
      "[4]: (0.002) don\n",
      "[5]: (0.002) pat\n",
      "[6]: (0.002) mate\n",
      "[7]: (0.001) m.\n",
      "[8]: (0.002) peter\n",
      "[9]: (0.001) ode\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "topscores = [[[] for _ in xrange(10)] for _ in xrange(5)]\n",
    "topwords = [[str() for _ in xrange(10)] for _ in xrange(5)]\n",
    "\n",
    "z = clf.params.W[:,50:100].dot(clf.sparams.L.T)\n",
    "h = tanh(z)\n",
    "y_hat = softmax(clf.params.U.dot(h))\n",
    "\n",
    "for i in range(1,5):\n",
    "    idx = argpartition(y_hat[i,:], -10)[-10:]\n",
    "    topscores[i] = y_hat[i,idx]\n",
    "    topwords[i] = [num_to_word[n] for n in idx]\n",
    "    print \"Output neuron %d: %s\" % (i, num_to_tag[i])\n",
    "    print_scores(topscores, topwords)\n",
    "    print \"\"\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
